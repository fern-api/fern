{
  "absoluteFilePath": "/DUMMY_PATH",
  "importedDefinitions": {},
  "namedDefinitionFiles": {
    "__package__.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "errors": {
          "CreateAccessTokenRequestBadRequestError": {
            "docs": "Contains the details of the error",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {},
              },
            ],
            "status-code": 400,
            "type": "OAuthError",
          },
          "CreateVoiceRequestBadRequestError": {
            "docs": "Invalid request params",
            "status-code": 400,
            "type": "unknown",
          },
          "CreateVoiceRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "CreateVoiceRequestPaymentRequiredError": {
            "docs": "Current billing plan does not have access to voice cloning",
            "status-code": 402,
            "type": "unknown",
          },
          "DeleteVoiceRequestBadRequestError": {
            "docs": "Missing or invalid voice ID",
            "status-code": 400,
            "type": "unknown",
          },
          "DeleteVoiceRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "DeleteVoiceRequestNotFoundError": {
            "docs": "Voice not found",
            "status-code": 404,
            "type": "unknown",
          },
          "ExperimentalStreamCreateRequestBadRequestError": {
            "docs": "Invalid request params",
            "status-code": 400,
            "type": "unknown",
          },
          "ExperimentalStreamCreateRequestForbiddenError": {
            "docs": "Request access rejected",
            "status-code": 403,
            "type": "unknown",
          },
          "ExperimentalStreamCreateRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "ExperimentalStreamCreateRequestPaymentRequiredError": {
            "docs": "Insufficient credits",
            "status-code": 402,
            "type": "unknown",
          },
          "GetSpeechRequestBadRequestError": {
            "docs": "Invalid request params",
            "status-code": 400,
            "type": "unknown",
          },
          "GetSpeechRequestForbiddenError": {
            "docs": "Request access rejected",
            "status-code": 403,
            "type": "unknown",
          },
          "GetSpeechRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "GetSpeechRequestPaymentRequiredError": {
            "docs": "Insufficient credits",
            "status-code": 402,
            "type": "unknown",
          },
          "GetStreamRequestBadRequestError": {
            "docs": "Invalid request params",
            "status-code": 400,
            "type": "unknown",
          },
          "GetStreamRequestForbiddenError": {
            "docs": "Request access rejected",
            "status-code": 403,
            "type": "unknown",
          },
          "GetStreamRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "GetStreamRequestPaymentRequiredError": {
            "docs": "Insufficient credits",
            "status-code": 402,
            "type": "unknown",
          },
          "GetVoicesRequestInternalServerError": {
            "docs": "Internal server error",
            "status-code": 500,
            "type": "unknown",
          },
          "GetVoicesRequestNotFoundError": {
            "docs": "No voices found",
            "status-code": 404,
            "type": "unknown",
          },
        },
        "types": {
          "AccessToken": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "access_token": "optional<string>",
              "expires_in": {
                "docs": "Expiration time, in seconds from the issue time",
                "type": "optional<long>",
              },
              "scope": {
                "docs": "The scope, or a space-delimited list of scopes the token is issued for",
                "type": "optional<AccessTokenScope>",
              },
              "token_type": {
                "docs": "Token type",
                "type": "optional<literal<"bearer">>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "AccessTokenScope": {
            "docs": "The scope, or a space-delimited list of scopes the token is issued for",
            "enum": [
              {
                "name": "AudioSpeech",
                "value": "audio:speech",
              },
              {
                "name": "AudioStream",
                "value": "audio:stream",
              },
              {
                "name": "AudioAll",
                "value": "audio:all",
              },
              {
                "name": "VoicesRead",
                "value": "voices:read",
              },
              {
                "name": "VoicesCreate",
                "value": "voices:create",
              },
              {
                "name": "VoicesDelete",
                "value": "voices:delete",
              },
              {
                "name": "VoicesAll",
                "value": "voices:all",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "ApiKey": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "api_key": {
                "docs": "API key",
                "type": "optional<string>",
              },
              "created_at": {
                "docs": "Creation time of the key",
                "type": "optional<long>",
              },
              "id": {
                "docs": "ID of the key",
                "type": "optional<long>",
              },
              "name": {
                "docs": "Name of the key",
                "type": "optional<string>",
              },
              "updated_at": {
                "docs": "Last updated time of the key",
                "type": "optional<long>",
              },
              "user_id": {
                "docs": "User ID to whom the key belongs",
                "type": "optional<string>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "CreateVoiceLanguage": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "locale": "optional<string>",
              "preview_audio": "optional<string>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "CreateVoiceModel": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "languages": "optional<list<CreateVoiceLanguage>>",
              "name": "optional<CreateVoiceModelName>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "CreateVoiceModelName": {
            "enum": [
              {
                "name": "SimbaBase",
                "value": "simba-base",
              },
              {
                "name": "SimbaEnglish",
                "value": "simba-english",
              },
              {
                "name": "SimbaMultilingual",
                "value": "simba-multilingual",
              },
              {
                "name": "SimbaTurbo",
                "value": "simba-turbo",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "CreatedVoice": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "display_name": "optional<string>",
              "id": "optional<string>",
              "models": "optional<list<CreateVoiceModel>>",
              "type": "optional<CreatedVoiceType>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "CreatedVoiceType": {
            "enum": [
              "shared",
              "personal",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "ExperimentalStreamResponse": {
            "docs": "ExperimentalStreamResponse represents generated audio stream info",
            "inline": undefined,
            "properties": {
              "audio_url": {
                "docs": "URL to the synthesized audio file. It includes the expiration time and a signature in the query params.
The audio file will be available for download until the expiration time.
For the URL to work correctly, it must be used verbatim, with all the query parameters.",
                "type": "optional<string>",
              },
              "expires": {
                "docs": "Expiry time of the audio file, in ISO-8601 format.",
                "type": "optional<string>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetSpeechOptionsRequest": {
            "docs": "GetSpeechOptionsRequest is the wrapper for request parameters to the client",
            "inline": undefined,
            "properties": {
              "loudness_normalization": {
                "default": true,
                "docs": "Determines whether to normalize the audio loudness to a standard level.
When enabled, loudness normalization aligns the audio output to the following standards:
Integrated loudness: -14 LUFS
True peak: -2 dBTP
Loudness range: 7 LU
If disabled, the audio loudness will match the original loudness of the selected voice, which may vary significantly and be either too quiet or too loud.
Enabling loudness normalization can increase latency due to additional processing required for audio level adjustments.",
                "type": "optional<boolean>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetSpeechResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "audio_data": {
                "docs": "Synthesized speech audio, Base64-encoded",
                "type": "optional<list<integer>>",
              },
              "audio_format": {
                "docs": "The format of the audio data",
                "type": "optional<GetSpeechResponseAudioFormat>",
              },
              "billable_characters_count": {
                "docs": "The number of billable characters processed in the request.",
                "type": "optional<long>",
              },
              "speech_marks": "optional<SpeechMarks>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetSpeechResponseAudioFormat": {
            "docs": "The format of the audio data",
            "enum": [
              "wav",
              "mp3",
              "ogg",
              "aac",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetStreamOptionsRequest": {
            "docs": "GetStreamOptionsRequest is the wrapper for request parameters to the client",
            "inline": undefined,
            "properties": {
              "loudness_normalization": {
                "default": true,
                "docs": "Determines whether to normalize the audio loudness to a standard level.
When enabled, loudness normalization aligns the audio output to the following standards:
Integrated loudness: -14 LUFS
True peak: -2 dBTP
Loudness range: 7 LU
If disabled, the audio loudness will match the original loudness of the selected voice, which may vary significantly and be either too quiet or too loud.
Enabling loudness normalization can increase latency due to additional processing required for audio level adjustments.",
                "type": "optional<boolean>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoice": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "avatar_image": "optional<string>",
              "display_name": "optional<string>",
              "gender": "optional<GetVoiceGender>",
              "id": "optional<string>",
              "models": "optional<list<GetVoicesModel>>",
              "preview_audio": "optional<string>",
              "tags": "optional<list<string>>",
              "type": "optional<GetVoiceType>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoiceGender": {
            "enum": [
              "male",
              "female",
              "notSpecified",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoiceLanguage": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "locale": "optional<string>",
              "preview_audio": "optional<string>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoiceType": {
            "enum": [
              "shared",
              "personal",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoicesModel": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "languages": "optional<list<GetVoiceLanguage>>",
              "name": "optional<GetVoicesModelName>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetVoicesModelName": {
            "enum": [
              {
                "name": "SimbaBase",
                "value": "simba-base",
              },
              {
                "name": "SimbaEnglish",
                "value": "simba-english",
              },
              {
                "name": "SimbaMultilingual",
                "value": "simba-multilingual",
              },
              {
                "name": "SimbaTurbo",
                "value": "simba-turbo",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "NestedChunk": {
            "docs": "It details the type of segment, its start and end points in the text, and its start and end times in the synthesized speech audio.",
            "inline": undefined,
            "properties": {
              "end": {
                "type": "optional<long>",
              },
              "end_time": {
                "type": "optional<double>",
              },
              "start": {
                "type": "optional<long>",
              },
              "start_time": {
                "type": "optional<double>",
              },
              "type": {
                "type": "optional<string>",
              },
              "value": {
                "type": "optional<string>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "OAuthError": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "error": "optional<OAuthErrorError>",
              "error_description": "optional<string>",
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "OAuthErrorError": {
            "enum": [
              "invalid_client",
              "unauthorized_client",
              "invalid_request",
              "unsupported_grant_type",
              "invalid_scope",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "SpeechMarks": {
            "docs": "It is used to annotate the audio data with metadata about the synthesis process, like word timing or phoneme details.",
            "inline": undefined,
            "properties": {
              "chunks": {
                "docs": "Array of NestedChunk, each providing detailed segment information within the synthesized speech.",
                "type": "optional<list<NestedChunk>>",
              },
              "end": {
                "type": "optional<long>",
              },
              "end_time": {
                "type": "optional<double>",
              },
              "start": {
                "type": "optional<long>",
              },
              "start_time": {
                "type": "optional<double>",
              },
              "type": {
                "type": "optional<string>",
              },
              "value": {
                "type": "optional<string>",
              },
            },
            "source": {
              "openapi": "../openapi.json",
            },
          },
        },
      },
      "rawContents": "errors:
  ExperimentalStreamCreateRequestBadRequestError:
    status-code: 400
    type: unknown
    docs: Invalid request params
  ExperimentalStreamCreateRequestPaymentRequiredError:
    status-code: 402
    type: unknown
    docs: Insufficient credits
  ExperimentalStreamCreateRequestForbiddenError:
    status-code: 403
    type: unknown
    docs: Request access rejected
  ExperimentalStreamCreateRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
  GetSpeechRequestBadRequestError:
    status-code: 400
    type: unknown
    docs: Invalid request params
  GetSpeechRequestPaymentRequiredError:
    status-code: 402
    type: unknown
    docs: Insufficient credits
  GetSpeechRequestForbiddenError:
    status-code: 403
    type: unknown
    docs: Request access rejected
  GetSpeechRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
  GetStreamRequestBadRequestError:
    status-code: 400
    type: unknown
    docs: Invalid request params
  GetStreamRequestPaymentRequiredError:
    status-code: 402
    type: unknown
    docs: Insufficient credits
  GetStreamRequestForbiddenError:
    status-code: 403
    type: unknown
    docs: Request access rejected
  GetStreamRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
  CreateAccessTokenRequestBadRequestError:
    status-code: 400
    type: OAuthError
    docs: Contains the details of the error
    examples:
      - value: {}
  GetVoicesRequestNotFoundError:
    status-code: 404
    type: unknown
    docs: No voices found
  GetVoicesRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
  CreateVoiceRequestBadRequestError:
    status-code: 400
    type: unknown
    docs: Invalid request params
  CreateVoiceRequestPaymentRequiredError:
    status-code: 402
    type: unknown
    docs: Current billing plan does not have access to voice cloning
  CreateVoiceRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
  DeleteVoiceRequestBadRequestError:
    status-code: 400
    type: unknown
    docs: Missing or invalid voice ID
  DeleteVoiceRequestNotFoundError:
    status-code: 404
    type: unknown
    docs: Voice not found
  DeleteVoiceRequestInternalServerError:
    status-code: 500
    type: unknown
    docs: Internal server error
types:
  ApiKey:
    properties:
      api_key:
        type: optional<string>
        docs: API key
      created_at:
        type: optional<long>
        docs: Creation time of the key
      id:
        type: optional<long>
        docs: ID of the key
      name:
        type: optional<string>
        docs: Name of the key
      updated_at:
        type: optional<long>
        docs: Last updated time of the key
      user_id:
        type: optional<string>
        docs: User ID to whom the key belongs
    source:
      openapi: ../openapi.json
  AccessTokenScope:
    enum:
      - value: audio:speech
        name: AudioSpeech
      - value: audio:stream
        name: AudioStream
      - value: audio:all
        name: AudioAll
      - value: voices:read
        name: VoicesRead
      - value: voices:create
        name: VoicesCreate
      - value: voices:delete
        name: VoicesDelete
      - value: voices:all
        name: VoicesAll
    docs: The scope, or a space-delimited list of scopes the token is issued for
    inline: true
    source:
      openapi: ../openapi.json
  AccessToken:
    properties:
      access_token: optional<string>
      expires_in:
        type: optional<long>
        docs: Expiration time, in seconds from the issue time
      scope:
        type: optional<AccessTokenScope>
        docs: The scope, or a space-delimited list of scopes the token is issued for
      token_type:
        type: optional<literal<"bearer">>
        docs: Token type
    source:
      openapi: ../openapi.json
  CreateVoiceLanguage:
    properties:
      locale: optional<string>
      preview_audio: optional<string>
    source:
      openapi: ../openapi.json
  CreateVoiceModelName:
    enum:
      - value: simba-base
        name: SimbaBase
      - value: simba-english
        name: SimbaEnglish
      - value: simba-multilingual
        name: SimbaMultilingual
      - value: simba-turbo
        name: SimbaTurbo
    inline: true
    source:
      openapi: ../openapi.json
  CreateVoiceModel:
    properties:
      languages: optional<list<CreateVoiceLanguage>>
      name: optional<CreateVoiceModelName>
    source:
      openapi: ../openapi.json
  CreatedVoiceType:
    enum:
      - shared
      - personal
    inline: true
    source:
      openapi: ../openapi.json
  CreatedVoice:
    properties:
      display_name: optional<string>
      id: optional<string>
      models: optional<list<CreateVoiceModel>>
      type: optional<CreatedVoiceType>
    source:
      openapi: ../openapi.json
  ExperimentalStreamResponse:
    docs: ExperimentalStreamResponse represents generated audio stream info
    properties:
      audio_url:
        type: optional<string>
        docs: >-
          URL to the synthesized audio file. It includes the expiration time and
          a signature in the query params.

          The audio file will be available for download until the expiration
          time.

          For the URL to work correctly, it must be used verbatim, with all the
          query parameters.
      expires:
        type: optional<string>
        docs: Expiry time of the audio file, in ISO-8601 format.
    source:
      openapi: ../openapi.json
  GetSpeechOptionsRequest:
    docs: >-
      GetSpeechOptionsRequest is the wrapper for request parameters to the
      client
    properties:
      loudness_normalization:
        type: optional<boolean>
        docs: >-
          Determines whether to normalize the audio loudness to a standard
          level.

          When enabled, loudness normalization aligns the audio output to the
          following standards:

          Integrated loudness: -14 LUFS

          True peak: -2 dBTP

          Loudness range: 7 LU

          If disabled, the audio loudness will match the original loudness of
          the selected voice, which may vary significantly and be either too
          quiet or too loud.

          Enabling loudness normalization can increase latency due to additional
          processing required for audio level adjustments.
        default: true
    source:
      openapi: ../openapi.json
  GetSpeechResponseAudioFormat:
    enum:
      - wav
      - mp3
      - ogg
      - aac
    docs: The format of the audio data
    inline: true
    source:
      openapi: ../openapi.json
  GetSpeechResponse:
    properties:
      audio_data:
        type: optional<list<integer>>
        docs: Synthesized speech audio, Base64-encoded
      audio_format:
        type: optional<GetSpeechResponseAudioFormat>
        docs: The format of the audio data
      billable_characters_count:
        type: optional<long>
        docs: The number of billable characters processed in the request.
      speech_marks: optional<SpeechMarks>
    source:
      openapi: ../openapi.json
  GetStreamOptionsRequest:
    docs: >-
      GetStreamOptionsRequest is the wrapper for request parameters to the
      client
    properties:
      loudness_normalization:
        type: optional<boolean>
        docs: >-
          Determines whether to normalize the audio loudness to a standard
          level.

          When enabled, loudness normalization aligns the audio output to the
          following standards:

          Integrated loudness: -14 LUFS

          True peak: -2 dBTP

          Loudness range: 7 LU

          If disabled, the audio loudness will match the original loudness of
          the selected voice, which may vary significantly and be either too
          quiet or too loud.

          Enabling loudness normalization can increase latency due to additional
          processing required for audio level adjustments.
        default: true
    source:
      openapi: ../openapi.json
  GetVoiceGender:
    enum:
      - male
      - female
      - notSpecified
    inline: true
    source:
      openapi: ../openapi.json
  GetVoiceType:
    enum:
      - shared
      - personal
    inline: true
    source:
      openapi: ../openapi.json
  GetVoice:
    properties:
      avatar_image: optional<string>
      display_name: optional<string>
      gender: optional<GetVoiceGender>
      id: optional<string>
      models: optional<list<GetVoicesModel>>
      preview_audio: optional<string>
      tags: optional<list<string>>
      type: optional<GetVoiceType>
    source:
      openapi: ../openapi.json
  GetVoiceLanguage:
    properties:
      locale: optional<string>
      preview_audio: optional<string>
    source:
      openapi: ../openapi.json
  GetVoicesModelName:
    enum:
      - value: simba-base
        name: SimbaBase
      - value: simba-english
        name: SimbaEnglish
      - value: simba-multilingual
        name: SimbaMultilingual
      - value: simba-turbo
        name: SimbaTurbo
    inline: true
    source:
      openapi: ../openapi.json
  GetVoicesModel:
    properties:
      languages: optional<list<GetVoiceLanguage>>
      name: optional<GetVoicesModelName>
    source:
      openapi: ../openapi.json
  NestedChunk:
    docs: >-
      It details the type of segment, its start and end points in the text, and
      its start and end times in the synthesized speech audio.
    properties:
      end:
        type: optional<long>
      end_time:
        type: optional<double>
      start:
        type: optional<long>
      start_time:
        type: optional<double>
      type:
        type: optional<string>
      value:
        type: optional<string>
    source:
      openapi: ../openapi.json
  OAuthErrorError:
    enum:
      - invalid_client
      - unauthorized_client
      - invalid_request
      - unsupported_grant_type
      - invalid_scope
    inline: true
    source:
      openapi: ../openapi.json
  OAuthError:
    properties:
      error: optional<OAuthErrorError>
      error_description: optional<string>
    source:
      openapi: ../openapi.json
  SpeechMarks:
    docs: >-
      It is used to annotate the audio data with metadata about the synthesis
      process, like word timing or phoneme details.
    properties:
      chunks:
        type: optional<list<NestedChunk>>
        docs: >-
          Array of NestedChunk, each providing detailed segment information
          within the synthesized speech.
      end:
        type: optional<long>
      end_time:
        type: optional<double>
      start:
        type: optional<long>
      start_time:
        type: optional<double>
      type:
        type: optional<string>
      value:
        type: optional<string>
    source:
      openapi: ../openapi.json
",
    },
    "api.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "types": {
          "GetStreamRequestAccept": {
            "enum": [
              {
                "name": "AudioMpeg",
                "value": "audio/mpeg",
              },
              {
                "name": "AudioOgg",
                "value": "audio/ogg",
              },
              {
                "name": "AudioAac",
                "value": "audio/aac",
              },
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.json",
            },
          },
        },
      },
      "rawContents": "types:
  GetStreamRequestAccept:
    enum:
      - value: audio/mpeg
        name: AudioMpeg
      - value: audio/ogg
        name: AudioOgg
      - value: audio/aac
        name: AudioAac
    source:
      openapi: ../openapi.json
",
    },
    "apikey.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "CreateAPIKey": {
              "auth": true,
              "docs": "Create a new API key for the logged in user",
              "examples": [
                {
                  "response": {
                    "body": {
                      "api_key": "api_key",
                      "created_at": 1000000,
                      "id": 1000000,
                      "name": "name",
                      "updated_at": 1000000,
                      "user_id": "user_id",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v1/token",
              "response": {
                "docs": "APIKeyResponseWrapper contains the details of the API key which can be used by the user to access the API",
                "status-code": 200,
                "type": "root.ApiKey",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "DeleteAPIKey": {
              "auth": true,
              "docs": "Deletes the given API key for the logged in user",
              "examples": [
                {
                  "path-parameters": {
                    "id": "id",
                  },
                },
              ],
              "method": "DELETE",
              "pagination": undefined,
              "path": "/v1/token/{id}",
              "path-parameters": {
                "id": {
                  "docs": "The ID of the API key to delete",
                  "type": "string",
                },
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "ListAPIKeys": {
              "auth": true,
              "docs": "Fetches all the API keys for the logged in user",
              "examples": [
                {
                  "response": {
                    "body": [
                      {
                        "api_key": "api_key",
                        "created_at": 1000000,
                        "id": 1000000,
                        "name": "name",
                        "updated_at": 1000000,
                        "user_id": "user_id",
                      },
                    ],
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v1/token",
              "response": {
                "docs": "An array of all API keys for the user for the given request",
                "status-code": 200,
                "type": "list<root.ApiKey>",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "UpdateApiKey": {
              "auth": true,
              "docs": "Update API key name for the logged in user",
              "examples": [
                {
                  "path-parameters": {
                    "id": "id",
                  },
                  "request": "string",
                  "response": {
                    "body": {
                      "api_key": "api_key",
                      "created_at": 1000000,
                      "id": 1000000,
                      "name": "name",
                      "updated_at": 1000000,
                      "user_id": "user_id",
                    },
                  },
                },
              ],
              "method": "PATCH",
              "pagination": undefined,
              "path": "/v1/token/{id}",
              "path-parameters": {
                "id": {
                  "docs": "The ID of the key to edit",
                  "type": "string",
                },
              },
              "request": {
                "body": "string",
                "content-type": "application/json",
              },
              "response": {
                "docs": "APIKeyResponseWrapper contains the details of the API key which can be used by the user to access the API",
                "status-code": 200,
                "type": "root.ApiKey",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
          },
          "source": {
            "openapi": "../openapi.json",
          },
        },
      },
      "rawContents": "imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    ListAPIKeys:
      path: /v1/token
      method: GET
      auth: true
      docs: Fetches all the API keys for the logged in user
      source:
        openapi: ../openapi.json
      response:
        docs: An array of all API keys for the user for the given request
        type: list<root.ApiKey>
        status-code: 200
      examples:
        - response:
            body:
              - api_key: api_key
                created_at: 1000000
                id: 1000000
                name: name
                updated_at: 1000000
                user_id: user_id
    CreateAPIKey:
      path: /v1/token
      method: POST
      auth: true
      docs: Create a new API key for the logged in user
      source:
        openapi: ../openapi.json
      response:
        docs: >-
          APIKeyResponseWrapper contains the details of the API key which can be
          used by the user to access the API
        type: root.ApiKey
        status-code: 200
      examples:
        - response:
            body:
              api_key: api_key
              created_at: 1000000
              id: 1000000
              name: name
              updated_at: 1000000
              user_id: user_id
    DeleteAPIKey:
      path: /v1/token/{id}
      method: DELETE
      auth: true
      docs: Deletes the given API key for the logged in user
      source:
        openapi: ../openapi.json
      path-parameters:
        id:
          type: string
          docs: The ID of the API key to delete
      examples:
        - path-parameters:
            id: id
    UpdateApiKey:
      path: /v1/token/{id}
      method: PATCH
      auth: true
      docs: Update API key name for the logged in user
      source:
        openapi: ../openapi.json
      path-parameters:
        id:
          type: string
          docs: The ID of the key to edit
      request:
        body: string
        content-type: application/json
      response:
        docs: >-
          APIKeyResponseWrapper contains the details of the API key which can be
          used by the user to access the API
        type: root.ApiKey
        status-code: 200
      examples:
        - path-parameters:
            id: id
          request: string
          response:
            body:
              api_key: api_key
              created_at: 1000000
              id: 1000000
              name: name
              updated_at: 1000000
              user_id: user_id
  source:
    openapi: ../openapi.json
",
    },
    "audio.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "ExperimentalStreamCreate": {
              "auth": true,
              "docs": "Gets the stream speech for the given input",
              "errors": [
                "root.ExperimentalStreamCreateRequestBadRequestError",
                "root.ExperimentalStreamCreateRequestPaymentRequiredError",
                "root.ExperimentalStreamCreateRequestForbiddenError",
                "root.ExperimentalStreamCreateRequestInternalServerError",
              ],
              "examples": [
                {
                  "request": {
                    "input": "input",
                    "voice_id": "voice_id",
                  },
                  "response": {
                    "body": {
                      "audio_url": "https://streaming.sws.speechify.com/v1/stream/abc.mp3?exp=2024-07-26T10:54:44.429Z&s=xyz",
                      "expires": "2024-07-26T11:18:41.236Z",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/experimental/audio/stream",
              "request": {
                "body": {
                  "properties": {
                    "input": {
                      "docs": "Plain text or SSML to be synthesized to speech.
Refer to https://docs.sws.speechify.com/docs/api-limits for the input size limits.
Emotion, Pitch and Speed Rate are configured in the ssml input, please refer to the ssml documentation for more information: https://docs.sws.speechify.com/docs/ssml#prosody",
                      "type": "string",
                    },
                    "language": {
                      "docs": "Language of the input. Follow the format of an ISO 639-1 language code and an ISO 3166-1 region code, separated by a hyphen, e.g. en-US.
Please refer to the list of the supported languages and recommendations regarding this parameter: https://docs.sws.speechify.com/docs/language-support.",
                      "type": "optional<string>",
                    },
                    "model": {
                      "default": "simba-base",
                      "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
                      "type": "optional<ExperimentalStreamRequestModel>",
                    },
                    "voice_id": {
                      "docs": "Id of the voice to be used for synthesizing speech. Refer to /v1/voices endpoint for available voices",
                      "type": "string",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "ExperimentalStreamRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "root.ExperimentalStreamResponse",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "GetSpeech": {
              "auth": true,
              "docs": "Gets the speech data for the given input",
              "errors": [
                "root.GetSpeechRequestBadRequestError",
                "root.GetSpeechRequestPaymentRequiredError",
                "root.GetSpeechRequestForbiddenError",
                "root.GetSpeechRequestInternalServerError",
              ],
              "examples": [
                {
                  "request": {
                    "input": "input",
                    "voice_id": "voice_id",
                  },
                  "response": {
                    "body": {
                      "audio_data": [
                        1,
                      ],
                      "audio_format": "wav",
                      "billable_characters_count": 1000000,
                      "speech_marks": {
                        "chunks": [
                          {},
                        ],
                        "end": 1000000,
                        "end_time": 1.1,
                        "start": 1000000,
                        "start_time": 1.1,
                        "type": "type",
                        "value": "value",
                      },
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v1/audio/speech",
              "request": {
                "body": {
                  "properties": {
                    "audio_format": {
                      "default": "wav",
                      "docs": "The format for the output audio. Note, that the current default is "wav", but there's no guarantee it will not change in the future. We recommend always passing the specific param you expect.",
                      "type": "optional<GetSpeechRequestAudioFormat>",
                    },
                    "input": {
                      "docs": "Plain text or SSML to be synthesized to speech.
Refer to https://docs.sws.speechify.com/docs/api-limits for the input size limits.
Emotion, Pitch and Speed Rate are configured in the ssml input, please refer to the ssml documentation for more information: https://docs.sws.speechify.com/docs/ssml#prosody",
                      "type": "string",
                    },
                    "language": {
                      "docs": "Language of the input. Follow the format of an ISO 639-1 language code and an ISO 3166-1 region code, separated by a hyphen, e.g. en-US.
Please refer to the list of the supported languages and recommendations regarding this parameter: https://docs.sws.speechify.com/docs/language-support.",
                      "type": "optional<string>",
                    },
                    "model": {
                      "default": "simba-base",
                      "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
                      "type": "optional<GetSpeechRequestModel>",
                    },
                    "options": "optional<root.GetSpeechOptionsRequest>",
                    "voice_id": {
                      "docs": "Id of the voice to be used for synthesizing speech. Refer to /v1/voices endpoint for available voices",
                      "type": "string",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "GetSpeechRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "root.GetSpeechResponse",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "GetStream": {
              "auth": true,
              "docs": "Gets the stream speech for the given input",
              "errors": [
                "root.GetStreamRequestBadRequestError",
                "root.GetStreamRequestPaymentRequiredError",
                "root.GetStreamRequestForbiddenError",
                "root.GetStreamRequestInternalServerError",
              ],
              "examples": [
                {
                  "headers": {
                    "Accept": "audio/mpeg",
                  },
                  "request": {
                    "input": "input",
                    "voice_id": "voice_id",
                  },
                  "response": {
                    "body": [
                      1,
                    ],
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v1/audio/stream",
              "request": {
                "body": {
                  "properties": {
                    "input": {
                      "docs": "Plain text or SSML to be synthesized to speech.
Refer to https://docs.sws.speechify.com/docs/api-limits for the input size limits.
Emotion, Pitch and Speed Rate are configured in the ssml input, please refer to the ssml documentation for more information: https://docs.sws.speechify.com/docs/ssml#prosody",
                      "type": "string",
                    },
                    "language": {
                      "docs": "Language of the input. Follow the format of an ISO 639-1 language code and an ISO 3166-1 region code, separated by a hyphen, e.g. en-US.
Please refer to the list of the supported languages and recommendations regarding this parameter: https://docs.sws.speechify.com/docs/language-support.",
                      "type": "optional<string>",
                    },
                    "model": {
                      "default": "simba-base",
                      "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
                      "type": "optional<GetStreamRequestModel>",
                    },
                    "options": "optional<root.GetStreamOptionsRequest>",
                    "voice_id": {
                      "docs": "Id of the voice to be used for synthesizing speech. Refer to /v1/voices endpoint for available voices",
                      "type": "string",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": {
                  "Accept": {
                    "name": "accept",
                    "type": "GetStreamRequestAccept",
                  },
                },
                "name": "GetStreamRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "list<integer>",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
          },
          "source": {
            "openapi": "../openapi.json",
          },
        },
        "types": {
          "ExperimentalStreamRequestModel": {
            "default": "simba-base",
            "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
            "enum": [
              {
                "name": "SimbaBase",
                "value": "simba-base",
              },
              {
                "name": "SimbaEnglish",
                "value": "simba-english",
              },
              {
                "name": "SimbaMultilingual",
                "value": "simba-multilingual",
              },
              {
                "name": "SimbaTurbo",
                "value": "simba-turbo",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetSpeechRequestAudioFormat": {
            "default": "wav",
            "docs": "The format for the output audio. Note, that the current default is "wav", but there's no guarantee it will not change in the future. We recommend always passing the specific param you expect.",
            "enum": [
              "wav",
              "mp3",
              "ogg",
              "aac",
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetSpeechRequestModel": {
            "default": "simba-base",
            "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
            "enum": [
              {
                "name": "SimbaBase",
                "value": "simba-base",
              },
              {
                "name": "SimbaEnglish",
                "value": "simba-english",
              },
              {
                "name": "SimbaMultilingual",
                "value": "simba-multilingual",
              },
              {
                "name": "SimbaTurbo",
                "value": "simba-turbo",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetStreamRequestAccept": {
            "enum": [
              {
                "name": "AudioMpeg",
                "value": "audio/mpeg",
              },
              {
                "name": "AudioOgg",
                "value": "audio/ogg",
              },
              {
                "name": "AudioAac",
                "value": "audio/aac",
              },
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.json",
            },
          },
          "GetStreamRequestModel": {
            "default": "simba-base",
            "docs": "Model used for audio synthesis
simba-base ModelBase  ModelBase is deprecated. Use simba-english or simba-multilingual instead.  @deprecated
simba-english ModelEnglish
simba-multilingual ModelMultilingual
simba-turbo ModelTurbo",
            "enum": [
              {
                "name": "SimbaBase",
                "value": "simba-base",
              },
              {
                "name": "SimbaEnglish",
                "value": "simba-english",
              },
              {
                "name": "SimbaMultilingual",
                "value": "simba-multilingual",
              },
              {
                "name": "SimbaTurbo",
                "value": "simba-turbo",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
        },
      },
      "rawContents": "types:
  ExperimentalStreamRequestModel:
    enum:
      - value: simba-base
        name: SimbaBase
      - value: simba-english
        name: SimbaEnglish
      - value: simba-multilingual
        name: SimbaMultilingual
      - value: simba-turbo
        name: SimbaTurbo
    docs: >-
      Model used for audio synthesis

      simba-base ModelBase  ModelBase is deprecated. Use simba-english or
      simba-multilingual instead.  @deprecated

      simba-english ModelEnglish

      simba-multilingual ModelMultilingual

      simba-turbo ModelTurbo
    default: simba-base
    inline: true
    source:
      openapi: ../openapi.json
  GetSpeechRequestAudioFormat:
    enum:
      - wav
      - mp3
      - ogg
      - aac
    docs: >-
      The format for the output audio. Note, that the current default is "wav",
      but there's no guarantee it will not change in the future. We recommend
      always passing the specific param you expect.
    default: wav
    inline: true
    source:
      openapi: ../openapi.json
  GetSpeechRequestModel:
    enum:
      - value: simba-base
        name: SimbaBase
      - value: simba-english
        name: SimbaEnglish
      - value: simba-multilingual
        name: SimbaMultilingual
      - value: simba-turbo
        name: SimbaTurbo
    docs: >-
      Model used for audio synthesis

      simba-base ModelBase  ModelBase is deprecated. Use simba-english or
      simba-multilingual instead.  @deprecated

      simba-english ModelEnglish

      simba-multilingual ModelMultilingual

      simba-turbo ModelTurbo
    default: simba-base
    inline: true
    source:
      openapi: ../openapi.json
  GetStreamRequestAccept:
    enum:
      - value: audio/mpeg
        name: AudioMpeg
      - value: audio/ogg
        name: AudioOgg
      - value: audio/aac
        name: AudioAac
    source:
      openapi: ../openapi.json
  GetStreamRequestModel:
    enum:
      - value: simba-base
        name: SimbaBase
      - value: simba-english
        name: SimbaEnglish
      - value: simba-multilingual
        name: SimbaMultilingual
      - value: simba-turbo
        name: SimbaTurbo
    docs: >-
      Model used for audio synthesis

      simba-base ModelBase  ModelBase is deprecated. Use simba-english or
      simba-multilingual instead.  @deprecated

      simba-english ModelEnglish

      simba-multilingual ModelMultilingual

      simba-turbo ModelTurbo
    default: simba-base
    inline: true
    source:
      openapi: ../openapi.json
imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    ExperimentalStreamCreate:
      path: /experimental/audio/stream
      method: POST
      auth: true
      docs: Gets the stream speech for the given input
      source:
        openapi: ../openapi.json
      request:
        name: ExperimentalStreamRequest
        body:
          properties:
            input:
              type: string
              docs: >-
                Plain text or SSML to be synthesized to speech.

                Refer to https://docs.sws.speechify.com/docs/api-limits for the
                input size limits.

                Emotion, Pitch and Speed Rate are configured in the ssml input,
                please refer to the ssml documentation for more information:
                https://docs.sws.speechify.com/docs/ssml#prosody
            language:
              type: optional<string>
              docs: >-
                Language of the input. Follow the format of an ISO 639-1
                language code and an ISO 3166-1 region code, separated by a
                hyphen, e.g. en-US.

                Please refer to the list of the supported languages and
                recommendations regarding this parameter:
                https://docs.sws.speechify.com/docs/language-support.
            model:
              type: optional<ExperimentalStreamRequestModel>
              docs: >-
                Model used for audio synthesis

                simba-base ModelBase  ModelBase is deprecated. Use simba-english
                or simba-multilingual instead.  @deprecated

                simba-english ModelEnglish

                simba-multilingual ModelMultilingual

                simba-turbo ModelTurbo
              default: simba-base
            voice_id:
              type: string
              docs: >-
                Id of the voice to be used for synthesizing speech. Refer to
                /v1/voices endpoint for available voices
        content-type: application/json
      response:
        docs: ''
        type: root.ExperimentalStreamResponse
        status-code: 200
      errors:
        - root.ExperimentalStreamCreateRequestBadRequestError
        - root.ExperimentalStreamCreateRequestPaymentRequiredError
        - root.ExperimentalStreamCreateRequestForbiddenError
        - root.ExperimentalStreamCreateRequestInternalServerError
      examples:
        - request:
            input: input
            voice_id: voice_id
          response:
            body:
              audio_url: >-
                https://streaming.sws.speechify.com/v1/stream/abc.mp3?exp=2024-07-26T10:54:44.429Z&s=xyz
              expires: '2024-07-26T11:18:41.236Z'
    GetSpeech:
      path: /v1/audio/speech
      method: POST
      auth: true
      docs: Gets the speech data for the given input
      source:
        openapi: ../openapi.json
      request:
        name: GetSpeechRequest
        body:
          properties:
            audio_format:
              type: optional<GetSpeechRequestAudioFormat>
              docs: >-
                The format for the output audio. Note, that the current default
                is "wav", but there's no guarantee it will not change in the
                future. We recommend always passing the specific param you
                expect.
              default: wav
            input:
              type: string
              docs: >-
                Plain text or SSML to be synthesized to speech.

                Refer to https://docs.sws.speechify.com/docs/api-limits for the
                input size limits.

                Emotion, Pitch and Speed Rate are configured in the ssml input,
                please refer to the ssml documentation for more information:
                https://docs.sws.speechify.com/docs/ssml#prosody
            language:
              type: optional<string>
              docs: >-
                Language of the input. Follow the format of an ISO 639-1
                language code and an ISO 3166-1 region code, separated by a
                hyphen, e.g. en-US.

                Please refer to the list of the supported languages and
                recommendations regarding this parameter:
                https://docs.sws.speechify.com/docs/language-support.
            model:
              type: optional<GetSpeechRequestModel>
              docs: >-
                Model used for audio synthesis

                simba-base ModelBase  ModelBase is deprecated. Use simba-english
                or simba-multilingual instead.  @deprecated

                simba-english ModelEnglish

                simba-multilingual ModelMultilingual

                simba-turbo ModelTurbo
              default: simba-base
            options: optional<root.GetSpeechOptionsRequest>
            voice_id:
              type: string
              docs: >-
                Id of the voice to be used for synthesizing speech. Refer to
                /v1/voices endpoint for available voices
        content-type: application/json
      response:
        docs: ''
        type: root.GetSpeechResponse
        status-code: 200
      errors:
        - root.GetSpeechRequestBadRequestError
        - root.GetSpeechRequestPaymentRequiredError
        - root.GetSpeechRequestForbiddenError
        - root.GetSpeechRequestInternalServerError
      examples:
        - request:
            input: input
            voice_id: voice_id
          response:
            body:
              audio_data:
                - 1
              audio_format: wav
              billable_characters_count: 1000000
              speech_marks:
                chunks:
                  - {}
                end: 1000000
                end_time: 1.1
                start: 1000000
                start_time: 1.1
                type: type
                value: value
    GetStream:
      path: /v1/audio/stream
      method: POST
      auth: true
      docs: Gets the stream speech for the given input
      source:
        openapi: ../openapi.json
      request:
        name: GetStreamRequest
        headers:
          Accept:
            type: GetStreamRequestAccept
            name: accept
        body:
          properties:
            input:
              type: string
              docs: >-
                Plain text or SSML to be synthesized to speech.

                Refer to https://docs.sws.speechify.com/docs/api-limits for the
                input size limits.

                Emotion, Pitch and Speed Rate are configured in the ssml input,
                please refer to the ssml documentation for more information:
                https://docs.sws.speechify.com/docs/ssml#prosody
            language:
              type: optional<string>
              docs: >-
                Language of the input. Follow the format of an ISO 639-1
                language code and an ISO 3166-1 region code, separated by a
                hyphen, e.g. en-US.

                Please refer to the list of the supported languages and
                recommendations regarding this parameter:
                https://docs.sws.speechify.com/docs/language-support.
            model:
              type: optional<GetStreamRequestModel>
              docs: >-
                Model used for audio synthesis

                simba-base ModelBase  ModelBase is deprecated. Use simba-english
                or simba-multilingual instead.  @deprecated

                simba-english ModelEnglish

                simba-multilingual ModelMultilingual

                simba-turbo ModelTurbo
              default: simba-base
            options: optional<root.GetStreamOptionsRequest>
            voice_id:
              type: string
              docs: >-
                Id of the voice to be used for synthesizing speech. Refer to
                /v1/voices endpoint for available voices
        content-type: application/json
      response:
        docs: ''
        type: list<integer>
        status-code: 200
      errors:
        - root.GetStreamRequestBadRequestError
        - root.GetStreamRequestPaymentRequiredError
        - root.GetStreamRequestForbiddenError
        - root.GetStreamRequestInternalServerError
      examples:
        - headers:
            Accept: audio/mpeg
          request:
            input: input
            voice_id: voice_id
          response:
            body:
              - 1
  source:
    openapi: ../openapi.json
",
    },
    "auth.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "CreateAccessToken": {
              "auth": true,
              "docs": "Create a new API token for the logged in user",
              "errors": [
                "root.CreateAccessTokenRequestBadRequestError",
              ],
              "examples": [
                {
                  "request": {
                    "grant_type": "client_credentials",
                  },
                  "response": {
                    "body": {
                      "access_token": "access_token",
                      "expires_in": 1000000,
                      "scope": "audio:speech",
                      "token_type": "bearer",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v1/auth/token",
              "request": {
                "body": {
                  "properties": {
                    "grant_type": {
                      "docs": "in: body",
                      "type": "literal<"client_credentials">",
                    },
                    "scope": {
                      "docs": "The scope, or a space-delimited list of scopes the token is requested for
in: body",
                      "type": "optional<CreateAccessTokenRequestScope>",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "CreateAccessTokenRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "Contains the details of the token which can be used by the user to access the API",
                "status-code": 200,
                "type": "root.AccessToken",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
          },
          "source": {
            "openapi": "../openapi.json",
          },
        },
        "types": {
          "CreateAccessTokenRequestScope": {
            "docs": "The scope, or a space-delimited list of scopes the token is requested for
in: body",
            "enum": [
              {
                "name": "AudioSpeech",
                "value": "audio:speech",
              },
              {
                "name": "AudioStream",
                "value": "audio:stream",
              },
              {
                "name": "AudioAll",
                "value": "audio:all",
              },
              {
                "name": "VoicesRead",
                "value": "voices:read",
              },
              {
                "name": "VoicesCreate",
                "value": "voices:create",
              },
              {
                "name": "VoicesDelete",
                "value": "voices:delete",
              },
              {
                "name": "VoicesAll",
                "value": "voices:all",
              },
            ],
            "inline": true,
            "source": {
              "openapi": "../openapi.json",
            },
          },
        },
      },
      "rawContents": "types:
  CreateAccessTokenRequestScope:
    enum:
      - value: audio:speech
        name: AudioSpeech
      - value: audio:stream
        name: AudioStream
      - value: audio:all
        name: AudioAll
      - value: voices:read
        name: VoicesRead
      - value: voices:create
        name: VoicesCreate
      - value: voices:delete
        name: VoicesDelete
      - value: voices:all
        name: VoicesAll
    docs: |-
      The scope, or a space-delimited list of scopes the token is requested for
      in: body
    inline: true
    source:
      openapi: ../openapi.json
imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    CreateAccessToken:
      path: /v1/auth/token
      method: POST
      auth: true
      docs: Create a new API token for the logged in user
      source:
        openapi: ../openapi.json
      request:
        name: CreateAccessTokenRequest
        body:
          properties:
            grant_type:
              type: literal<"client_credentials">
              docs: 'in: body'
            scope:
              type: optional<CreateAccessTokenRequestScope>
              docs: >-
                The scope, or a space-delimited list of scopes the token is
                requested for

                in: body
        content-type: application/json
      response:
        docs: >-
          Contains the details of the token which can be used by the user to
          access the API
        type: root.AccessToken
        status-code: 200
      errors:
        - root.CreateAccessTokenRequestBadRequestError
      examples:
        - request:
            grant_type: client_credentials
          response:
            body:
              access_token: access_token
              expires_in: 1000000
              scope: audio:speech
              token_type: bearer
  source:
    openapi: ../openapi.json
",
    },
    "voices.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "CreateVoice": {
              "auth": true,
              "docs": "Create a personal (cloned) voice for the user",
              "errors": [
                "root.CreateVoiceRequestBadRequestError",
                "root.CreateVoiceRequestPaymentRequiredError",
                "root.CreateVoiceRequestInternalServerError",
              ],
              "examples": [
                {
                  "request": {
                    "consent": "consent",
                    "name": "name",
                  },
                  "response": {
                    "body": {
                      "display_name": "display_name",
                      "id": "id",
                      "models": [
                        {
                          "languages": [
                            {},
                          ],
                          "name": "simba-base",
                        },
                      ],
                      "type": "shared",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v1/voices",
              "request": {
                "body": {
                  "properties": {
                    "consent": {
                      "docs": "User consent information in JSON format: {"fullName": "", "email": ""}
For example, {"fullName": "John Doe", "email": "john@example.com"}
This should include the fullName and email of the consenting individual.",
                      "style": "form",
                      "type": "string",
                    },
                    "name": {
                      "docs": "Name of the personal voice",
                      "style": "form",
                      "type": "string",
                    },
                    "sample": {
                      "docs": "Audio sample file",
                      "type": "file",
                    },
                  },
                },
                "content-type": "multipart/form-data",
                "headers": undefined,
                "name": "CreateVoiceRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "A created voice",
                "status-code": 200,
                "type": "root.CreatedVoice",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "DeleteVoice": {
              "auth": true,
              "docs": "Delete a personal (cloned) voice",
              "errors": [
                "root.DeleteVoiceRequestBadRequestError",
                "root.DeleteVoiceRequestNotFoundError",
                "root.DeleteVoiceRequestInternalServerError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "id": "id",
                  },
                },
              ],
              "method": "DELETE",
              "pagination": undefined,
              "path": "/v1/voices/{id}",
              "path-parameters": {
                "id": {
                  "docs": "The ID of the voice to delete",
                  "type": "string",
                },
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
            "GetVoices": {
              "auth": true,
              "docs": "Gets the list of voices available for the user",
              "errors": [
                "root.GetVoicesRequestNotFoundError",
                "root.GetVoicesRequestInternalServerError",
              ],
              "examples": [
                {
                  "response": {
                    "body": [
                      {
                        "avatar_image": "avatar_image",
                        "display_name": "display_name",
                        "gender": "male",
                        "id": "id",
                        "models": [
                          {},
                        ],
                        "preview_audio": "preview_audio",
                        "tags": [
                          "tags",
                        ],
                        "type": "shared",
                      },
                    ],
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v1/voices",
              "response": {
                "docs": "A list of voices",
                "status-code": 200,
                "type": "list<root.GetVoice>",
              },
              "source": {
                "openapi": "../openapi.json",
              },
            },
          },
          "source": {
            "openapi": "../openapi.json",
          },
        },
      },
      "rawContents": "imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    GetVoices:
      path: /v1/voices
      method: GET
      auth: true
      docs: Gets the list of voices available for the user
      source:
        openapi: ../openapi.json
      response:
        docs: A list of voices
        type: list<root.GetVoice>
        status-code: 200
      errors:
        - root.GetVoicesRequestNotFoundError
        - root.GetVoicesRequestInternalServerError
      examples:
        - response:
            body:
              - avatar_image: avatar_image
                display_name: display_name
                gender: male
                id: id
                models:
                  - {}
                preview_audio: preview_audio
                tags:
                  - tags
                type: shared
    CreateVoice:
      path: /v1/voices
      method: POST
      auth: true
      docs: Create a personal (cloned) voice for the user
      source:
        openapi: ../openapi.json
      request:
        name: CreateVoiceRequest
        body:
          properties:
            name:
              type: string
              docs: Name of the personal voice
              style: form
            sample:
              type: file
              docs: Audio sample file
            consent:
              type: string
              docs: >-
                User consent information in JSON format: {"fullName": "",
                "email": ""}

                For example, {"fullName": "John Doe", "email":
                "john@example.com"}

                This should include the fullName and email of the consenting
                individual.
              style: form
        content-type: multipart/form-data
      response:
        docs: A created voice
        type: root.CreatedVoice
        status-code: 200
      errors:
        - root.CreateVoiceRequestBadRequestError
        - root.CreateVoiceRequestPaymentRequiredError
        - root.CreateVoiceRequestInternalServerError
      examples:
        - request:
            name: name
            consent: consent
          response:
            body:
              display_name: display_name
              id: id
              models:
                - languages:
                    - {}
                  name: simba-base
              type: shared
    DeleteVoice:
      path: /v1/voices/{id}
      method: DELETE
      auth: true
      docs: Delete a personal (cloned) voice
      source:
        openapi: ../openapi.json
      path-parameters:
        id:
          type: string
          docs: The ID of the voice to delete
      errors:
        - root.DeleteVoiceRequestBadRequestError
        - root.DeleteVoiceRequestNotFoundError
        - root.DeleteVoiceRequestInternalServerError
      examples:
        - path-parameters:
            id: id
  source:
    openapi: ../openapi.json
",
    },
  },
  "packageMarkers": {},
  "rootApiFile": {
    "contents": {
      "auth": "bearerAuth",
      "auth-schemes": {
        "bearerAuth": {
          "header": "Authorization",
          "name": "apiKey",
          "type": "string",
        },
      },
      "default-environment": "Default",
      "display-name": "Speechify API:",
      "environments": {
        "Default": "https://api.sws.speechify.com",
      },
      "error-discrimination": {
        "strategy": "status-code",
      },
      "headers": {
        "Authorization": {
          "env": undefined,
          "name": "authorization",
          "type": "string",
        },
      },
      "name": "api",
    },
    "defaultUrl": undefined,
    "rawContents": "name: api
error-discrimination:
  strategy: status-code
display-name: 'Speechify API:'
environments:
  Default: https://api.sws.speechify.com
default-environment: Default
auth-schemes:
  bearerAuth:
    header: Authorization
    name: apiKey
    type: string
auth: bearerAuth
headers:
  Authorization:
    type: string
    name: authorization
",
  },
}