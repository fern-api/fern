{
  "absoluteFilePath": "/DUMMY_PATH",
  "importedDefinitions": {},
  "namedDefinitionFiles": {
    "__package__.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "errors": {
          "BadRequestError": {
            "docs": "Bad Request",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {
                  "error": "error",
                },
              },
            ],
            "status-code": 400,
            "type": "Error",
          },
          "GatewayTimeoutError": {
            "docs": "Gateway Timeout",
            "status-code": 504,
            "type": "unknown",
          },
          "InternalServerError": {
            "docs": "An error occurred while processing the request",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {
                  "error": "error",
                },
              },
            ],
            "status-code": 500,
            "type": "Error",
          },
          "NotFoundError": {
            "docs": "Not found",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {
                  "error": "error",
                },
              },
            ],
            "status-code": 404,
            "type": "Error",
          },
          "ServiceUnavailableError": {
            "docs": "Service Unavailable",
            "status-code": 503,
            "type": "unknown",
          },
          "TooManyRequestsError": {
            "docs": "Too Many Requests",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {
                  "error": "error",
                },
              },
            ],
            "status-code": 429,
            "type": "Error",
          },
          "UnauthorizedError": {
            "docs": "Unauthorized",
            "examples": [
              {
                "docs": undefined,
                "name": undefined,
                "value": {
                  "error": "error",
                },
              },
            ],
            "status-code": 401,
            "type": "Error",
          },
        },
        "types": {
          "AudioIntelligenceModelStatus": {
            "docs": "Will be either success, or unavailable in the rare case that the model failed.",
            "enum": [
              "success",
              "unavailable",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "AutoHighlightResult": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "count": {
                "docs": "The total number of times the key phrase appears in the audio file",
                "type": "integer",
              },
              "rank": {
                "docs": "The total relevancy to the overall audio file of this key phrase - a greater number means more relevant",
                "type": "float",
              },
              "text": {
                "docs": "The text itself of the key phrase",
                "type": "string",
              },
              "timestamps": {
                "docs": "The timestamp of the of the key phrase",
                "type": "list<Timestamp>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "AutoHighlightsResult": {
            "docs": "An array of results for the Key Phrases model, if it was enabled during the transcription request.
See [Key phrases](https://www.assemblyai.com/docs/Models/key_phrases) for more information.
",
            "inline": undefined,
            "properties": {
              "results": {
                "docs": "A temporally-sequential array of Key Phrases",
                "type": "list<AutoHighlightResult>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Chapter": {
            "docs": "Chapter of the audio file",
            "inline": undefined,
            "properties": {
              "end": {
                "docs": "The starting time, in milliseconds, for the chapter",
                "type": "integer",
              },
              "gist": {
                "docs": "An ultra-short summary (just a few words) of the content spoken in the chapter",
                "type": "string",
              },
              "headline": {
                "docs": "A single sentence summary of the content spoken during the chapter",
                "type": "string",
              },
              "start": {
                "docs": "The starting time, in milliseconds, for the chapter",
                "type": "integer",
              },
              "summary": {
                "docs": "A one paragraph summary of the content spoken during the chapter",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "ContentSafetyLabel": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "The confidence score for the topic being discussed, from 0 to 1",
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "label": {
                "docs": "The label of the sensitive topic",
                "type": "string",
              },
              "severity": {
                "docs": "How severely the topic is discussed in the section, from 0 to 1",
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "ContentSafetyLabelResult": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "labels": {
                "docs": "An array of objects, one per sensitive topic that was detected in the section",
                "type": "list<ContentSafetyLabel>",
              },
              "sentences_idx_end": {
                "docs": "The sentence index at which the section ends",
                "type": "integer",
              },
              "sentences_idx_start": {
                "docs": "The sentence index at which the section begins",
                "type": "integer",
              },
              "severity_score_summary": {
                "docs": "A summary of the Content Moderation severity results for the entire audio file",
                "type": "map<string, SeverityScoreSummary>",
              },
              "summary": {
                "docs": "A summary of the Content Moderation confidence results for the entire audio file",
                "type": "map<string, double>",
              },
              "text": {
                "docs": "The transcript of the section flagged by the Content Moderation model",
                "type": "string",
              },
              "timestamp": {
                "docs": "Timestamp information for the section",
                "type": "Timestamp",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "CreateTranscriptOptionalParameters": {
            "docs": "The parameters for creating a transcript",
            "inline": undefined,
            "properties": {
              "audio_end_at": {
                "docs": "The point in time, in milliseconds, to stop transcribing in your media file",
                "type": "optional<integer>",
              },
              "audio_start_from": {
                "docs": "The point in time, in milliseconds, to begin transcription from in your media file",
                "type": "optional<integer>",
              },
              "auto_chapters": {
                "docs": "Enable [Auto Chapters](https://www.assemblyai.com/docs/Models/auto_chapters), can be true or false",
                "type": "optional<boolean>",
              },
              "auto_highlights": {
                "docs": "Whether Key Phrases was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "boost_param": {
                "docs": "The word boost parameter value, if provided in the transcription request.",
                "type": "optional<TranscriptBoostParam>",
              },
              "content_safety": {
                "docs": "Enable [Content Moderation](https://www.assemblyai.com/docs/Models/content_moderation), can be true or false",
                "type": "optional<boolean>",
              },
              "custom_spelling": {
                "docs": "Customize how words are spelled and formatted using to and from values",
                "type": "optional<list<TranscriptCustomSpelling>>",
              },
              "custom_topics": {
                "docs": "Whether custom topics was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "disfluencies": {
                "docs": "Transcribe Filler Words, like "umm", in your media file; can be true or false.",
                "type": "optional<boolean>",
              },
              "dual_channel": {
                "docs": "Enable [Dual Channel](https://assemblyai.com/docs/Models/speech_recognition#dual-channel-transcription) transcription, can be true or false.",
                "type": "optional<boolean>",
              },
              "entity_detection": {
                "docs": "Enable [Entity Detection](https://www.assemblyai.com/docs/Models/entity_detection), can be true or false",
                "type": "optional<boolean>",
              },
              "filter_profanity": {
                "docs": "Filter profanity from the transcribed text, can be true or false.",
                "type": "optional<boolean>",
              },
              "format_text": {
                "docs": "Enable Text Formatting, can be true or false.",
                "type": "optional<boolean>",
              },
              "iab_categories": {
                "docs": "Enable [Topic Detection](https://www.assemblyai.com/docs/Models/iab_classification), can be true or false",
                "type": "optional<boolean>",
              },
              "language_code": {
                "docs": "The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).
The default value is 'en_us'.
",
                "type": "optional<TranscriptLanguageCode>",
              },
              "language_detection": {
                "docs": "Whether [Automatic language detection](https://www.assemblyai.com/docs/Models/speech_recognition#automatic-language-detection) was enabled in the transcription request, either true or false.",
                "type": "optional<boolean>",
              },
              "punctuate": {
                "docs": "Enable Automatic Punctuation, can be true or false.",
                "type": "optional<boolean>",
              },
              "redact_pii": {
                "docs": "Redact PII from the transcribed text using the Redact PII model, can be true or false",
                "type": "optional<boolean>",
              },
              "redact_pii_audio": {
                "docs": "Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
                "type": "optional<boolean>",
              },
              "redact_pii_audio_quality": {
                "default": "mp3",
                "docs": "Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
                "type": "optional<string>",
              },
              "redact_pii_policies": {
                "docs": "The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
                "type": "optional<list<PiiPolicy>>",
              },
              "redact_pii_sub": {
                "docs": "The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
                "type": "optional<SubstitutionPolicy>",
              },
              "sentiment_analysis": {
                "docs": "Enable [Sentiment Analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis), can be true or false",
                "type": "optional<boolean>",
              },
              "speaker_labels": {
                "docs": "Enable [Speaker diarization](https://www.assemblyai.com/docs/Models/speaker_diarization), can be true or false",
                "type": "optional<boolean>",
              },
              "speakers_expected": {
                "docs": "Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/Models/speaker_diarization) for more details.",
                "type": "optional<integer>",
              },
              "speech_threshold": {
                "docs": "Reject audio files that contain less than this fraction of speech.
Valid values are in the range [0, 1] inclusive.
",
                "type": "optional<float>",
              },
              "summarization": {
                "docs": "Enable [Summarization](https://www.assemblyai.com/docs/Models/summarization), can be true or false",
                "type": "optional<boolean>",
              },
              "summary_model": {
                "docs": "The model to summarize the transcript",
                "type": "optional<SummaryModel>",
              },
              "summary_type": {
                "docs": "The type of summary",
                "type": "optional<SummaryType>",
              },
              "topics": {
                "docs": "The list of custom topics provided if custom topics was enabled in the transcription request",
                "type": "optional<list<string>>",
              },
              "webhook_auth_header_name": {
                "docs": "The header name which should be sent back with webhook calls, if provided in the transcription request.",
                "type": "optional<string>",
              },
              "webhook_auth_header_value": {
                "docs": "Defaults to null. Optionally allows a user to specify a header name and value to send back with a webhook call for added security.",
                "type": "optional<string>",
              },
              "webhook_url": {
                "docs": "The URL to which we send webhooks upon trancription completion, if provided in the transcription request.",
                "type": "optional<string>",
              },
              "word_boost": {
                "docs": "The list of custom vocabulary to boost transcription probability for, if provided in the transcription request.",
                "type": "optional<list<string>>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Entity": {
            "docs": "A detected entity",
            "inline": undefined,
            "properties": {
              "end": {
                "docs": "The ending time, in milliseconds, for the detected entity in the audio file",
                "type": "integer",
              },
              "entity_type": {
                "docs": "The type of entity for the detected entity",
                "type": "EntityType",
              },
              "start": {
                "docs": "The starting time, in milliseconds, at which the detected entity appears in the audio file",
                "type": "integer",
              },
              "text": {
                "docs": "The text for the detected entity",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "EntityType": {
            "docs": "The type of entity for the detected entity",
            "enum": [
              {
                "docs": "Banking information, including account and routing numbers",
                "value": "banking_information",
              },
              {
                "docs": "Blood type (e.g., O-, AB positive)",
                "value": "blood_type",
              },
              {
                "docs": "Credit card verification code (e.g., CVV: 080)
",
                "value": "credit_card_cvv",
              },
              {
                "docs": "Expiration date of a credit card",
                "value": "credit_card_expiration",
              },
              {
                "docs": "Credit card number",
                "value": "credit_card_number",
              },
              {
                "docs": "Specific calendar date (e.g., December 18)",
                "value": "date",
              },
              {
                "docs": "Date of Birth (e.g., Date of Birth: March 7, 1961)
",
                "value": "date_of_birth",
              },
              {
                "docs": "Driver's license number (e.g., DL #356933-540)
",
                "value": "drivers_license",
              },
              {
                "docs": "Medications, vitamins, or supplements (e.g., Advil, Acetaminophen, Panadol)",
                "value": "drug",
              },
              {
                "docs": "Email address (e.g., support@assemblyai.com)",
                "value": "email_address",
              },
              {
                "docs": "Name of an event or holiday (e.g., Olympics, Yom Kippur)",
                "value": "event",
              },
              {
                "docs": "Bodily injury (e.g., I broke my arm, I have a sprained wrist)",
                "value": "injury",
              },
              {
                "docs": "Name of a natural language (e.g., Spanish, French)",
                "value": "language",
              },
              {
                "docs": "Any location reference including mailing address, postal code, city, state, province, or country",
                "value": "location",
              },
              {
                "docs": "Name of a medical condition, disease, syndrome, deficit, or disorder (e.g., chronic fatigue syndrome, arrhythmia, depression)",
                "value": "medical_condition",
              },
              {
                "docs": "Medical process, including treatments, procedures, and tests (e.g., heart surgery, CT scan)",
                "value": "medical_process",
              },
              {
                "docs": "Name and/or amount of currency (e.g., 15 pesos, $94.50)",
                "value": "money_amount",
              },
              {
                "docs": "Terms indicating nationality, ethnicity, or race (e.g., American, Asian, Caucasian)",
                "value": "nationality",
              },
              {
                "docs": "Job title or profession (e.g., professor, actors, engineer, CPA)",
                "value": "occupation",
              },
              {
                "docs": "Name of an organization (e.g., CNN, McDonalds, University of Alaska)",
                "value": "organization",
              },
              {
                "docs": "Account passwords, PINs, access keys, or verification answers (e.g., 27%alfalfa, temp1234, My mother's maiden name is Smith)",
                "value": "password",
              },
              {
                "docs": "Number associated with an age (e.g., 27, 75)",
                "value": "person_age",
              },
              {
                "docs": "Name of a person (e.g., Bob, Doug Jones)",
                "value": "person_name",
              },
              {
                "docs": "Telephone or fax number",
                "value": "phone_number",
              },
              {
                "docs": "Terms referring to a political party, movement, or ideology (e.g., Republican, Liberal)",
                "value": "political_affiliation",
              },
              {
                "docs": "Terms indicating religious affiliation (e.g., Hindu, Catholic)",
                "value": "religion",
              },
              {
                "docs": "Expressions indicating clock times (e.g., 19:37:28, 10pm EST)",
                "value": "time",
              },
              {
                "docs": "Internet addresses (e.g., www.assemblyai.com)",
                "value": "url",
              },
              {
                "docs": "Social Security Number or equivalent",
                "value": "us_social_security_number",
              },
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Error": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "error": {
                "docs": "Error message",
                "type": "string",
              },
              "status": "optional<literal<"error">>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurActionItemsParameters": "LemurBaseParameters",
          "LemurActionItemsResponse": {
            "docs": undefined,
            "extends": [
              "LemurBaseResponse",
            ],
            "inline": undefined,
            "properties": {
              "response": {
                "docs": "The response generated by LeMUR.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurBaseParameters": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "context": {
                "docs": "Context to provide the model. This can be a string or a free-form JSON value.",
                "type": "optional<LemurBaseParametersContext>",
              },
              "final_model": "optional<LemurModel>",
              "max_output_size": {
                "docs": "Max output size in tokens. Up to 4000 allowed.",
                "type": "optional<integer>",
              },
              "temperature": {
                "docs": "The temperature to use for the model.
Higher values result in answers that are more creative, lower values are more conservative.
Can be any value between 0.0 and 1.0 inclusive.
",
                "type": "optional<float>",
              },
              "transcript_ids": {
                "docs": "A list of completed transcripts with text. Up to 100 files max, or 100 hours max. Whichever is lower.",
                "type": "list<string>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurBaseParametersContext": {
            "discriminated": false,
            "docs": "Context to provide the model. This can be a string or a free-form JSON value.",
            "encoding": undefined,
            "inline": true,
            "source": {
              "openapi": "../openapi.yml",
            },
            "union": [
              "string",
              "map<string, unknown>",
            ],
          },
          "LemurBaseResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "request_id": {
                "docs": "The ID of the LeMUR request",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurModel": {
            "docs": "The model that is used for the final prompt after compression is performed (options: "basic" and "default").
",
            "enum": [
              "default",
              "basic",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurQuestion": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "answer_format": {
                "docs": "How you want the answer to be returned. This can be any text. Can't be used with answer_options. Examples: "short sentence", "bullet points"
",
                "type": "optional<string>",
              },
              "answer_options": {
                "docs": "What discrete options to return. Useful for precise responses. Can't be used with answer_format. Example: ["Yes", "No"]
",
                "type": "optional<list<string>>",
              },
              "context": {
                "docs": "Any context about the transcripts you wish to provide. This can be a string, or free-form JSON.",
                "type": "optional<LemurQuestionContext>",
              },
              "question": {
                "docs": "The question you wish to ask. For more complex questions use default model.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurQuestionAnswer": {
            "docs": "An answer generated by LeMUR and its question.",
            "inline": undefined,
            "properties": {
              "answer": {
                "docs": "The answer generated by LeMUR.",
                "type": "string",
              },
              "question": {
                "docs": "The question for LeMUR to answer.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurQuestionAnswerResponse": {
            "docs": undefined,
            "extends": [
              "LemurBaseResponse",
            ],
            "inline": undefined,
            "properties": {
              "response": {
                "docs": "The answers generated by LeMUR and their questions.",
                "type": "list<LemurQuestionAnswer>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurQuestionContext": {
            "discriminated": false,
            "docs": "Any context about the transcripts you wish to provide. This can be a string, or free-form JSON.",
            "encoding": undefined,
            "inline": true,
            "source": {
              "openapi": "../openapi.yml",
            },
            "union": [
              "string",
              "map<string, unknown>",
            ],
          },
          "LemurSummaryResponse": {
            "docs": undefined,
            "extends": [
              "LemurBaseResponse",
            ],
            "inline": undefined,
            "properties": {
              "response": {
                "docs": "The response generated by LeMUR.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "LemurTaskResponse": {
            "docs": undefined,
            "extends": [
              "LemurBaseResponse",
            ],
            "inline": undefined,
            "properties": {
              "response": {
                "docs": "The response generated by LeMUR.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "PageDetails": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "current_url": "string",
              "limit": "integer",
              "next_url": "optional<string>",
              "prev_url": "string",
              "result_count": "integer",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "ParagraphsResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "audio_duration": "double",
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "id": "string",
              "paragraphs": "list<TranscriptParagraph>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "PiiPolicy": {
            "enum": [
              "medical_process",
              "medical_condition",
              "blood_type",
              "drug",
              "injury",
              "number_sequence",
              "email_address",
              "date_of_birth",
              "phone_number",
              "us_social_security_number",
              "credit_card_number",
              "credit_card_expiration",
              "credit_card_cvv",
              "date",
              "nationality",
              "event",
              "language",
              "location",
              "money_amount",
              "person_name",
              "person_age",
              "organization",
              "political_affiliation",
              "occupation",
              "religion",
              "drivers_license",
              "banking_information",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "PurgeLemurRequestDataResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "deleted": {
                "docs": "Whether the request data was deleted.",
                "type": "boolean",
              },
              "request_id": {
                "docs": "The ID of the LeMUR request",
                "type": "string",
              },
              "request_id_to_purge": {
                "docs": "The ID of the deletion request of the LeMUR request",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "RealtimeTemporaryTokenResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "token": {
                "docs": "The temporary authentication token for real-time transcription",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "RedactedAudioResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "redacted_audio_url": {
                "docs": "The URL of the redacted audio file",
                "type": "string",
              },
              "status": {
                "docs": "The status of the redacted audio",
                "type": "RedactedAudioStatus",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "RedactedAudioStatus": {
            "docs": "The status of the redacted audio",
            "type": "literal<"redacted_audio_ready">",
          },
          "SentencesResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "audio_duration": "double",
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "id": "string",
              "sentences": "list<TranscriptSentence>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Sentiment": {
            "enum": [
              "POSITIVE",
              "NEUTRAL",
              "NEGATIVE",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SentimentAnalysisResult": {
            "docs": "The result of the sentiment analysis model.",
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "The confidence score for the detected sentiment of the sentence, from 0 to 1",
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "end": {
                "docs": "The ending time, in milliseconds, of the sentence",
                "type": "integer",
              },
              "sentiment": {
                "docs": "The detected sentiment for the sentence, one of POSITIVE, NEUTRAL, NEGATIVE",
                "type": "Sentiment",
              },
              "speaker": {
                "docs": "The speaker of the sentence if Speaker Diarization is enabled, else null",
                "type": "optional<string>",
              },
              "start": {
                "docs": "The starting time, in milliseconds, of the sentence",
                "type": "integer",
              },
              "text": {
                "docs": "The transcript of the sentence",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SeverityScoreSummary": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "high": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "low": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "medium": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SubstitutionPolicy": {
            "docs": "The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
            "enum": [
              "entity_type",
              "hash",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SubtitleFormat": {
            "docs": "Format of the subtitles",
            "enum": [
              "srt",
              "vtt",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SummaryModel": {
            "default": "informative",
            "docs": "The model to summarize the transcript",
            "enum": [
              "informative",
              "conversational",
              "catchy",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "SummaryType": {
            "default": "bullets",
            "docs": "The type of summary",
            "enum": [
              "bullets",
              "bullets_verbose",
              "gist",
              "headline",
              "paragraph",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Timestamp": {
            "docs": "Timestamp containing a start and end property in milliseconds.",
            "inline": undefined,
            "properties": {
              "end": {
                "docs": "The end time in milliseconds",
                "type": "integer",
              },
              "start": {
                "docs": "The start time in milliseconds",
                "type": "integer",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TopicDetectionResult": {
            "docs": "THe result of the topic detection model.",
            "inline": undefined,
            "properties": {
              "labels": "optional<list<TopicDetectionResultLabelsItem>>",
              "text": {
                "docs": "The text in the transcript in which a detected topic occurs",
                "type": "string",
              },
              "timestamp": "optional<Timestamp>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TopicDetectionResultLabelsItem": {
            "docs": undefined,
            "inline": true,
            "properties": {
              "label": {
                "docs": "The IAB taxonomical label for the label of the detected topic, where > denotes supertopic/subtopic relationship",
                "type": "string",
              },
              "relevance": {
                "docs": "How relevant the detected topic is of a detected topic",
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "Transcript": {
            "docs": "A transcript object",
            "inline": undefined,
            "properties": {
              "acoustic_model": {
                "availability": "deprecated",
                "docs": "The acoustic model that was used for the transcription",
                "type": "string",
              },
              "audio_duration": {
                "docs": "The duration of this transcript object's media file, in seconds",
                "type": "optional<float>",
              },
              "audio_end_at": {
                "docs": "The point in time, in milliseconds, in the file at which the transcription was terminated, if provided in the transcription request",
                "type": "optional<integer>",
              },
              "audio_start_from": {
                "docs": "The point in time, in milliseconds, in the file at which the transcription was started, if provided in the transcription request",
                "type": "optional<integer>",
              },
              "audio_url": {
                "docs": "The URL of the media that was transcribed",
                "type": "string",
              },
              "auto_chapters": {
                "docs": "Enable [Auto Chapters](https://www.assemblyai.com/docs/Models/auto_chapters), can be true or false",
                "type": "optional<boolean>",
              },
              "auto_highlights": {
                "docs": "Whether Key Phrases was enabled in the transcription request, either true or false",
                "type": "boolean",
              },
              "auto_highlights_result": {
                "docs": "An array of results for the Key Phrases model, if it was enabled during the transcription request.
See [Key phrases](https://www.assemblyai.com/docs/Models/key_phrases) for more information.
",
                "type": "optional<AutoHighlightsResult>",
              },
              "boost_param": {
                "docs": "The word boost parameter value, if provided in the transcription request",
                "type": "optional<string>",
              },
              "chapters": {
                "docs": "An array of temporally sequential chapters for the audio file",
                "type": "optional<list<Chapter>>",
              },
              "confidence": {
                "docs": "The confidence score for the transcript, between 0.0 (low confidence) and 1.0 (high confidence)",
                "type": "optional<double>",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "content_safety": {
                "docs": "Enable [Content Moderation](https://www.assemblyai.com/docs/Models/content_moderation), can be true or false",
                "type": "optional<boolean>",
              },
              "content_safety_labels": {
                "docs": "An array of results for the Content Moderation model, if it was enabled during the transcription request.
See [Content moderation](https://www.assemblyai.com/docs/Models/content_moderation) for more information.
",
                "type": "optional<TranscriptContentSafetyLabels>",
              },
              "custom_spelling": {
                "docs": "Customize how words are spelled and formatted using to and from values",
                "type": "optional<list<TranscriptCustomSpelling>>",
              },
              "custom_topics": {
                "docs": "Whether custom topics was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "disfluencies": {
                "docs": "Transcribe Filler Words, like "umm", in your media file; can be true or false",
                "type": "optional<boolean>",
              },
              "dual_channel": {
                "docs": "Whether [Dual channel transcription](https://www.assemblyai.com/docs/Models/speech_recognition#dual-channel-transcription) was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "entities": {
                "docs": "An array of results for the Entity Detection model, if it was enabled during the transcription request.
See [Entity detection](https://www.assemblyai.com/docs/Models/entity_detection) for more information.
",
                "type": "optional<list<Entity>>",
              },
              "entity_detection": {
                "docs": "Enable [Entity Detection](https://www.assemblyai.com/docs/Models/entity_detection), can be true or false",
                "type": "optional<boolean>",
              },
              "error": {
                "docs": "Error message of why the transcript failed",
                "type": "optional<string>",
              },
              "filter_profanity": {
                "docs": "Whether [Profanity Filtering](https://www.assemblyai.com/docs/Models/speech_recognition#profanity-filtering) was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "format_text": {
                "docs": "Whether Text Formatting was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "iab_categories": {
                "docs": "Enable [Topic Detection](https://www.assemblyai.com/docs/Models/iab_classification), can be true or false",
                "type": "optional<boolean>",
              },
              "iab_categories_result": {
                "docs": "An array of results for the Topic Detection model, if it was enabled during the transcription request.
See [Topic Detection](https://www.assemblyai.com/docs/Models/iab_classification) for more information.
",
                "type": "optional<TranscriptIabCategoriesResult>",
              },
              "id": {
                "docs": "The unique identifier of your transcription",
                "type": "string",
              },
              "language_code": {
                "docs": "The language of your audio file.
Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).
The default value is 'en_us'.
",
                "type": "optional<TranscriptLanguageCode>",
              },
              "language_detection": {
                "docs": "Whether [Automatic language detection](https://www.assemblyai.com/docs/Models/speech_recognition#automatic-language-detection) was enabled in the transcription request, either true or false",
                "type": "optional<boolean>",
              },
              "language_model": {
                "availability": "deprecated",
                "docs": "The language model that was used for the transcription",
                "type": "string",
              },
              "punctuate": {
                "docs": "Whether Automatic Punctuation was enabled in the transcription request, either true or false.",
                "type": "optional<boolean>",
              },
              "redact_pii": {
                "docs": "Whether [PII Redaction](https://www.assemblyai.com/docs/Models/pii_redaction) was enabled in the transcription request, either true or false",
                "type": "boolean",
              },
              "redact_pii_audio": {
                "docs": "Whether a redacted version of the audio file was generated (enabled or disabled in the transcription request),
either true or false. See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more information.
",
                "type": "optional<boolean>",
              },
              "redact_pii_audio_quality": {
                "docs": "The audio quality of the PII-redacted audio file, if enabled in the transcription request.
See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more information.
",
                "type": "optional<string>",
              },
              "redact_pii_policies": {
                "docs": "The list of PII Redaction policies that were enabled, if PII Redaction is enabled.
See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more information.
",
                "type": "optional<list<PiiPolicy>>",
              },
              "redact_pii_sub": {
                "docs": "The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for more details.",
                "type": "optional<SubstitutionPolicy>",
              },
              "sentiment_analysis": {
                "docs": "Enable [Sentiment Analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis), can be true or false",
                "type": "optional<boolean>",
              },
              "sentiment_analysis_results": {
                "docs": "An array of results for the Sentiment Analysis model, if it was enabled during the transcription request.
See [Sentiment analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis) for more information.
",
                "type": "optional<list<SentimentAnalysisResult>>",
              },
              "speaker_labels": {
                "docs": "Enable [Speaker diarization](https://www.assemblyai.com/docs/Models/speaker_diarization), can be true or false",
                "type": "optional<boolean>",
              },
              "speakers_expected": {
                "docs": "Defaults to null. Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/Models/speaker_diarization) for more details.",
                "type": "optional<integer>",
              },
              "speech_threshold": {
                "docs": "Defaults to null. Reject audio files that contain less than this fraction of speech.
Valid values are in the range [0, 1] inclusive.
",
                "type": "optional<float>",
              },
              "speed_boost": {
                "availability": "deprecated",
                "docs": "Whether speed boost was enabled in the transcription request",
                "type": "optional<boolean>",
              },
              "status": {
                "docs": "The status of your transcription. Possible values are queued, processing, completed, or error.",
                "type": "TranscriptStatus",
              },
              "summarization": {
                "docs": "Whether [Summarization](https://www.assemblyai.com/docs/Models/summarization) was enabled in the transcription request, either true or false",
                "type": "boolean",
              },
              "summary": {
                "docs": "The generated summary of the media file, if [Summarization](https://www.assemblyai.com/docs/Models/summarization) was enabled in the transcription request",
                "type": "optional<string>",
              },
              "summary_model": {
                "docs": "The Summarization model used to generate the summary,
if [Summarization](https://www.assemblyai.com/docs/Models/summarization) was enabled in the transcription request
",
                "type": "optional<string>",
              },
              "summary_type": {
                "docs": "The type of summary generated, if [Summarization](https://www.assemblyai.com/docs/Models/summarization) was enabled in the transcription request",
                "type": "optional<string>",
              },
              "text": {
                "docs": "The textual transcript of your media file",
                "type": "optional<string>",
              },
              "throttled": {
                "docs": "True while a request is throttled and false when a request is no longer throttled",
                "type": "optional<boolean>",
              },
              "topics": {
                "docs": "The list of custom topics provided if custom topics was enabled in the transcription request",
                "type": "optional<list<string>>",
              },
              "utterances": {
                "docs": "When dual_channel or speaker_labels is enabled, a list of turn-by-turn utterance objects.
See [Speaker diarization](https://www.assemblyai.com/docs/Models/speaker_diarization) for more information.
",
                "type": "optional<list<TranscriptUtterance>>",
              },
              "webhook_auth": {
                "docs": "Whether webhook authentication details were provided in the transcription request",
                "type": "boolean",
              },
              "webhook_auth_header_name": {
                "docs": "The header name which should be sent back with webhook calls, if provided in the transcription request",
                "type": "optional<string>",
              },
              "webhook_status_code": {
                "docs": "The status code we received from your server when delivering your webhook, if a webhook URL was provided in the transcription request",
                "type": "optional<integer>",
              },
              "webhook_url": {
                "docs": "The URL to which we send webhooks upon trancription completion, if provided in the transcription request",
                "type": "optional<string>",
              },
              "word_boost": {
                "docs": "The list of custom vocabulary to boost transcription probability for, if provided in the transcription request",
                "type": "optional<list<string>>",
              },
              "words": {
                "docs": "An array of temporally-sequential word objects, one for each word in the transcript.
See [Speech recognition](https://www.assemblyai.com/docs/Models/speech_recognition) for more information.
",
                "type": "optional<list<TranscriptWord>>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptBoostParam": {
            "docs": "The word boost parameter value, if provided in the transcription request.",
            "enum": [
              "low",
              "default",
              "high",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptContentSafetyLabels": {
            "docs": "An array of results for the Content Moderation model, if it was enabled during the transcription request.
See [Content moderation](https://www.assemblyai.com/docs/Models/content_moderation) for more information.
",
            "inline": true,
            "properties": {
              "results": "list<ContentSafetyLabelResult>",
              "status": {
                "docs": "Will be either success, or unavailable in the rare case that the Content Safety Labels model failed.",
                "type": "AudioIntelligenceModelStatus",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptCustomSpelling": {
            "docs": "Object containing words or phrases to replace, and the word or phrase to replace with",
            "inline": undefined,
            "properties": {
              "from": {
                "docs": "Words or phrases to replace",
                "type": "list<string>",
              },
              "to": {
                "docs": "Word or phrase to replace with",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptIabCategoriesResult": {
            "docs": "An array of results for the Topic Detection model, if it was enabled during the transcription request.
See [Topic Detection](https://www.assemblyai.com/docs/Models/iab_classification) for more information.
",
            "inline": true,
            "properties": {
              "results": {
                "docs": "An array of results for the Topic Detection model.",
                "type": "list<TopicDetectionResult>",
              },
              "status": {
                "docs": "Will be either success, or unavailable in the rare case that the Content Moderation model failed.",
                "type": "AudioIntelligenceModelStatus",
              },
              "summary": {
                "docs": "The overall relevance of topic to the entire audio file",
                "type": "map<string, double>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptLanguageCode": {
            "default": "en_us",
            "docs": "The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).
The default value is 'en_us'.
",
            "enum": [
              "en",
              "en_au",
              "en_uk",
              "en_us",
              "es",
              "fr",
              "de",
              "it",
              "pt",
              "nl",
              "hi",
              "ja",
              "zh",
              "fi",
              "ko",
              "pl",
              "ru",
              "tr",
              "uk",
              "vi",
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptList": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "page_details": "PageDetails",
              "transcripts": "list<TranscriptListItem>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptListItem": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "audio_url": "string",
              "completed": "optional<datetime>",
              "created": "datetime",
              "id": "string",
              "resource_url": "string",
              "status": "TranscriptStatus",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptListParameters": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "after_id": {
                "docs": "Get transcripts that were created after this transcript ID",
                "type": "optional<string>",
              },
              "before_id": {
                "docs": "Get transcripts that were created before this transcript ID",
                "type": "optional<string>",
              },
              "created_on": {
                "docs": "Only get transcripts created on this date",
                "type": "optional<string>",
                "validation": {
                  "format": "date",
                  "maxLength": undefined,
                  "minLength": undefined,
                  "pattern": undefined,
                },
              },
              "limit": {
                "default": 10,
                "docs": "Maximum amount of transcripts to retrieve",
                "type": "optional<long>",
              },
              "status": {
                "docs": "Filter by transcript status",
                "type": "optional<TranscriptStatus>",
              },
              "throttled_only": {
                "docs": "Only get throttled transcripts, overrides the status filter",
                "type": "optional<boolean>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptParagraph": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "end": "integer",
              "start": "integer",
              "text": "string",
              "words": "list<TranscriptWord>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptSentence": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "end": "integer",
              "start": "integer",
              "text": "string",
              "words": "list<TranscriptWord>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptStatus": {
            "docs": "The status of your transcription. Possible values are queued, processing, completed, or error.",
            "enum": [
              {
                "docs": "The audio file is in the queue to be processed by the API.",
                "value": "queued",
              },
              {
                "docs": "The audio file is being processed by the API.",
                "value": "processing",
              },
              {
                "docs": "The transcription job has been completed successfully.",
                "value": "completed",
              },
              {
                "docs": "An error occurred while processing the audio file.",
                "value": "error",
              },
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptUtterance": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "channel": "string",
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "end": "integer",
              "start": "integer",
              "text": "string",
              "words": "list<TranscriptWord>",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "TranscriptWord": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "type": "double",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "end": "integer",
              "speaker": "optional<string>",
              "start": "integer",
              "text": "string",
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "UploadedFile": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "upload_url": {
                "docs": "A URL that points to your audio file, accessible only by AssemblyAI's servers",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "WordSearchMatch": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "count": {
                "docs": "The total amount of times the word is in the transcript",
                "type": "integer",
              },
              "indexes": {
                "docs": "An array of all index locations for that word within the `words` array of the completed transcript",
                "type": "list<integer>",
              },
              "text": {
                "docs": "The matched word",
                "type": "string",
              },
              "timestamps": {
                "docs": "An array of timestamps",
                "type": "list<WordSearchTimestamp>",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "WordSearchResponse": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "The ID of the transcript",
                "type": "string",
              },
              "matches": {
                "docs": "The matches of the search",
                "type": "list<WordSearchMatch>",
              },
              "total_count": {
                "docs": "The total count of all matched instances. For e.g., word 1 matched 2 times, and word 2 matched 3 times, `total_count` will equal 5.",
                "type": "integer",
              },
            },
            "source": {
              "openapi": "../openapi.yml",
            },
          },
          "WordSearchTimestamp": {
            "docs": "An array of timestamps structured as [`start_time`, `end_time`] in milliseconds",
            "type": "list<integer>",
          },
        },
      },
      "rawContents": "errors:
  BadRequestError:
    status-code: 400
    type: Error
    docs: Bad Request
    examples:
      - value:
          error: error
  UnauthorizedError:
    status-code: 401
    type: Error
    docs: Unauthorized
    examples:
      - value:
          error: error
  NotFoundError:
    status-code: 404
    type: Error
    docs: Not found
    examples:
      - value:
          error: error
  TooManyRequestsError:
    status-code: 429
    type: Error
    docs: Too Many Requests
    examples:
      - value:
          error: error
  InternalServerError:
    status-code: 500
    type: Error
    docs: An error occurred while processing the request
    examples:
      - value:
          error: error
  ServiceUnavailableError:
    status-code: 503
    type: unknown
    docs: Service Unavailable
  GatewayTimeoutError:
    status-code: 504
    type: unknown
    docs: Gateway Timeout
types:
  RedactedAudioResponse:
    properties:
      status:
        type: RedactedAudioStatus
        docs: The status of the redacted audio
      redacted_audio_url:
        type: string
        docs: The URL of the redacted audio file
    source:
      openapi: ../openapi.yml
  RedactedAudioStatus:
    type: literal<"redacted_audio_ready">
    docs: The status of the redacted audio
  SubtitleFormat:
    enum:
      - srt
      - vtt
    docs: Format of the subtitles
    source:
      openapi: ../openapi.yml
  WordSearchResponse:
    properties:
      id:
        type: string
        docs: The ID of the transcript
      total_count:
        type: integer
        docs: >-
          The total count of all matched instances. For e.g., word 1 matched 2
          times, and word 2 matched 3 times, `total_count` will equal 5.
      matches:
        docs: The matches of the search
        type: list<WordSearchMatch>
    source:
      openapi: ../openapi.yml
  WordSearchMatch:
    properties:
      text:
        type: string
        docs: The matched word
      count:
        type: integer
        docs: The total amount of times the word is in the transcript
      timestamps:
        docs: An array of timestamps
        type: list<WordSearchTimestamp>
      indexes:
        docs: >-
          An array of all index locations for that word within the `words` array
          of the completed transcript
        type: list<integer>
    source:
      openapi: ../openapi.yml
  WordSearchTimestamp:
    docs: >-
      An array of timestamps structured as [`start_time`, `end_time`] in
      milliseconds
    type: list<integer>
  Timestamp:
    docs: Timestamp containing a start and end property in milliseconds.
    properties:
      start:
        type: integer
        docs: The start time in milliseconds
      end:
        type: integer
        docs: The end time in milliseconds
    source:
      openapi: ../openapi.yml
  CreateTranscriptOptionalParameters:
    docs: The parameters for creating a transcript
    properties:
      language_code:
        type: optional<TranscriptLanguageCode>
        docs: >
          The language of your audio file. Possible values are found in
          [Supported
          Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).

          The default value is 'en_us'.
      punctuate:
        type: optional<boolean>
        docs: Enable Automatic Punctuation, can be true or false.
      format_text:
        type: optional<boolean>
        docs: Enable Text Formatting, can be true or false.
      dual_channel:
        type: optional<boolean>
        docs: >-
          Enable [Dual
          Channel](https://assemblyai.com/docs/Models/speech_recognition#dual-channel-transcription)
          transcription, can be true or false.
      webhook_url:
        type: optional<string>
        docs: >-
          The URL to which we send webhooks upon trancription completion, if
          provided in the transcription request.
      webhook_auth_header_name:
        type: optional<string>
        docs: >-
          The header name which should be sent back with webhook calls, if
          provided in the transcription request.
      webhook_auth_header_value:
        type: optional<string>
        docs: >-
          Defaults to null. Optionally allows a user to specify a header name
          and value to send back with a webhook call for added security.
      auto_highlights:
        type: optional<boolean>
        docs: >-
          Whether Key Phrases was enabled in the transcription request, either
          true or false
      audio_start_from:
        type: optional<integer>
        docs: >-
          The point in time, in milliseconds, to begin transcription from in
          your media file
      audio_end_at:
        type: optional<integer>
        docs: >-
          The point in time, in milliseconds, to stop transcribing in your media
          file
      word_boost:
        type: optional<list<string>>
        docs: >-
          The list of custom vocabulary to boost transcription probability for,
          if provided in the transcription request.
      boost_param:
        type: optional<TranscriptBoostParam>
        docs: >-
          The word boost parameter value, if provided in the transcription
          request.
      filter_profanity:
        type: optional<boolean>
        docs: Filter profanity from the transcribed text, can be true or false.
      redact_pii:
        type: optional<boolean>
        docs: >-
          Redact PII from the transcribed text using the Redact PII model, can
          be true or false
      redact_pii_audio:
        type: optional<boolean>
        docs: >-
          Generate a copy of the original media file with spoken PII "beeped"
          out, can be true or false. See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more details.
      redact_pii_audio_quality:
        type: optional<string>
        docs: >-
          Controls the filetype of the audio created by redact_pii_audio.
          Currently supports mp3 (default) and wav. See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more details.
        default: mp3
      redact_pii_policies:
        type: optional<list<PiiPolicy>>
        docs: >-
          The list of PII Redaction policies to enable. See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more details.
      redact_pii_sub:
        type: optional<SubstitutionPolicy>
        docs: >-
          The replacement logic for detected PII, can be "entity_type" or
          "hash". See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more details.
      speaker_labels:
        type: optional<boolean>
        docs: >-
          Enable [Speaker
          diarization](https://www.assemblyai.com/docs/Models/speaker_diarization),
          can be true or false
      speakers_expected:
        type: optional<integer>
        docs: >-
          Tells the speaker label model how many speakers it should attempt to
          identify, up to 10. See [Speaker
          diarization](https://www.assemblyai.com/docs/Models/speaker_diarization)
          for more details.
      content_safety:
        type: optional<boolean>
        docs: >-
          Enable [Content
          Moderation](https://www.assemblyai.com/docs/Models/content_moderation),
          can be true or false
      iab_categories:
        type: optional<boolean>
        docs: >-
          Enable [Topic
          Detection](https://www.assemblyai.com/docs/Models/iab_classification),
          can be true or false
      language_detection:
        type: optional<boolean>
        docs: >-
          Whether [Automatic language
          detection](https://www.assemblyai.com/docs/Models/speech_recognition#automatic-language-detection)
          was enabled in the transcription request, either true or false.
      custom_spelling:
        type: optional<list<TranscriptCustomSpelling>>
        docs: Customize how words are spelled and formatted using to and from values
      disfluencies:
        type: optional<boolean>
        docs: >-
          Transcribe Filler Words, like "umm", in your media file; can be true
          or false.
      sentiment_analysis:
        type: optional<boolean>
        docs: >-
          Enable [Sentiment
          Analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis),
          can be true or false
      auto_chapters:
        type: optional<boolean>
        docs: >-
          Enable [Auto
          Chapters](https://www.assemblyai.com/docs/Models/auto_chapters), can
          be true or false
      entity_detection:
        type: optional<boolean>
        docs: >-
          Enable [Entity
          Detection](https://www.assemblyai.com/docs/Models/entity_detection),
          can be true or false
      speech_threshold:
        type: optional<float>
        docs: |
          Reject audio files that contain less than this fraction of speech.
          Valid values are in the range [0, 1] inclusive.
      summarization:
        type: optional<boolean>
        docs: >-
          Enable
          [Summarization](https://www.assemblyai.com/docs/Models/summarization),
          can be true or false
      summary_model:
        type: optional<SummaryModel>
        docs: The model to summarize the transcript
      summary_type:
        type: optional<SummaryType>
        docs: The type of summary
      custom_topics:
        type: optional<boolean>
        docs: >-
          Whether custom topics was enabled in the transcription request, either
          true or false
      topics:
        type: optional<list<string>>
        docs: >-
          The list of custom topics provided if custom topics was enabled in the
          transcription request
    source:
      openapi: ../openapi.yml
  SummaryModel:
    enum:
      - informative
      - conversational
      - catchy
    docs: The model to summarize the transcript
    default: informative
    source:
      openapi: ../openapi.yml
  SummaryType:
    enum:
      - bullets
      - bullets_verbose
      - gist
      - headline
      - paragraph
    docs: The type of summary
    default: bullets
    source:
      openapi: ../openapi.yml
  TranscriptBoostParam:
    enum:
      - low
      - default
      - high
    docs: The word boost parameter value, if provided in the transcription request.
    source:
      openapi: ../openapi.yml
  TranscriptCustomSpelling:
    docs: >-
      Object containing words or phrases to replace, and the word or phrase to
      replace with
    properties:
      from:
        docs: Words or phrases to replace
        type: list<string>
      to:
        type: string
        docs: Word or phrase to replace with
    source:
      openapi: ../openapi.yml
  TranscriptUtterance:
    properties:
      channel: string
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      start: integer
      end: integer
      text: string
      words: list<TranscriptWord>
    source:
      openapi: ../openapi.yml
  SubstitutionPolicy:
    enum:
      - entity_type
      - hash
    docs: >-
      The replacement logic for detected PII, can be "entity_type" or "hash".
      See [PII redaction](https://www.assemblyai.com/docs/Models/pii_redaction)
      for more details.
    source:
      openapi: ../openapi.yml
  PiiPolicy:
    enum:
      - medical_process
      - medical_condition
      - blood_type
      - drug
      - injury
      - number_sequence
      - email_address
      - date_of_birth
      - phone_number
      - us_social_security_number
      - credit_card_number
      - credit_card_expiration
      - credit_card_cvv
      - date
      - nationality
      - event
      - language
      - location
      - money_amount
      - person_name
      - person_age
      - organization
      - political_affiliation
      - occupation
      - religion
      - drivers_license
      - banking_information
    source:
      openapi: ../openapi.yml
  TranscriptLanguageCode:
    enum:
      - en
      - en_au
      - en_uk
      - en_us
      - es
      - fr
      - de
      - it
      - pt
      - nl
      - hi
      - ja
      - zh
      - fi
      - ko
      - pl
      - ru
      - tr
      - uk
      - vi
    docs: >
      The language of your audio file. Possible values are found in [Supported
      Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).

      The default value is 'en_us'.
    default: en_us
    source:
      openapi: ../openapi.yml
  TranscriptStatus:
    enum:
      - value: queued
        docs: The audio file is in the queue to be processed by the API.
      - value: processing
        docs: The audio file is being processed by the API.
      - value: completed
        docs: The transcription job has been completed successfully.
      - value: error
        docs: An error occurred while processing the audio file.
    docs: >-
      The status of your transcription. Possible values are queued, processing,
      completed, or error.
    source:
      openapi: ../openapi.yml
  TranscriptContentSafetyLabels:
    docs: >
      An array of results for the Content Moderation model, if it was enabled
      during the transcription request.

      See [Content
      moderation](https://www.assemblyai.com/docs/Models/content_moderation) for
      more information.
    properties:
      status:
        type: AudioIntelligenceModelStatus
        docs: >-
          Will be either success, or unavailable in the rare case that the
          Content Safety Labels model failed.
      results: list<ContentSafetyLabelResult>
    source:
      openapi: ../openapi.yml
    inline: true
  TranscriptIabCategoriesResult:
    docs: >
      An array of results for the Topic Detection model, if it was enabled
      during the transcription request.

      See [Topic
      Detection](https://www.assemblyai.com/docs/Models/iab_classification) for
      more information.
    properties:
      status:
        type: AudioIntelligenceModelStatus
        docs: >-
          Will be either success, or unavailable in the rare case that the
          Content Moderation model failed.
      results:
        docs: An array of results for the Topic Detection model.
        type: list<TopicDetectionResult>
      summary:
        type: map<string, double>
        docs: The overall relevance of topic to the entire audio file
    source:
      openapi: ../openapi.yml
    inline: true
  Transcript:
    docs: A transcript object
    properties:
      id:
        type: string
        docs: The unique identifier of your transcription
      language_model:
        type: string
        docs: The language model that was used for the transcription
        availability: deprecated
      acoustic_model:
        type: string
        docs: The acoustic model that was used for the transcription
        availability: deprecated
      status:
        type: TranscriptStatus
        docs: >-
          The status of your transcription. Possible values are queued,
          processing, completed, or error.
      language_code:
        type: optional<TranscriptLanguageCode>
        docs: >
          The language of your audio file.

          Possible values are found in [Supported
          Languages](https://www.assemblyai.com/docs/Concepts/supported_languages).

          The default value is 'en_us'.
      audio_url:
        type: string
        docs: The URL of the media that was transcribed
      text:
        type: optional<string>
        docs: The textual transcript of your media file
      words:
        type: optional<list<TranscriptWord>>
        docs: >
          An array of temporally-sequential word objects, one for each word in
          the transcript.

          See [Speech
          recognition](https://www.assemblyai.com/docs/Models/speech_recognition)
          for more information.
      utterances:
        type: optional<list<TranscriptUtterance>>
        docs: >
          When dual_channel or speaker_labels is enabled, a list of turn-by-turn
          utterance objects.

          See [Speaker
          diarization](https://www.assemblyai.com/docs/Models/speaker_diarization)
          for more information.
      confidence:
        type: optional<double>
        docs: >-
          The confidence score for the transcript, between 0.0 (low confidence)
          and 1.0 (high confidence)
        validation:
          min: 0
          max: 1
      audio_duration:
        type: optional<float>
        docs: The duration of this transcript object's media file, in seconds
      punctuate:
        type: optional<boolean>
        docs: >-
          Whether Automatic Punctuation was enabled in the transcription
          request, either true or false.
      format_text:
        type: optional<boolean>
        docs: >-
          Whether Text Formatting was enabled in the transcription request,
          either true or false
      dual_channel:
        type: optional<boolean>
        docs: >-
          Whether [Dual channel
          transcription](https://www.assemblyai.com/docs/Models/speech_recognition#dual-channel-transcription)
          was enabled in the transcription request, either true or false
      webhook_url:
        type: optional<string>
        docs: >-
          The URL to which we send webhooks upon trancription completion, if
          provided in the transcription request
      webhook_status_code:
        type: optional<integer>
        docs: >-
          The status code we received from your server when delivering your
          webhook, if a webhook URL was provided in the transcription request
      webhook_auth:
        type: boolean
        docs: >-
          Whether webhook authentication details were provided in the
          transcription request
      webhook_auth_header_name:
        type: optional<string>
        docs: >-
          The header name which should be sent back with webhook calls, if
          provided in the transcription request
      speed_boost:
        type: optional<boolean>
        docs: Whether speed boost was enabled in the transcription request
        availability: deprecated
      auto_highlights:
        type: boolean
        docs: >-
          Whether Key Phrases was enabled in the transcription request, either
          true or false
      auto_highlights_result:
        type: optional<AutoHighlightsResult>
        docs: >
          An array of results for the Key Phrases model, if it was enabled
          during the transcription request.

          See [Key phrases](https://www.assemblyai.com/docs/Models/key_phrases)
          for more information.
      audio_start_from:
        type: optional<integer>
        docs: >-
          The point in time, in milliseconds, in the file at which the
          transcription was started, if provided in the transcription request
      audio_end_at:
        type: optional<integer>
        docs: >-
          The point in time, in milliseconds, in the file at which the
          transcription was terminated, if provided in the transcription request
      word_boost:
        type: optional<list<string>>
        docs: >-
          The list of custom vocabulary to boost transcription probability for,
          if provided in the transcription request
      boost_param:
        type: optional<string>
        docs: >-
          The word boost parameter value, if provided in the transcription
          request
      filter_profanity:
        type: optional<boolean>
        docs: >-
          Whether [Profanity
          Filtering](https://www.assemblyai.com/docs/Models/speech_recognition#profanity-filtering)
          was enabled in the transcription request, either true or false
      redact_pii:
        type: boolean
        docs: >-
          Whether [PII
          Redaction](https://www.assemblyai.com/docs/Models/pii_redaction) was
          enabled in the transcription request, either true or false
      redact_pii_audio:
        type: optional<boolean>
        docs: >
          Whether a redacted version of the audio file was generated (enabled or
          disabled in the transcription request),

          either true or false. See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more information.
      redact_pii_audio_quality:
        type: optional<string>
        docs: >
          The audio quality of the PII-redacted audio file, if enabled in the
          transcription request.

          See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more information.
      redact_pii_policies:
        type: optional<list<PiiPolicy>>
        docs: >
          The list of PII Redaction policies that were enabled, if PII Redaction
          is enabled.

          See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more information.
      redact_pii_sub:
        type: optional<SubstitutionPolicy>
        docs: >-
          The replacement logic for detected PII, can be "entity_type" or
          "hash". See [PII
          redaction](https://www.assemblyai.com/docs/Models/pii_redaction) for
          more details.
      speaker_labels:
        type: optional<boolean>
        docs: >-
          Enable [Speaker
          diarization](https://www.assemblyai.com/docs/Models/speaker_diarization),
          can be true or false
      speakers_expected:
        type: optional<integer>
        docs: >-
          Defaults to null. Tells the speaker label model how many speakers it
          should attempt to identify, up to 10. See [Speaker
          diarization](https://www.assemblyai.com/docs/Models/speaker_diarization)
          for more details.
      content_safety:
        type: optional<boolean>
        docs: >-
          Enable [Content
          Moderation](https://www.assemblyai.com/docs/Models/content_moderation),
          can be true or false
      content_safety_labels:
        type: optional<TranscriptContentSafetyLabels>
        docs: >
          An array of results for the Content Moderation model, if it was
          enabled during the transcription request.

          See [Content
          moderation](https://www.assemblyai.com/docs/Models/content_moderation)
          for more information.
      iab_categories:
        type: optional<boolean>
        docs: >-
          Enable [Topic
          Detection](https://www.assemblyai.com/docs/Models/iab_classification),
          can be true or false
      iab_categories_result:
        type: optional<TranscriptIabCategoriesResult>
        docs: >
          An array of results for the Topic Detection model, if it was enabled
          during the transcription request.

          See [Topic
          Detection](https://www.assemblyai.com/docs/Models/iab_classification)
          for more information.
      language_detection:
        type: optional<boolean>
        docs: >-
          Whether [Automatic language
          detection](https://www.assemblyai.com/docs/Models/speech_recognition#automatic-language-detection)
          was enabled in the transcription request, either true or false
      custom_spelling:
        type: optional<list<TranscriptCustomSpelling>>
        docs: Customize how words are spelled and formatted using to and from values
      auto_chapters:
        type: optional<boolean>
        docs: >-
          Enable [Auto
          Chapters](https://www.assemblyai.com/docs/Models/auto_chapters), can
          be true or false
      chapters:
        type: optional<list<Chapter>>
        docs: An array of temporally sequential chapters for the audio file
      summarization:
        type: boolean
        docs: >-
          Whether
          [Summarization](https://www.assemblyai.com/docs/Models/summarization)
          was enabled in the transcription request, either true or false
      summary_type:
        type: optional<string>
        docs: >-
          The type of summary generated, if
          [Summarization](https://www.assemblyai.com/docs/Models/summarization)
          was enabled in the transcription request
      summary_model:
        type: optional<string>
        docs: >
          The Summarization model used to generate the summary,

          if
          [Summarization](https://www.assemblyai.com/docs/Models/summarization)
          was enabled in the transcription request
      summary:
        type: optional<string>
        docs: >-
          The generated summary of the media file, if
          [Summarization](https://www.assemblyai.com/docs/Models/summarization)
          was enabled in the transcription request
      custom_topics:
        type: optional<boolean>
        docs: >-
          Whether custom topics was enabled in the transcription request, either
          true or false
      topics:
        type: optional<list<string>>
        docs: >-
          The list of custom topics provided if custom topics was enabled in the
          transcription request
      disfluencies:
        type: optional<boolean>
        docs: >-
          Transcribe Filler Words, like "umm", in your media file; can be true
          or false
      sentiment_analysis:
        type: optional<boolean>
        docs: >-
          Enable [Sentiment
          Analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis),
          can be true or false
      sentiment_analysis_results:
        type: optional<list<SentimentAnalysisResult>>
        docs: >
          An array of results for the Sentiment Analysis model, if it was
          enabled during the transcription request.

          See [Sentiment
          analysis](https://www.assemblyai.com/docs/Models/sentiment_analysis)
          for more information.
      entity_detection:
        type: optional<boolean>
        docs: >-
          Enable [Entity
          Detection](https://www.assemblyai.com/docs/Models/entity_detection),
          can be true or false
      entities:
        type: optional<list<Entity>>
        docs: >
          An array of results for the Entity Detection model, if it was enabled
          during the transcription request.

          See [Entity
          detection](https://www.assemblyai.com/docs/Models/entity_detection)
          for more information.
      speech_threshold:
        type: optional<float>
        docs: >
          Defaults to null. Reject audio files that contain less than this
          fraction of speech.

          Valid values are in the range [0, 1] inclusive.
      throttled:
        type: optional<boolean>
        docs: >-
          True while a request is throttled and false when a request is no
          longer throttled
      error:
        type: optional<string>
        docs: Error message of why the transcript failed
    source:
      openapi: ../openapi.yml
  Chapter:
    docs: Chapter of the audio file
    properties:
      gist:
        type: string
        docs: >-
          An ultra-short summary (just a few words) of the content spoken in the
          chapter
      headline:
        type: string
        docs: A single sentence summary of the content spoken during the chapter
      summary:
        type: string
        docs: A one paragraph summary of the content spoken during the chapter
      start:
        type: integer
        docs: The starting time, in milliseconds, for the chapter
      end:
        type: integer
        docs: The starting time, in milliseconds, for the chapter
    source:
      openapi: ../openapi.yml
  Entity:
    docs: A detected entity
    properties:
      entity_type:
        type: EntityType
        docs: The type of entity for the detected entity
      text:
        type: string
        docs: The text for the detected entity
      start:
        type: integer
        docs: >-
          The starting time, in milliseconds, at which the detected entity
          appears in the audio file
      end:
        type: integer
        docs: >-
          The ending time, in milliseconds, for the detected entity in the audio
          file
    source:
      openapi: ../openapi.yml
  EntityType:
    enum:
      - value: banking_information
        docs: Banking information, including account and routing numbers
      - value: blood_type
        docs: Blood type (e.g., O-, AB positive)
      - value: credit_card_cvv
        docs: |
          Credit card verification code (e.g., CVV: 080)
      - value: credit_card_expiration
        docs: Expiration date of a credit card
      - value: credit_card_number
        docs: Credit card number
      - value: date
        docs: Specific calendar date (e.g., December 18)
      - value: date_of_birth
        docs: |
          Date of Birth (e.g., Date of Birth: March 7, 1961)
      - value: drivers_license
        docs: |
          Driver's license number (e.g., DL #356933-540)
      - value: drug
        docs: >-
          Medications, vitamins, or supplements (e.g., Advil, Acetaminophen,
          Panadol)
      - value: email_address
        docs: Email address (e.g., support@assemblyai.com)
      - value: event
        docs: Name of an event or holiday (e.g., Olympics, Yom Kippur)
      - value: injury
        docs: Bodily injury (e.g., I broke my arm, I have a sprained wrist)
      - value: language
        docs: Name of a natural language (e.g., Spanish, French)
      - value: location
        docs: >-
          Any location reference including mailing address, postal code, city,
          state, province, or country
      - value: medical_condition
        docs: >-
          Name of a medical condition, disease, syndrome, deficit, or disorder
          (e.g., chronic fatigue syndrome, arrhythmia, depression)
      - value: medical_process
        docs: >-
          Medical process, including treatments, procedures, and tests (e.g.,
          heart surgery, CT scan)
      - value: money_amount
        docs: Name and/or amount of currency (e.g., 15 pesos, $94.50)
      - value: nationality
        docs: >-
          Terms indicating nationality, ethnicity, or race (e.g., American,
          Asian, Caucasian)
      - value: occupation
        docs: Job title or profession (e.g., professor, actors, engineer, CPA)
      - value: organization
        docs: Name of an organization (e.g., CNN, McDonalds, University of Alaska)
      - value: password
        docs: >-
          Account passwords, PINs, access keys, or verification answers (e.g.,
          27%alfalfa, temp1234, My mother's maiden name is Smith)
      - value: person_age
        docs: Number associated with an age (e.g., 27, 75)
      - value: person_name
        docs: Name of a person (e.g., Bob, Doug Jones)
      - value: phone_number
        docs: Telephone or fax number
      - value: political_affiliation
        docs: >-
          Terms referring to a political party, movement, or ideology (e.g.,
          Republican, Liberal)
      - value: religion
        docs: Terms indicating religious affiliation (e.g., Hindu, Catholic)
      - value: time
        docs: Expressions indicating clock times (e.g., 19:37:28, 10pm EST)
      - value: url
        docs: Internet addresses (e.g., www.assemblyai.com)
      - value: us_social_security_number
        docs: Social Security Number or equivalent
    docs: The type of entity for the detected entity
    source:
      openapi: ../openapi.yml
  SentimentAnalysisResult:
    docs: The result of the sentiment analysis model.
    properties:
      text:
        type: string
        docs: The transcript of the sentence
      start:
        type: integer
        docs: The starting time, in milliseconds, of the sentence
      end:
        type: integer
        docs: The ending time, in milliseconds, of the sentence
      sentiment:
        type: Sentiment
        docs: >-
          The detected sentiment for the sentence, one of POSITIVE, NEUTRAL,
          NEGATIVE
      confidence:
        type: double
        docs: >-
          The confidence score for the detected sentiment of the sentence, from
          0 to 1
        validation:
          min: 0
          max: 1
      speaker:
        type: optional<string>
        docs: >-
          The speaker of the sentence if Speaker Diarization is enabled, else
          null
    source:
      openapi: ../openapi.yml
  Sentiment:
    enum:
      - POSITIVE
      - NEUTRAL
      - NEGATIVE
    source:
      openapi: ../openapi.yml
  TopicDetectionResultLabelsItem:
    properties:
      relevance:
        type: double
        docs: How relevant the detected topic is of a detected topic
        validation:
          min: 0
          max: 1
      label:
        type: string
        docs: >-
          The IAB taxonomical label for the label of the detected topic, where >
          denotes supertopic/subtopic relationship
    source:
      openapi: ../openapi.yml
    inline: true
  TopicDetectionResult:
    docs: THe result of the topic detection model.
    properties:
      text:
        type: string
        docs: The text in the transcript in which a detected topic occurs
      labels: optional<list<TopicDetectionResultLabelsItem>>
      timestamp: optional<Timestamp>
    source:
      openapi: ../openapi.yml
  ContentSafetyLabel:
    properties:
      label:
        type: string
        docs: The label of the sensitive topic
      confidence:
        type: double
        docs: The confidence score for the topic being discussed, from 0 to 1
        validation:
          min: 0
          max: 1
      severity:
        type: double
        docs: How severely the topic is discussed in the section, from 0 to 1
        validation:
          min: 0
          max: 1
    source:
      openapi: ../openapi.yml
  ContentSafetyLabelResult:
    properties:
      text:
        type: string
        docs: The transcript of the section flagged by the Content Moderation model
      labels:
        docs: >-
          An array of objects, one per sensitive topic that was detected in the
          section
        type: list<ContentSafetyLabel>
      sentences_idx_start:
        type: integer
        docs: The sentence index at which the section begins
      sentences_idx_end:
        type: integer
        docs: The sentence index at which the section ends
      timestamp:
        type: Timestamp
        docs: Timestamp information for the section
      summary:
        type: map<string, double>
        docs: >-
          A summary of the Content Moderation confidence results for the entire
          audio file
      severity_score_summary:
        type: map<string, SeverityScoreSummary>
        docs: >-
          A summary of the Content Moderation severity results for the entire
          audio file
    source:
      openapi: ../openapi.yml
  SeverityScoreSummary:
    properties:
      low:
        type: double
        validation:
          min: 0
          max: 1
      medium:
        type: double
        validation:
          min: 0
          max: 1
      high:
        type: double
        validation:
          min: 0
          max: 1
    source:
      openapi: ../openapi.yml
  AutoHighlightsResult:
    docs: >
      An array of results for the Key Phrases model, if it was enabled during
      the transcription request.

      See [Key phrases](https://www.assemblyai.com/docs/Models/key_phrases) for
      more information.
    properties:
      results:
        docs: A temporally-sequential array of Key Phrases
        type: list<AutoHighlightResult>
    source:
      openapi: ../openapi.yml
  AutoHighlightResult:
    properties:
      count:
        type: integer
        docs: The total number of times the key phrase appears in the audio file
      rank:
        type: float
        docs: >-
          The total relevancy to the overall audio file of this key phrase - a
          greater number means more relevant
      text:
        type: string
        docs: The text itself of the key phrase
      timestamps:
        docs: The timestamp of the of the key phrase
        type: list<Timestamp>
    source:
      openapi: ../openapi.yml
  TranscriptWord:
    properties:
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      start: integer
      end: integer
      text: string
      speaker: optional<string>
    source:
      openapi: ../openapi.yml
  TranscriptSentence:
    properties:
      text: string
      start: integer
      end: integer
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      words: list<TranscriptWord>
    source:
      openapi: ../openapi.yml
  SentencesResponse:
    properties:
      id: string
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      audio_duration: double
      sentences: list<TranscriptSentence>
    source:
      openapi: ../openapi.yml
  TranscriptParagraph:
    properties:
      text: string
      start: integer
      end: integer
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      words: list<TranscriptWord>
    source:
      openapi: ../openapi.yml
  ParagraphsResponse:
    properties:
      id: string
      confidence:
        type: double
        validation:
          min: 0
          max: 1
      audio_duration: double
      paragraphs: list<TranscriptParagraph>
    source:
      openapi: ../openapi.yml
  PageDetails:
    properties:
      limit: integer
      result_count: integer
      current_url: string
      prev_url: string
      next_url: optional<string>
    source:
      openapi: ../openapi.yml
  TranscriptListParameters:
    properties:
      limit:
        type: optional<long>
        docs: Maximum amount of transcripts to retrieve
        default: 10
      status:
        type: optional<TranscriptStatus>
        docs: Filter by transcript status
      created_on:
        type: optional<string>
        docs: Only get transcripts created on this date
        validation:
          format: date
      before_id:
        type: optional<string>
        docs: Get transcripts that were created before this transcript ID
      after_id:
        type: optional<string>
        docs: Get transcripts that were created after this transcript ID
      throttled_only:
        type: optional<boolean>
        docs: Only get throttled transcripts, overrides the status filter
    source:
      openapi: ../openapi.yml
  TranscriptListItem:
    properties:
      id: string
      resource_url: string
      status: TranscriptStatus
      created: datetime
      completed: optional<datetime>
      audio_url: string
    source:
      openapi: ../openapi.yml
  TranscriptList:
    properties:
      page_details: PageDetails
      transcripts: list<TranscriptListItem>
    source:
      openapi: ../openapi.yml
  UploadedFile:
    properties:
      upload_url:
        type: string
        docs: >-
          A URL that points to your audio file, accessible only by AssemblyAI's
          servers
    source:
      openapi: ../openapi.yml
  RealtimeTemporaryTokenResponse:
    properties:
      token:
        type: string
        docs: The temporary authentication token for real-time transcription
    source:
      openapi: ../openapi.yml
  AudioIntelligenceModelStatus:
    enum:
      - success
      - unavailable
    docs: >-
      Will be either success, or unavailable in the rare case that the model
      failed.
    source:
      openapi: ../openapi.yml
  PurgeLemurRequestDataResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the LeMUR request
      request_id_to_purge:
        type: string
        docs: The ID of the deletion request of the LeMUR request
      deleted:
        type: boolean
        docs: Whether the request data was deleted.
    source:
      openapi: ../openapi.yml
  LemurBaseResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the LeMUR request
    source:
      openapi: ../openapi.yml
  LemurSummaryResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR.
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswerResponse:
    properties:
      response:
        docs: The answers generated by LeMUR and their questions.
        type: list<LemurQuestionAnswer>
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswer:
    docs: An answer generated by LeMUR and its question.
    properties:
      question:
        type: string
        docs: The question for LeMUR to answer.
      answer:
        type: string
        docs: The answer generated by LeMUR.
    source:
      openapi: ../openapi.yml
  LemurActionItemsResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR.
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurTaskResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR.
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurBaseParametersContext:
    discriminated: false
    docs: >-
      Context to provide the model. This can be a string or a free-form JSON
      value.
    union:
      - string
      - map<string, unknown>
    source:
      openapi: ../openapi.yml
    inline: true
  LemurBaseParameters:
    properties:
      transcript_ids:
        docs: >-
          A list of completed transcripts with text. Up to 100 files max, or 100
          hours max. Whichever is lower.
        type: list<string>
      context:
        type: optional<LemurBaseParametersContext>
        docs: >-
          Context to provide the model. This can be a string or a free-form JSON
          value.
      final_model: optional<LemurModel>
      max_output_size:
        type: optional<integer>
        docs: Max output size in tokens. Up to 4000 allowed.
      temperature:
        type: optional<float>
        docs: >
          The temperature to use for the model.

          Higher values result in answers that are more creative, lower values
          are more conservative.

          Can be any value between 0.0 and 1.0 inclusive.
    source:
      openapi: ../openapi.yml
  LemurQuestionContext:
    discriminated: false
    docs: >-
      Any context about the transcripts you wish to provide. This can be a
      string, or free-form JSON.
    union:
      - string
      - map<string, unknown>
    source:
      openapi: ../openapi.yml
    inline: true
  LemurQuestion:
    properties:
      question:
        type: string
        docs: >-
          The question you wish to ask. For more complex questions use default
          model.
      context:
        type: optional<LemurQuestionContext>
        docs: >-
          Any context about the transcripts you wish to provide. This can be a
          string, or free-form JSON.
      answer_format:
        type: optional<string>
        docs: >
          How you want the answer to be returned. This can be any text. Can't be
          used with answer_options. Examples: "short sentence", "bullet points"
      answer_options:
        type: optional<list<string>>
        docs: >
          What discrete options to return. Useful for precise responses. Can't
          be used with answer_format. Example: ["Yes", "No"]
    source:
      openapi: ../openapi.yml
  LemurActionItemsParameters: LemurBaseParameters
  LemurModel:
    enum:
      - default
      - basic
    docs: >
      The model that is used for the final prompt after compression is performed
      (options: "basic" and "default").
    source:
      openapi: ../openapi.yml
  Error:
    properties:
      error:
        type: string
        docs: Error message
      status: optional<literal<"error">>
    source:
      openapi: ../openapi.yml
",
    },
    "files.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "upload": {
              "auth": true,
              "display-name": "Upload an audio or video file which can be transcribed.",
              "docs": "Upload your audio or video file directly to the AssemblyAI API if it isn't accessible via a URL already.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v2/upload",
              "request": {
                "body": "bytes",
                "content-type": "application/octet-stream",
              },
              "response": {
                "docs": "File uploaded successfully",
                "type": "root.UploadedFile",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yml",
          },
        },
      },
      "rawContents": "imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    upload:
      path: /v2/upload
      method: POST
      auth: true
      docs: >-
        Upload your audio or video file directly to the AssemblyAI API if it
        isn't accessible via a URL already.
      source:
        openapi: ../openapi.yml
      display-name: Upload an audio or video file which can be transcribed.
      request:
        body: bytes
        content-type: application/octet-stream
      response:
        docs: File uploaded successfully
        type: root.UploadedFile
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
  source:
    openapi: ../openapi.yml
",
    },
    "lemur.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "docs": "LeMUR related operations",
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "display-name": "LeMUR",
          "endpoints": {
            "actionItems": {
              "auth": true,
              "display-name": "Extract action items from one or more meeting transcripts.",
              "docs": "Use LeMUR to generate a list of Action Items from a transcript",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "transcript_ids": [
                      "transcript_ids",
                    ],
                  },
                  "response": {
                    "body": {
                      "request_id": "request_id",
                      "response": "response",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/lemur/v3/generate/action-items",
              "request": {
                "body": "root.LemurActionItemsParameters",
                "content-type": "application/json",
              },
              "response": {
                "docs": "LeMUR action items response",
                "type": "root.LemurActionItemsResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "purgeRequestData": {
              "auth": true,
              "display-name": "Delete the data for a previously submitted LeMUR request.",
              "docs": "Delete the data for a previously submitted LeMUR request.
The LLM response data, as well as any context provided in the original request will be removed.
",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "request_id": "request_id",
                  },
                  "response": {
                    "body": {
                      "deleted": true,
                      "request_id": "request_id",
                      "request_id_to_purge": "request_id_to_purge",
                    },
                  },
                },
              ],
              "method": "DELETE",
              "pagination": undefined,
              "path": "/lemur/v3/{request_id}",
              "path-parameters": {
                "request_id": {
                  "docs": "The ID of the LeMUR request whose data you want to delete. This would be found in the response of the original request.",
                  "type": "string",
                },
              },
              "response": {
                "docs": "LeMUR request data deleted.",
                "type": "root.PurgeLemurRequestDataResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "questionAnswer": {
              "auth": true,
              "display-name": "Create answers to one or more questions about one or more transcripts.",
              "docs": "Question & Answer allows you to ask free-form questions about a single transcript or a group of transcripts. The questions can be any whose answers you find useful, such as judging whether a caller is likely to become a customer or whether all items on a meeting's agenda were covered.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "questions": [
                      {
                        "question": "question",
                      },
                    ],
                    "transcript_ids": [
                      "transcript_ids",
                    ],
                  },
                  "response": {
                    "body": {
                      "request_id": "request_id",
                      "response": [
                        {
                          "answer": "answer",
                          "question": "question",
                        },
                      ],
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/lemur/v3/generate/question-answer",
              "request": {
                "body": {
                  "extends": [
                    "root.LemurBaseParameters",
                  ],
                  "properties": {
                    "questions": {
                      "docs": "A list of questions to ask.",
                      "type": "list<root.LemurQuestion>",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "LemurQuestionAnswerParameters",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "LeMUR question & answer response",
                "type": "root.LemurQuestionAnswerResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "summary": {
              "auth": true,
              "display-name": "Generate a custom summary from one or more transcripts.",
              "docs": "Custom Summary allows you to distill a piece of audio into a few impactful sentences. You can give the model context to obtain more targeted results while outputting the results in a variety of formats described in human language.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "transcript_ids": [
                      "transcript_ids",
                    ],
                  },
                  "response": {
                    "body": {
                      "request_id": "request_id",
                      "response": "response",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/lemur/v3/generate/summary",
              "request": {
                "body": {
                  "extends": [
                    "root.LemurBaseParameters",
                  ],
                  "properties": {
                    "answer_format": {
                      "docs": "How you want the summary to be returned. This can be any text. Examples: "TLDR", "bullet points"
",
                      "type": "optional<string>",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "LemurSummaryParameters",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "LeMUR summary response",
                "type": "root.LemurSummaryResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "task": {
              "auth": true,
              "display-name": "Ask LeMUR to use one or more transcripts with a Custom Task to handle your specialized task.",
              "docs": "Use LeMUR to ask anything with Custom Task",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "prompt": "prompt",
                    "transcript_ids": [
                      "transcript_ids",
                    ],
                  },
                  "response": {
                    "body": {
                      "request_id": "request_id",
                      "response": "response",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/lemur/v3/generate/task",
              "request": {
                "body": {
                  "extends": [
                    "root.LemurBaseParameters",
                  ],
                  "properties": {
                    "prompt": {
                      "docs": "Your text to prompt the model to produce a desired output, including any context you want to pass into the model.",
                      "type": "string",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "LemurTaskParameters",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "LeMUR task response",
                "type": "root.LemurTaskResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yml",
          },
        },
      },
      "rawContents": "imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    summary:
      path: /lemur/v3/generate/summary
      method: POST
      auth: true
      docs: >-
        Custom Summary allows you to distill a piece of audio into a few
        impactful sentences. You can give the model context to obtain more
        targeted results while outputting the results in a variety of formats
        described in human language.
      source:
        openapi: ../openapi.yml
      display-name: Generate a custom summary from one or more transcripts.
      request:
        name: LemurSummaryParameters
        body:
          properties:
            answer_format:
              type: optional<string>
              docs: >
                How you want the summary to be returned. This can be any text.
                Examples: "TLDR", "bullet points"
          extends:
            - root.LemurBaseParameters
        content-type: application/json
      response:
        docs: LeMUR summary response
        type: root.LemurSummaryResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - transcript_ids
          response:
            body:
              request_id: request_id
              response: response
    questionAnswer:
      path: /lemur/v3/generate/question-answer
      method: POST
      auth: true
      docs: >-
        Question & Answer allows you to ask free-form questions about a single
        transcript or a group of transcripts. The questions can be any whose
        answers you find useful, such as judging whether a caller is likely to
        become a customer or whether all items on a meeting's agenda were
        covered.
      source:
        openapi: ../openapi.yml
      display-name: Create answers to one or more questions about one or more transcripts.
      request:
        name: LemurQuestionAnswerParameters
        body:
          properties:
            questions:
              docs: A list of questions to ask.
              type: list<root.LemurQuestion>
          extends:
            - root.LemurBaseParameters
        content-type: application/json
      response:
        docs: LeMUR question & answer response
        type: root.LemurQuestionAnswerResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - transcript_ids
            questions:
              - question: question
          response:
            body:
              request_id: request_id
              response:
                - question: question
                  answer: answer
    actionItems:
      path: /lemur/v3/generate/action-items
      method: POST
      auth: true
      docs: Use LeMUR to generate a list of Action Items from a transcript
      source:
        openapi: ../openapi.yml
      display-name: Extract action items from one or more meeting transcripts.
      request:
        body: root.LemurActionItemsParameters
        content-type: application/json
      response:
        docs: LeMUR action items response
        type: root.LemurActionItemsResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - transcript_ids
          response:
            body:
              request_id: request_id
              response: response
    task:
      path: /lemur/v3/generate/task
      method: POST
      auth: true
      docs: Use LeMUR to ask anything with Custom Task
      source:
        openapi: ../openapi.yml
      display-name: >-
        Ask LeMUR to use one or more transcripts with a Custom Task to handle
        your specialized task.
      request:
        name: LemurTaskParameters
        body:
          properties:
            prompt:
              type: string
              docs: >-
                Your text to prompt the model to produce a desired output,
                including any context you want to pass into the model.
          extends:
            - root.LemurBaseParameters
        content-type: application/json
      response:
        docs: LeMUR task response
        type: root.LemurTaskResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - transcript_ids
            prompt: prompt
          response:
            body:
              request_id: request_id
              response: response
    purgeRequestData:
      path: /lemur/v3/{request_id}
      method: DELETE
      auth: true
      docs: >
        Delete the data for a previously submitted LeMUR request.

        The LLM response data, as well as any context provided in the original
        request will be removed.
      source:
        openapi: ../openapi.yml
      path-parameters:
        request_id:
          type: string
          docs: >-
            The ID of the LeMUR request whose data you want to delete. This
            would be found in the response of the original request.
      display-name: Delete the data for a previously submitted LeMUR request.
      response:
        docs: LeMUR request data deleted.
        type: root.PurgeLemurRequestDataResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            request_id: request_id
          response:
            body:
              request_id: request_id
              request_id_to_purge: request_id_to_purge
              deleted: true
  source:
    openapi: ../openapi.yml
  display-name: LeMUR
docs: LeMUR related operations
",
    },
    "realtime.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "docs": "Real-time transcription",
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "display-name": "realtime",
          "endpoints": {
            "createTemporaryToken": {
              "auth": true,
              "display-name": "Create a temporary authentication token for real-time transcription",
              "docs": "Create a temporary authentication token for real-time transcription",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "expires_in": 1,
                  },
                  "response": {
                    "body": {
                      "token": "token",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v2/realtime/token",
              "request": {
                "body": {
                  "properties": {
                    "expires_in": {
                      "docs": "The amount of time until the token expires in seconds.",
                      "type": "integer",
                      "validation": {
                        "exclusiveMax": undefined,
                        "exclusiveMin": undefined,
                        "max": undefined,
                        "min": 60,
                        "multipleOf": undefined,
                      },
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "CreateRealtimeTemporaryTokenParameters",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "Temporary authentication token generated.",
                "type": "root.RealtimeTemporaryTokenResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yml",
          },
        },
      },
      "rawContents": "imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    createTemporaryToken:
      path: /v2/realtime/token
      method: POST
      auth: true
      docs: Create a temporary authentication token for real-time transcription
      source:
        openapi: ../openapi.yml
      display-name: Create a temporary authentication token for real-time transcription
      request:
        name: CreateRealtimeTemporaryTokenParameters
        body:
          properties:
            expires_in:
              type: integer
              docs: The amount of time until the token expires in seconds.
              validation:
                min: 60
        content-type: application/json
      response:
        docs: Temporary authentication token generated.
        type: root.RealtimeTemporaryTokenResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            expires_in: 1
          response:
            body:
              token: token
  source:
    openapi: ../openapi.yml
  display-name: realtime
docs: Real-time transcription
",
    },
    "transcript.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "docs": "Transcript related operations",
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "display-name": "transcript",
          "endpoints": {
            "create": {
              "auth": true,
              "display-name": "Create a transcript from an audio file",
              "docs": "Create a transcript from an audio or video file that is accessible via a URL.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "request": {
                    "audio_url": "audio_url",
                  },
                  "response": {
                    "body": {
                      "acoustic_model": "acoustic_model",
                      "audio_url": "audio_url",
                      "auto_highlights": true,
                      "id": "id",
                      "language_model": "language_model",
                      "redact_pii": true,
                      "status": "queued",
                      "summarization": true,
                      "webhook_auth": true,
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v2/transcript",
              "request": {
                "body": {
                  "extends": [
                    "root.CreateTranscriptOptionalParameters",
                  ],
                  "properties": {
                    "audio_url": {
                      "docs": "The URL of the audio or video file to transcribe.",
                      "type": "string",
                    },
                  },
                },
                "content-type": "application/json",
                "headers": undefined,
                "name": "CreateTranscriptParameters",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "Transcript created and queued for processing.",
                "type": "root.Transcript",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "delete": {
              "auth": true,
              "display-name": "Delete the transcript",
              "docs": "Delete the transcript",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "transcript_id": "transcript_id",
                  },
                  "response": {
                    "body": {
                      "acoustic_model": "acoustic_model",
                      "audio_url": "audio_url",
                      "auto_highlights": true,
                      "id": "id",
                      "language_model": "language_model",
                      "redact_pii": true,
                      "status": "queued",
                      "summarization": true,
                      "webhook_auth": true,
                    },
                  },
                },
              ],
              "method": "DELETE",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "response": {
                "docs": "The deleted transcript response.",
                "type": "root.Transcript",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "get": {
              "auth": true,
              "display-name": "Get the transcript",
              "docs": "Get the transcript resource. The transcript is ready when the "status" is "completed".",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "transcript_id": "transcript_id",
                  },
                  "response": {
                    "body": {
                      "acoustic_model": "acoustic_model",
                      "audio_url": "audio_url",
                      "auto_highlights": true,
                      "id": "id",
                      "language_model": "language_model",
                      "redact_pii": true,
                      "status": "queued",
                      "summarization": true,
                      "webhook_auth": true,
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "response": {
                "docs": "The transcript resource",
                "type": "root.Transcript",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "getParagraphs": {
              "auth": true,
              "display-name": "Get the transcript split by paragraphs",
              "docs": "Get the transcript split by paragraphs. The API will attempt to semantically segment your transcript into paragraphs to create more reader-friendly transcripts.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "transcript_id": "transcript_id",
                  },
                  "response": {
                    "body": {
                      "audio_duration": 1.1,
                      "confidence": 1.1,
                      "id": "id",
                      "paragraphs": [
                        {
                          "confidence": 1.1,
                          "end": 1,
                          "start": 1,
                          "text": "text",
                          "words": [
                            {
                              "confidence": 1.1,
                              "end": 1,
                              "start": 1,
                              "text": "text",
                            },
                          ],
                        },
                      ],
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}/paragraphs",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "response": {
                "docs": "Exported paragraphs",
                "type": "root.ParagraphsResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "getRedactedAudio": {
              "auth": true,
              "display-name": "Retrieves the redacted audio object containing the status and URL to the redacted audio.",
              "docs": "Retrieves the redacted audio object containing the status and URL to the redacted audio.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "transcript_id": "transcript_id",
                  },
                  "response": {
                    "body": {
                      "redacted_audio_url": "redacted_audio_url",
                      "status": "redacted_audio_ready",
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}/redacted-audio",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "response": {
                "docs": "The redacted audio object containing the status and URL to the redacted audio.",
                "type": "root.RedactedAudioResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "getSentences": {
              "auth": true,
              "display-name": "Get the transcript split by sentences",
              "docs": "Get the transcript split by sentences. The API will attempt to semantically segment the transcript into sentences to create more reader-friendly transcripts.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "path-parameters": {
                    "transcript_id": "transcript_id",
                  },
                  "response": {
                    "body": {
                      "audio_duration": 1.1,
                      "confidence": 1.1,
                      "id": "id",
                      "sentences": [
                        {
                          "confidence": 1.1,
                          "end": 1,
                          "start": 1,
                          "text": "text",
                          "words": [
                            {
                              "confidence": 1.1,
                              "end": 1,
                              "start": 1,
                              "text": "text",
                            },
                          ],
                        },
                      ],
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}/sentences",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "response": {
                "docs": "Exported sentences",
                "type": "root.SentencesResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "getSubtitles": {
              "auth": true,
              "display-name": "Export transcript as SRT or VTT captions.",
              "docs": "Export your transcript in SRT or VTT format, to be plugged into a video player for subtitles and closed captions.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}/{subtitle_format}",
              "path-parameters": {
                "subtitle_format": {
                  "docs": "The format of the captions.",
                  "type": "root.SubtitleFormat",
                },
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "request": {
                "name": "TranscriptGetSubtitlesRequest",
                "query-parameters": {
                  "chars_per_caption": {
                    "docs": "The maximum number of characters per caption",
                    "type": "optional<integer>",
                  },
                },
              },
              "response": {
                "docs": "The exported captions as text",
                "type": "text",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "list": {
              "auth": true,
              "display-name": "List transcripts",
              "docs": "Retrieve a list of transcripts you have created.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "examples": [
                {
                  "response": {
                    "body": {
                      "page_details": {
                        "current_url": "current_url",
                        "limit": 1,
                        "prev_url": "prev_url",
                        "result_count": 1,
                      },
                      "transcripts": [
                        {
                          "audio_url": "audio_url",
                          "created": "2024-01-15T09:30:00Z",
                          "id": "id",
                          "resource_url": "resource_url",
                          "status": "queued",
                        },
                      ],
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript",
              "request": {
                "name": "TranscriptListRequest",
                "query-parameters": {
                  "after_id": {
                    "docs": "Get transcripts that were created after this transcript ID",
                    "type": "optional<string>",
                  },
                  "before_id": {
                    "docs": "Get transcripts that were created before this transcript ID",
                    "type": "optional<string>",
                  },
                  "created_on": {
                    "docs": "Only get transcripts created on this date",
                    "type": "optional<string>",
                    "validation": {
                      "format": "date",
                      "maxLength": undefined,
                      "minLength": undefined,
                      "pattern": undefined,
                    },
                  },
                  "limit": {
                    "docs": "Maximum amount of transcripts to retrieve",
                    "type": "optional<long>",
                  },
                  "status": {
                    "docs": "Filter by transcript status",
                    "type": "optional<TranscriptListRequestStatus>",
                  },
                  "throttled_only": {
                    "docs": "Only get throttled transcripts, overrides the status filter",
                    "type": "optional<boolean>",
                  },
                },
              },
              "response": {
                "docs": "A list of transcripts filtered by `limit` and `status`",
                "type": "root.TranscriptList",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
            "wordSearch": {
              "auth": true,
              "display-name": "Search the given transcript for words, numbers, or phrases",
              "docs": "Search through the transcript for a specific set of keywords. You can search for individual words, numbers, or phrases containing up to five words or numbers.",
              "errors": [
                "root.BadRequestError",
                "root.UnauthorizedError",
                "root.NotFoundError",
                "root.TooManyRequestsError",
                "root.InternalServerError",
                "root.ServiceUnavailableError",
                "root.GatewayTimeoutError",
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v2/transcript/{transcript_id}/word-search",
              "path-parameters": {
                "transcript_id": {
                  "docs": "ID of the transcript",
                  "type": "string",
                },
              },
              "request": {
                "name": "TranscriptWordSearchRequest",
                "query-parameters": {
                  "words": {
                    "allow-multiple": true,
                    "docs": "Keywords to search for",
                    "type": "optional<string>",
                  },
                },
              },
              "response": {
                "docs": "Word search response",
                "type": "root.WordSearchResponse",
              },
              "source": {
                "openapi": "../openapi.yml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yml",
          },
        },
        "types": {
          "TranscriptListRequestStatus": {
            "docs": "The status of your transcription. Possible values are queued, processing, completed, or error.",
            "enum": [
              {
                "docs": "The audio file is in the queue to be processed by the API.",
                "value": "queued",
              },
              {
                "docs": "The audio file is being processed by the API.",
                "value": "processing",
              },
              {
                "docs": "The transcription job has been completed successfully.",
                "value": "completed",
              },
              {
                "docs": "An error occurred while processing the audio file.",
                "value": "error",
              },
            ],
            "source": {
              "openapi": "../openapi.yml",
            },
          },
        },
      },
      "rawContents": "types:
  TranscriptListRequestStatus:
    enum:
      - value: queued
        docs: The audio file is in the queue to be processed by the API.
      - value: processing
        docs: The audio file is being processed by the API.
      - value: completed
        docs: The transcription job has been completed successfully.
      - value: error
        docs: An error occurred while processing the audio file.
    docs: >-
      The status of your transcription. Possible values are queued, processing,
      completed, or error.
    source:
      openapi: ../openapi.yml
imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    list:
      path: /v2/transcript
      method: GET
      auth: true
      docs: Retrieve a list of transcripts you have created.
      source:
        openapi: ../openapi.yml
      display-name: List transcripts
      request:
        name: TranscriptListRequest
        query-parameters:
          limit:
            type: optional<long>
            docs: Maximum amount of transcripts to retrieve
          status:
            type: optional<TranscriptListRequestStatus>
            docs: Filter by transcript status
          created_on:
            type: optional<string>
            docs: Only get transcripts created on this date
            validation:
              format: date
          before_id:
            type: optional<string>
            docs: Get transcripts that were created before this transcript ID
          after_id:
            type: optional<string>
            docs: Get transcripts that were created after this transcript ID
          throttled_only:
            type: optional<boolean>
            docs: Only get throttled transcripts, overrides the status filter
      response:
        docs: A list of transcripts filtered by `limit` and `status`
        type: root.TranscriptList
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - response:
            body:
              page_details:
                limit: 1
                result_count: 1
                current_url: current_url
                prev_url: prev_url
              transcripts:
                - id: id
                  resource_url: resource_url
                  status: queued
                  created: '2024-01-15T09:30:00Z'
                  audio_url: audio_url
    create:
      path: /v2/transcript
      method: POST
      auth: true
      docs: >-
        Create a transcript from an audio or video file that is accessible via a
        URL.
      source:
        openapi: ../openapi.yml
      display-name: Create a transcript from an audio file
      request:
        name: CreateTranscriptParameters
        body:
          properties:
            audio_url:
              type: string
              docs: The URL of the audio or video file to transcribe.
          extends:
            - root.CreateTranscriptOptionalParameters
        content-type: application/json
      response:
        docs: Transcript created and queued for processing.
        type: root.Transcript
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            audio_url: audio_url
          response:
            body:
              id: id
              language_model: language_model
              acoustic_model: acoustic_model
              status: queued
              audio_url: audio_url
              webhook_auth: true
              auto_highlights: true
              redact_pii: true
              summarization: true
    get:
      path: /v2/transcript/{transcript_id}
      method: GET
      auth: true
      docs: >-
        Get the transcript resource. The transcript is ready when the "status"
        is "completed".
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: Get the transcript
      response:
        docs: The transcript resource
        type: root.Transcript
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            transcript_id: transcript_id
          response:
            body:
              id: id
              language_model: language_model
              acoustic_model: acoustic_model
              status: queued
              audio_url: audio_url
              webhook_auth: true
              auto_highlights: true
              redact_pii: true
              summarization: true
    delete:
      path: /v2/transcript/{transcript_id}
      method: DELETE
      auth: true
      docs: Delete the transcript
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: Delete the transcript
      response:
        docs: The deleted transcript response.
        type: root.Transcript
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            transcript_id: transcript_id
          response:
            body:
              id: id
              language_model: language_model
              acoustic_model: acoustic_model
              status: queued
              audio_url: audio_url
              webhook_auth: true
              auto_highlights: true
              redact_pii: true
              summarization: true
    getSubtitles:
      path: /v2/transcript/{transcript_id}/{subtitle_format}
      method: GET
      auth: true
      docs: >-
        Export your transcript in SRT or VTT format, to be plugged into a video
        player for subtitles and closed captions.
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
        subtitle_format:
          type: root.SubtitleFormat
          docs: The format of the captions.
      display-name: Export transcript as SRT or VTT captions.
      request:
        name: TranscriptGetSubtitlesRequest
        query-parameters:
          chars_per_caption:
            type: optional<integer>
            docs: The maximum number of characters per caption
      response:
        docs: The exported captions as text
        type: text
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
    getSentences:
      path: /v2/transcript/{transcript_id}/sentences
      method: GET
      auth: true
      docs: >-
        Get the transcript split by sentences. The API will attempt to
        semantically segment the transcript into sentences to create more
        reader-friendly transcripts.
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: Get the transcript split by sentences
      response:
        docs: Exported sentences
        type: root.SentencesResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            transcript_id: transcript_id
          response:
            body:
              id: id
              confidence: 1.1
              audio_duration: 1.1
              sentences:
                - text: text
                  start: 1
                  end: 1
                  confidence: 1.1
                  words:
                    - confidence: 1.1
                      start: 1
                      end: 1
                      text: text
    getParagraphs:
      path: /v2/transcript/{transcript_id}/paragraphs
      method: GET
      auth: true
      docs: >-
        Get the transcript split by paragraphs. The API will attempt to
        semantically segment your transcript into paragraphs to create more
        reader-friendly transcripts.
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: Get the transcript split by paragraphs
      response:
        docs: Exported paragraphs
        type: root.ParagraphsResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            transcript_id: transcript_id
          response:
            body:
              id: id
              confidence: 1.1
              audio_duration: 1.1
              paragraphs:
                - text: text
                  start: 1
                  end: 1
                  confidence: 1.1
                  words:
                    - confidence: 1.1
                      start: 1
                      end: 1
                      text: text
    wordSearch:
      path: /v2/transcript/{transcript_id}/word-search
      method: GET
      auth: true
      docs: >-
        Search through the transcript for a specific set of keywords. You can
        search for individual words, numbers, or phrases containing up to five
        words or numbers.
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: Search the given transcript for words, numbers, or phrases
      request:
        name: TranscriptWordSearchRequest
        query-parameters:
          words:
            type: optional<string>
            allow-multiple: true
            docs: Keywords to search for
      response:
        docs: Word search response
        type: root.WordSearchResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
    getRedactedAudio:
      path: /v2/transcript/{transcript_id}/redacted-audio
      method: GET
      auth: true
      docs: >-
        Retrieves the redacted audio object containing the status and URL to the
        redacted audio.
      source:
        openapi: ../openapi.yml
      path-parameters:
        transcript_id:
          type: string
          docs: ID of the transcript
      display-name: >-
        Retrieves the redacted audio object containing the status and URL to the
        redacted audio.
      response:
        docs: >-
          The redacted audio object containing the status and URL to the
          redacted audio.
        type: root.RedactedAudioResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            transcript_id: transcript_id
          response:
            body:
              status: redacted_audio_ready
              redacted_audio_url: redacted_audio_url
  source:
    openapi: ../openapi.yml
  display-name: transcript
docs: Transcript related operations
",
    },
  },
  "packageMarkers": {},
  "rootApiFile": {
    "contents": {
      "auth": "ApiKey",
      "auth-schemes": {
        "ApiKey": {
          "header": "Authorization",
          "name": "apiKey",
          "type": "string",
        },
      },
      "default-environment": "Default",
      "display-name": "AssemblyAI API",
      "environments": {
        "Default": "https://api.assemblyai.com",
      },
      "error-discrimination": {
        "strategy": "status-code",
      },
      "name": "api",
    },
    "defaultUrl": undefined,
    "rawContents": "name: api
error-discrimination:
  strategy: status-code
display-name: AssemblyAI API
environments:
  Default: https://api.assemblyai.com
default-environment: Default
auth-schemes:
  ApiKey:
    header: Authorization
    name: apiKey
    type: string
auth: ApiKey
",
  },
}