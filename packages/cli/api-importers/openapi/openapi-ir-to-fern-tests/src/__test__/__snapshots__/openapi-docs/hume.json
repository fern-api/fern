{
  "absoluteFilePath": "/DUMMY_PATH",
  "importedDefinitions": {},
  "namedDefinitionFiles": {
    "__package__.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "service": {
          "auth": false,
          "base-path": "",
          "endpoints": {
            "Get Job Predictions": {
              "auth": true,
              "display-name": "Get Job Predictions",
              "docs": "Get the JSON predictions of a completed job.",
              "examples": [
                {
                  "path-parameters": {
                    "id": "id",
                  },
                  "response": {
                    "body": [
                      {
                        "error": "error",
                        "results": {
                          "errors": [
                            {
                              "file": "file",
                              "message": "message",
                            },
                          ],
                          "predictions": [
                            {
                              "file": "file",
                              "models": {},
                            },
                          ],
                        },
                        "source": {
                          "type": "url",
                          "url": "url",
                        },
                      },
                    ],
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v0/batch/jobs/{id}/predictions",
              "path-parameters": {
                "id": "string",
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "list<SourceResult>",
              },
              "source": {
                "openapi": "../openapi.yaml",
              },
            },
            "List Jobs": {
              "auth": true,
              "display-name": "List Jobs",
              "docs": "Sort and filter jobs.",
              "examples": [
                {
                  "response": {
                    "body": [
                      {
                        "job_id": "job_id",
                        "request": {
                          "callback_url": "callback_url",
                          "files": [
                            {
                              "md5sum": "md5sum",
                            },
                          ],
                          "notify": true,
                          "urls": [
                            "urls",
                          ],
                        },
                        "state": {
                          "created_timestamp_ms": 1000000,
                          "status": "QUEUED",
                        },
                        "user_id": "user_id",
                      },
                    ],
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v0/batch/jobs",
              "request": {
                "name": "ListJobsRequest",
                "query-parameters": {
                  "direction": {
                    "docs": "The sort direction.",
                    "type": "optional<Direction>",
                  },
                  "limit": {
                    "default": 50,
                    "docs": "The maximum number of jobs to include in the response.",
                    "type": "optional<integer>",
                  },
                  "sort_by": {
                    "docs": "The job timestamp to sort by.",
                    "type": "optional<SortBy>",
                  },
                  "status": {
                    "allow-multiple": true,
                    "docs": "Include only jobs with these statuses.",
                    "type": "optional<Status>",
                  },
                  "timestamp_ms": {
                    "docs": "Defaults to the current date and time. See `when`.",
                    "type": "optional<long>",
                  },
                  "when": {
                    "docs": "Include only jobs that were created before or after `timestamp_ms`.",
                    "type": "optional<When>",
                  },
                },
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "list<JobRequest>",
              },
              "source": {
                "openapi": "../openapi.yaml",
              },
            },
            "Start Job": {
              "auth": true,
              "display-name": "Start Job",
              "docs": "Start a new batch job.",
              "examples": [
                {
                  "request": {},
                  "response": {
                    "body": {
                      "job_id": "job_id",
                    },
                  },
                },
              ],
              "method": "POST",
              "pagination": undefined,
              "path": "/v0/batch/jobs",
              "request": {
                "body": {
                  "properties": {
                    "callback_url": {
                      "docs": "If provided, a `POST` request will be made to the URL with the generated predictions on completion or the error message on failure.",
                      "type": "optional<string>",
                    },
                    "models": "optional<Models>",
                    "notify": {
                      "default": false,
                      "docs": "Whether to send an email notification to the user upon job failure.",
                      "type": "optional<boolean>",
                    },
                    "transcription": "optional<Transcription>",
                    "urls": {
                      "docs": "URLs to the media files to be processed. Each must be a valid public URL to a media file (see recommended input filetypes) or an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`) of media files.

If you wish to supply more than 100 URLs, consider providing them as an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`).",
                      "type": "optional<list<string>>",
                    },
                  },
                },
                "content-type": "application/json; charset=utf-8",
                "headers": undefined,
                "name": "BaseRequest",
                "path-parameters": undefined,
                "query-parameters": undefined,
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "JobId",
              },
              "source": {
                "openapi": "../openapi.yaml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yaml",
          },
        },
        "types": {
          "AssistantEnd": {
            "docs": "When provided, the output is an assistant end message.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "type": {
                "docs": "The type of message sent through the socket; for an Assistant End message, this must be `assistant_end`.

This message indicates the conclusion of the assistant’s response, signaling that the assistant has finished speaking for the current conversational turn.",
                "type": "literal<"assistant_end">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "AssistantInput": {
            "docs": "When provided, the input is spoken by EVI.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "type": "optional<string>",
              },
              "text": {
                "docs": "Assistant text to synthesize into spoken audio and insert into the conversation.

EVI uses this text to generate spoken audio using our proprietary expressive text-to-speech model. Our model adds appropriate emotional inflections and tones to the text based on the user’s expressions and the context of the conversation. The synthesized audio is streamed back to the user as an [Assistant Message](/reference/empathic-voice-interface-evi/chat/chat#receive.Assistant%20Message.type).",
                "type": "string",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `assistant_input` for our server to correctly identify and process it as an Assistant Input message.",
                "type": "literal<"assistant_input">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "AssistantMessage": {
            "docs": "When provided, the output is an assistant message.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "from_text": {
                "docs": "Indicates if this message was inserted into the conversation as text from an [Assistant Input message](/reference/empathic-voice-interface-evi/chat/chat#send.Assistant%20Input.text).",
                "type": "boolean",
              },
              "id": {
                "docs": "ID of the assistant message. Allows the Assistant Message to be tracked and referenced.",
                "type": "optional<string>",
              },
              "message": {
                "docs": "Transcript of the message.",
                "type": "ChatMessage",
              },
              "models": {
                "docs": "Inference model results.",
                "type": "Inference",
              },
              "type": {
                "docs": "The type of message sent through the socket; for an Assistant Message, this must be `assistant_message`.

This message contains both a transcript of the assistant’s response and the expression measurement predictions of the assistant’s audio output.",
                "type": "literal<"assistant_message">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "AudioConfiguration": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "channels": {
                "docs": "Number of audio channels.",
                "type": "integer",
              },
              "encoding": {
                "docs": "Encoding format of the audio input, such as `linear16`.",
                "type": "Encoding",
              },
              "sample_rate": {
                "docs": "Audio sample rate. Number of samples per second in the audio input, measured in Hertz.",
                "type": "integer",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "AudioInput": {
            "docs": "When provided, the input is audio.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "data": {
                "docs": "Base64 encoded audio input to insert into the conversation.

The content of an Audio Input message is treated as the user’s speech to EVI and must be streamed continuously. Pre-recorded audio files are not supported.

For optimal transcription quality, the audio data should be transmitted in small chunks.

Hume recommends streaming audio with a buffer window of 20 milliseconds (ms), or 100 milliseconds (ms) for web applications.",
                "type": "string",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `audio_input` for our server to correctly identify and process it as an Audio Input message.

This message is used for sending audio input data to EVI for processing and expression measurement. Audio data should be sent as a continuous stream, encoded in Base64.",
                "type": "literal<"audio_input">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "AudioOutput": {
            "docs": "The type of message sent through the socket; for an Audio Output message, this must be `audio_output`.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "data": {
                "docs": "Base64 encoded audio output. This encoded audio is transmitted to the client, where it can be decoded and played back as part of the user interaction.",
                "type": "string",
              },
              "id": {
                "docs": "ID of the audio output. Allows the Audio Output message to be tracked and referenced.",
                "type": "string",
              },
              "index": {
                "docs": "Index of the chunk of audio relative to the whole audio segment.",
                "type": "integer",
              },
              "is_final_chunk": {
                "docs": "This AudioOutput contains the final chunk for this particular segment.",
                "type": "boolean",
              },
              "type": {
                "type": "literal<"audio_output">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "Bcp47Tag": {
            "enum": [
              "zh",
              "da",
              "nl",
              "en",
              {
                "name": "EnAu",
                "value": "en-AU",
              },
              {
                "name": "EnIn",
                "value": "en-IN",
              },
              {
                "name": "EnNz",
                "value": "en-NZ",
              },
              {
                "name": "EnGb",
                "value": "en-GB",
              },
              "fr",
              {
                "name": "FrCa",
                "value": "fr-CA",
              },
              "de",
              "hi",
              {
                "name": "HiLatn",
                "value": "hi-Latn",
              },
              "id",
              "it",
              "ja",
              "ko",
              "no",
              "pl",
              "pt",
              {
                "name": "PtBr",
                "value": "pt-BR",
              },
              {
                "name": "PtPt",
                "value": "pt-PT",
              },
              "ru",
              "es",
              {
                "name": "Es419",
                "value": "es-419",
              },
              "sv",
              "ta",
              "tr",
              "uk",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "BoundingBox": {
            "docs": "A bounding box around a face.",
            "inline": undefined,
            "properties": {
              "h": {
                "docs": "Bounding box height.",
                "type": "double",
              },
              "w": {
                "docs": "Bounding box width.",
                "type": "double",
              },
              "x": {
                "docs": "x-coordinate of bounding box top left corner.",
                "type": "double",
              },
              "y": {
                "docs": "y-coordinate of bounding box top left corner.",
                "type": "double",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "BuiltInTool": {
            "enum": [
              "web_search",
              "hang_up",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "BuiltinToolConfig": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "fallback_content": {
                "docs": "Optional text passed to the supplemental LLM if the tool call fails. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation.",
                "type": "optional<string>",
              },
              "name": {
                "type": "BuiltInTool",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "BurstPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "descriptions": {
                "docs": "Modality-specific descriptive features and their scores.",
                "type": "list<DescriptionsScore>",
              },
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
              "time": "TimeInterval",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "ChatMessage": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "content": {
                "docs": "Transcript of the message.",
                "type": "optional<string>",
              },
              "role": {
                "docs": "Role of who is providing the message.",
                "type": "Role",
              },
              "tool_call": {
                "docs": "Function call name and arguments.",
                "type": "optional<ToolCallMessage>",
              },
              "tool_result": {
                "docs": "Function call response from client.",
                "type": "optional<ChatMessageToolResult>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ChatMessageToolResult": {
            "discriminated": false,
            "docs": "Function call response from client.",
            "encoding": undefined,
            "inline": true,
            "source": {
              "openapi": "../asyncapi.json",
            },
            "union": [
              {
                "type": "ToolResponseMessage",
              },
              {
                "type": "ToolErrorMessage",
              },
            ],
          },
          "ChatMetadata": {
            "docs": "When provided, the output is a chat metadata message.",
            "inline": undefined,
            "properties": {
              "chat_group_id": {
                "docs": "ID of the Chat Group.

Used to resume a Chat when passed in the [resumed_chat_group_id](/reference/empathic-voice-interface-evi/chat/chat#request.query.resumed_chat_group_id) query parameter of a subsequent connection request. This allows EVI to continue the conversation from where it left off within the Chat Group.

Learn more about [supporting chat resumability](/docs/empathic-voice-interface-evi/faq#does-evi-support-chat-resumability) from the EVI FAQ.",
                "type": "string",
              },
              "chat_id": {
                "docs": "ID of the Chat session. Allows the Chat session to be tracked and referenced.",
                "type": "string",
              },
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "request_id": {
                "docs": "ID of the initiating request.",
                "type": "optional<string>",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a Chat Metadata message, this must be `chat_metadata`.

The Chat Metadata message is the first message you receive after establishing a connection with EVI and contains important identifiers for the current Chat session.",
                "type": "literal<"chat_metadata">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "Completed": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "created_timestamp_ms": {
                "docs": "When this job was created (Unix timestamp in milliseconds).",
                "type": "long",
              },
              "ended_timestamp_ms": {
                "docs": "When this job ended (Unix timestamp in milliseconds).",
                "type": "long",
              },
              "num_errors": {
                "docs": "The number of errors that occurred while running this job.",
                "type": "integer",
              },
              "num_predictions": {
                "docs": "The number of predictions that were generated by this job.",
                "type": "integer",
              },
              "started_timestamp_ms": {
                "docs": "When this job started (Unix timestamp in milliseconds).",
                "type": "long",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Context": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "text": {
                "docs": "The context to be injected into the conversation. Helps inform the LLM's response by providing relevant information about the ongoing conversation.

This text will be appended to the end of user messages based on the chosen persistence level. For example, if you want to remind EVI of its role as a helpful weather assistant, the context you insert will be appended to the end of user messages as `{Context: You are a helpful weather assistant}`.",
                "type": "string",
              },
              "type": {
                "docs": "The persistence level of the injected context. Specifies how long the injected context will remain active in the session.

There are three possible context types:

- **Persistent**: The context is appended to all user messages for the duration of the session.

- **Temporary**: The context is appended only to the next user message.

 - **Editable**: The original context is updated to reflect the new context.

 If the type is not specified, it will default to `temporary`.",
                "type": "optional<ContextType>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ContextType": {
            "enum": [
              "editable",
              "persistent",
              "temporary",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "DescriptionsScore": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "name": {
                "docs": "Name of the descriptive feature being expressed.",
                "type": "string",
              },
              "score": {
                "docs": "Embedding value for the descriptive feature being expressed.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Direction": {
            "enum": [
              "asc",
              "desc",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "EmotionScore": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "name": {
                "docs": "Name of the emotion being expressed.",
                "type": "string",
              },
              "score": {
                "docs": "Embedding value for the emotion being expressed.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "EmotionScores": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "Admiration": "double",
              "Adoration": "double",
              "Aesthetic Appreciation": "double",
              "Amusement": "double",
              "Anger": "double",
              "Anxiety": "double",
              "Awe": "double",
              "Awkwardness": "double",
              "Boredom": "double",
              "Calmness": "double",
              "Concentration": "double",
              "Confusion": "double",
              "Contemplation": "double",
              "Contempt": "double",
              "Contentment": "double",
              "Craving": "double",
              "Desire": "double",
              "Determination": "double",
              "Disappointment": "double",
              "Disgust": "double",
              "Distress": "double",
              "Doubt": "double",
              "Ecstasy": "double",
              "Embarrassment": "double",
              "Empathic Pain": "double",
              "Entrancement": "double",
              "Envy": "double",
              "Excitement": "double",
              "Fear": "double",
              "Guilt": "double",
              "Horror": "double",
              "Interest": "double",
              "Joy": "double",
              "Love": "double",
              "Nostalgia": "double",
              "Pain": "double",
              "Pride": "double",
              "Realization": "double",
              "Relief": "double",
              "Romance": "double",
              "Sadness": "double",
              "Satisfaction": "double",
              "Shame": "double",
              "Surprise (negative)": "double",
              "Surprise (positive)": "double",
              "Sympathy": "double",
              "Tiredness": "double",
              "Triumph": "double",
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "Empty": {
            "docs": "To include predictions for this model type, set this field to `{}`. It is currently not configurable further.",
            "type": "map<string, unknown>",
          },
          "Encoding": {
            "type": "literal<"linear16">",
          },
          "Error": {
            "docs": "When provided, the output is an error message.",
            "inline": undefined,
            "properties": {
              "code": {
                "docs": "Error code. Identifies the type of error encountered.",
                "type": "string",
              },
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "message": {
                "docs": "Detailed description of the error.",
                "type": "string",
              },
              "slug": {
                "docs": "Short, human-readable identifier and description for the error. See a complete list of error slugs on the [Errors page](/docs/resources/errors).",
                "type": "string",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a Web Socket Error message, this must be `error`.

This message indicates a disruption in the WebSocket connection, such as an unexpected disconnection, protocol error, or data transmission issue.",
                "type": "literal<"error">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ErrorLevel": {
            "type": "literal<"warn">",
          },
          "Face": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "descriptions": "optional<Empty>",
              "facs": "optional<Empty>",
              "fps_pred": {
                "default": 3,
                "docs": "Number of frames per second to process. Other frames will be omitted from the response. Set to `0` to process every frame.",
                "type": "optional<double>",
              },
              "identify_faces": {
                "default": false,
                "docs": "Whether to return identifiers for faces across frames. If `true`, unique identifiers will be assigned to face bounding boxes to differentiate different faces. If `false`, all faces will be tagged with an `unknown` ID.",
                "type": "optional<boolean>",
              },
              "min_face_size": {
                "docs": "Minimum bounding box side length in pixels to treat as a face. Faces detected with a bounding box side length in pixels less than this threshold will be omitted from the response.",
                "type": "optional<uint64>",
              },
              "prob_threshold": {
                "default": 0.99,
                "docs": "Face detection probability threshold. Faces detected with a probability less than this threshold will be omitted from the response.",
                "type": "optional<double>",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": 1,
                  "min": 0,
                  "multipleOf": undefined,
                },
              },
              "save_faces": {
                "default": false,
                "docs": "Whether to extract and save the detected faces in the artifacts zip created by each job.",
                "type": "optional<boolean>",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "FacePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "box": "BoundingBox",
              "descriptions": {
                "docs": "Modality-specific descriptive features and their scores.",
                "type": "optional<list<DescriptionsScore>>",
              },
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
              "facs": {
                "docs": "FACS 2.0 features and their scores.",
                "type": "optional<list<FacsScore>>",
              },
              "frame": {
                "docs": "Frame number",
                "type": "uint64",
              },
              "prob": {
                "docs": "The predicted probability that a detected face was actually a face.",
                "type": "double",
              },
              "time": {
                "docs": "Time in seconds when face detection occurred.",
                "type": "double",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "FacemeshPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "FacsScore": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "name": {
                "docs": "Name of the FACS 2.0 feature being expressed.",
                "type": "string",
              },
              "score": {
                "docs": "Embedding value for the FACS 2.0 feature being expressed.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Failed": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "created_timestamp_ms": {
                "docs": "When this job was created (Unix timestamp in milliseconds).",
                "type": "long",
              },
              "ended_timestamp_ms": {
                "docs": "When this job ended (Unix timestamp in milliseconds).",
                "type": "long",
              },
              "message": {
                "docs": "An error message.",
                "type": "string",
              },
              "started_timestamp_ms": {
                "docs": "When this job started (Unix timestamp in milliseconds).",
                "type": "long",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "File": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "content_type": {
                "docs": "The content type of the file.",
                "type": "optional<string>",
              },
              "filename": {
                "docs": "The name of the file.",
                "type": "optional<string>",
              },
              "md5sum": {
                "docs": "The MD5 checksum of the file.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Granularity": {
            "docs": "The granularity at which to generate predictions. `utterance` corresponds to a natural pause or break in conversation, while `conversational_turn` corresponds to a change in speaker.",
            "enum": [
              "word",
              "sentence",
              "utterance",
              "conversational_turn",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsBurstPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<BurstPrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsFacePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<FacePrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsFacemeshPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<FacemeshPrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsLanguagePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<LanguagePrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsNerPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<NerPrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "GroupedPredictionsProsodyPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "id": {
                "docs": "An automatically generated label to identify individuals in your media file. Will be `unknown` if you have chosen to disable identification, or if the model is unable to distinguish between individuals.",
                "type": "string",
              },
              "predictions": "list<ProsodyPrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "InProgress": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "created_timestamp_ms": {
                "docs": "When this job was created (Unix timestamp in milliseconds).",
                "type": "long",
              },
              "started_timestamp_ms": {
                "docs": "When this job started (Unix timestamp in milliseconds).",
                "type": "long",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Inference": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "prosody": {
                "docs": "Prosody model inference results.

EVI uses the prosody model to measure 48 emotions related to speech and vocal characteristics within a given expression.",
                "type": "optional<ProsodyInference>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "JobId": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "job_id": {
                "docs": "The ID of the started job.",
                "type": "string",
                "validation": {
                  "format": "uuid",
                  "maxLength": undefined,
                  "minLength": undefined,
                  "pattern": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "JobRequest": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "job_id": {
                "docs": "The ID associated with this job.",
                "type": "string",
                "validation": {
                  "format": "uuid",
                  "maxLength": undefined,
                  "minLength": undefined,
                  "pattern": undefined,
                },
              },
              "request": "Request",
              "state": "State",
              "user_id": {
                "docs": "Your user ID.",
                "type": "string",
                "validation": {
                  "format": "uuid",
                  "maxLength": undefined,
                  "minLength": undefined,
                  "pattern": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Language": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "granularity": "optional<Granularity>",
              "identify_speakers": {
                "default": false,
                "docs": "Whether to return identifiers for speakers over time. If `true`, unique identifiers will be assigned to spoken words to differentiate different speakers. If `false`, all speakers will be tagged with an `unknown` ID.",
                "type": "optional<boolean>",
              },
              "sentiment": "optional<Empty>",
              "toxicity": "optional<Empty>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "LanguagePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence in this text.",
                "type": "optional<double>",
              },
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
              "position": "PositionInterval",
              "sentiment": {
                "docs": "Sentiment predictions returned as a distribution. This model predicts the probability that a given text could be interpreted as having each sentiment level from `1` (negative) to `9` (positive).

Compared to returning one estimate of sentiment, this enables a more nuanced analysis of a text's meaning. For example, a text with very neutral sentiment would have an average rating of `5`. But also a text that could be interpreted as having very positive sentiment or very negative sentiment would also have an average rating of `5`. The average sentiment is less informative than the distribution over sentiment, so this API returns a value for each sentiment level.",
                "type": "optional<list<SentimentScore>>",
              },
              "speaker_confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence that this text was spoken by this speaker.",
                "type": "optional<double>",
              },
              "text": {
                "docs": "A segment of text (like a word or a sentence).",
                "type": "string",
              },
              "time": "optional<TimeInterval>",
              "toxicity": {
                "docs": "Toxicity predictions returned as probabilities that the text can be classified into the following categories: `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, and `identity_hate`.",
                "type": "optional<list<ToxicityScore>>",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "MillisecondInterval": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "begin": {
                "docs": "Start time of the interval in milliseconds.",
                "type": "integer",
              },
              "end": {
                "docs": "End time of the interval in milliseconds.",
                "type": "integer",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "Models": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "burst": "optional<Empty>",
              "face": "optional<Face>",
              "facemesh": "optional<Empty>",
              "language": "optional<Language>",
              "ner": "optional<Ner>",
              "prosody": "optional<Prosody>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "ModelsPredictions": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "burst": "optional<PredictionsOptionalNullBurstPrediction>",
              "face": "optional<PredictionsOptionalNullFacePrediction>",
              "facemesh": "optional<PredictionsOptionalNullFacemeshPrediction>",
              "language": "optional<PredictionsOptionalTranscriptionMetadataLanguagePrediction>",
              "ner": "optional<PredictionsOptionalTranscriptionMetadataNerPrediction>",
              "prosody": "optional<PredictionsOptionalTranscriptionMetadataProsodyPrediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Ner": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "identify_speakers": {
                "default": false,
                "docs": "Whether to return identifiers for speakers over time. If `true`, unique identifiers will be assigned to spoken words to differentiate different speakers. If `false`, all speakers will be tagged with an `unknown` ID.",
                "type": "optional<boolean>",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "NerPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence in this text.",
                "type": "optional<double>",
              },
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
              "entity": {
                "docs": "The recognized topic or entity.",
                "type": "string",
              },
              "entity_confidence": {
                "docs": "Our NER model's relative confidence in the recognized topic or entity.",
                "type": "double",
              },
              "link_word": {
                "docs": "The specific word to which the emotion predictions are linked.",
                "type": "string",
              },
              "position": "PositionInterval",
              "speaker_confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence that this text was spoken by this speaker.",
                "type": "optional<double>",
              },
              "support": {
                "docs": "A measure of how often the entity is linked to by other entities.",
                "type": "double",
              },
              "time": "optional<TimeInterval>",
              "uri": {
                "docs": "A URL which provides more information about the recognized topic or entity.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Null": {
            "docs": "No associated metadata for this model. Value will be `null`.",
            "type": "map<string, unknown>",
          },
          "PauseAssistantMessage": {
            "docs": "Pause responses from EVI. Chat history is still saved and sent after resuming. ",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `pause_assistant_message` for our server to correctly identify and process it as a Pause Assistant message.

Once this message is sent, EVI will not respond until a [Resume Assistant message](/reference/empathic-voice-interface-evi/chat/chat#send.Resume%20Assistant%20Message.type) is sent. When paused, EVI won’t respond, but transcriptions of your audio inputs will still be recorded.",
                "type": "optional<literal<"pause_assistant_message">>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "PositionInterval": {
            "docs": "Position of a segment of text within a larger document, measured in characters. Uses zero-based indexing. The beginning index is inclusive and the end index is exclusive.",
            "inline": undefined,
            "properties": {
              "begin": {
                "docs": "The index of the first character in the text segment, inclusive.",
                "type": "uint64",
              },
              "end": {
                "docs": "The index of the last character in the text segment, exclusive.",
                "type": "uint64",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Prediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "file": {
                "docs": "A file path relative to the top level source URL or file.",
                "type": "string",
              },
              "models": "ModelsPredictions",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalNullBurstPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsBurstPrediction>",
              "metadata": "optional<Null>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalNullFacePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsFacePrediction>",
              "metadata": "optional<Null>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalNullFacemeshPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsFacemeshPrediction>",
              "metadata": "optional<Null>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalTranscriptionMetadataLanguagePrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsLanguagePrediction>",
              "metadata": "optional<TranscriptionMetadata>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalTranscriptionMetadataNerPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsNerPrediction>",
              "metadata": "optional<TranscriptionMetadata>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "PredictionsOptionalTranscriptionMetadataProsodyPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "grouped_predictions": "list<GroupedPredictionsProsodyPrediction>",
              "metadata": "optional<TranscriptionMetadata>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Prosody": {
            "docs": "NOTE: the `granularity` field is ignored if transcription is not enabled or if the `window` field has been set.",
            "inline": undefined,
            "properties": {
              "granularity": "optional<Granularity>",
              "identify_speakers": {
                "default": false,
                "docs": "Whether to return identifiers for speakers over time. If `true`, unique identifiers will be assigned to spoken words to differentiate different speakers. If `false`, all speakers will be tagged with an `unknown` ID.",
                "type": "optional<boolean>",
              },
              "window": "optional<Window>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "ProsodyInference": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "scores": {
                "docs": "The confidence scores for 48 emotions within the detected expression of an audio sample.

Scores typically range from 0 to 1, with higher values indicating a stronger confidence level in the measured attribute.

See our guide on [interpreting expression measurement results](/docs/expression-measurement/faq#how-do-i-interpret-my-results) to learn more.",
                "type": "EmotionScores",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ProsodyPrediction": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence in this text.",
                "type": "optional<double>",
              },
              "emotions": {
                "docs": "A high-dimensional embedding in emotion space.",
                "type": "list<EmotionScore>",
              },
              "speaker_confidence": {
                "docs": "Value between `0.0` and `1.0` that indicates our transcription model’s relative confidence that this text was spoken by this speaker.",
                "type": "optional<double>",
              },
              "text": {
                "docs": "A segment of text (like a word or a sentence).",
                "type": "optional<string>",
              },
              "time": "TimeInterval",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Queued": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "created_timestamp_ms": {
                "docs": "When this job was created (Unix timestamp in milliseconds).",
                "type": "long",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Request": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "callback_url": {
                "docs": "If provided, a `POST` request will be made to the URL with the generated predictions on completion or the error message on failure.",
                "type": "optional<string>",
              },
              "files": "list<File>",
              "models": "optional<Models>",
              "notify": {
                "default": false,
                "docs": "Whether to send an email notification to the user upon job completion/failure.",
                "type": "optional<boolean>",
              },
              "transcription": "optional<Transcription>",
              "urls": {
                "docs": "URLs to the media files to be processed. Each must be a valid public URL to a media file (see recommended input filetypes) or an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`) of media files.

If you wish to supply more than 100 URLs, consider providing them as an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`).",
                "type": "optional<list<string>>",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Results": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "errors": "list<Error>",
              "predictions": "list<Prediction>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "ResumeAssistantMessage": {
            "docs": "Resume responses from EVI. Chat history sent while paused will now be sent. ",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `resume_assistant_message` for our server to correctly identify and process it as a Resume Assistant message.

Upon resuming, if any audio input was sent during the pause, EVI will retain context from all messages sent but only respond to the last user message. (e.g., If you ask EVI two questions while paused and then send a `resume_assistant_message`, EVI will respond to the second question and have added the first question to its conversation context.)",
                "type": "optional<literal<"resume_assistant_message">>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "Role": {
            "enum": [
              "assistant",
              "system",
              "user",
              "all",
              "tool",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "SentimentScore": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "name": {
                "docs": "Level of sentiment, ranging from `1` (negative) to `9` (positive)",
                "type": "string",
              },
              "score": {
                "docs": "Prediction for this level of sentiment",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "SessionSettings": {
            "docs": "Settings for this chat session.",
            "inline": undefined,
            "properties": {
              "audio": {
                "docs": "Configuration details for the audio input used during the session. Ensures the audio is being correctly set up for processing.

This optional field is only required when the audio input is encoded in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For detailed instructions on how to configure session settings for PCM Linear 16 audio, please refer to the [Session Settings section](/docs/empathic-voice-interface-evi/configuration#session-settings) on the EVI Configuration page.",
                "type": "optional<AudioConfiguration>",
              },
              "builtin_tools": {
                "docs": "List of built-in tools to enable for the session.

Tools are resources used by EVI to perform various tasks, such as searching the web or calling external APIs. Built-in tools, like web search, are natively integrated, while user-defined tools are created and invoked by the user. To learn more, see our [Tool Use Guide](/docs/empathic-voice-interface-evi/tool-use).

Currently, the only built-in tool Hume provides is **Web Search**. When enabled, Web Search equips EVI with the ability to search the web for up-to-date information.",
                "type": "optional<list<BuiltinToolConfig>>",
              },
              "context": {
                "docs": "Allows developers to inject additional context into the conversation, which is appended to the end of user messages for the session.

When included in a Session Settings message, the provided context can be used to remind the LLM of its role in every user message, prevent it from forgetting important details, or add new relevant information to the conversation.

Set to `null` to disable context injection.",
                "type": "optional<Context>",
              },
              "custom_session_id": {
                "docs": "Unique identifier for the session. Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.

If included, the response sent from Hume to your backend will include this ID. This allows you to correlate frontend users with their incoming messages.

It is recommended to pass a `custom_session_id` if you are using a Custom Language Model. Please see our guide to [using a custom language model](/docs/empathic-voice-interface-evi/custom-language-model) with EVI to learn more.",
                "type": "optional<string>",
              },
              "language_model_api_key": {
                "docs": "Third party API key for the supplemental language model.

When provided, EVI will use this key instead of Hume’s API key for the supplemental LLM. This allows you to bypass rate limits and utilize your own API key as needed.",
                "type": "optional<string>",
              },
              "metadata": {
                "type": "optional<map<string, unknown>>",
              },
              "system_prompt": {
                "docs": "Instructions used to shape EVI’s behavior, responses, and style for the session.

When included in a Session Settings message, the provided Prompt overrides the existing one specified in the EVI configuration. If no Prompt was defined in the configuration, this Prompt will be the one used for the session.

You can use the Prompt to define a specific goal or role for EVI, specifying how it should act or what it should focus on during the conversation. For example, EVI can be instructed to act as a customer support representative, a fitness coach, or a travel advisor, each with its own set of behaviors and response styles.

For help writing a system prompt, see our [Prompting Guide](/docs/empathic-voice-interface-evi/prompting).",
                "type": "optional<string>",
              },
              "tools": {
                "docs": "List of user-defined tools to enable for the session.

Tools are resources used by EVI to perform various tasks, such as searching the web or calling external APIs. Built-in tools, like web search, are natively integrated, while user-defined tools are created and invoked by the user. To learn more, see our [Tool Use Guide](/docs/empathic-voice-interface-evi/tool-use).",
                "type": "optional<list<Tool>>",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `session_settings` for our server to correctly identify and process it as a Session Settings message.

Session settings are temporary and apply only to the current Chat session. These settings can be adjusted dynamically based on the requirements of each session to ensure optimal performance and user experience.

For more information, please refer to the [Session Settings section](/docs/empathic-voice-interface-evi/configuration#session-settings) on the EVI Configuration page.",
                "type": "literal<"session_settings">",
              },
              "variables": {
                "docs": "Dynamic values that can be used to populate EVI prompts.",
                "type": "optional<map<string, optional<string>>>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "SortBy": {
            "enum": [
              "created",
              "started",
              "ended",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Source": {
            "availability": undefined,
            "base-properties": {},
            "discriminant": "type",
            "docs": undefined,
            "encoding": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
            "union": {
              "file": "SourceFile",
              "url": "SourceUrl",
            },
          },
          "SourceFile": {
            "docs": undefined,
            "extends": [
              "File",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "SourceResult": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "error": {
                "docs": "An error message.",
                "type": "optional<string>",
              },
              "results": "optional<Results>",
              "source": "Source",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "SourceUrl": {
            "docs": undefined,
            "extends": [
              "Url",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "State": {
            "availability": undefined,
            "base-properties": {},
            "discriminant": "status",
            "docs": undefined,
            "encoding": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
            "union": {
              "COMPLETED": "StateCompleted",
              "FAILED": "StateFailed",
              "IN_PROGRESS": "StateInProgress",
              "QUEUED": "StateQueued",
            },
          },
          "StateCompleted": {
            "docs": undefined,
            "extends": [
              "Completed",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "StateFailed": {
            "docs": undefined,
            "extends": [
              "Failed",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "StateInProgress": {
            "docs": undefined,
            "extends": [
              "InProgress",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "StateQueued": {
            "docs": undefined,
            "extends": [
              "Queued",
            ],
            "inline": undefined,
            "properties": {},
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Status": {
            "enum": [
              "QUEUED",
              "IN_PROGRESS",
              "COMPLETED",
              "FAILED",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "TimeInterval": {
            "docs": "A time range with a beginning and end, measured in seconds.",
            "inline": undefined,
            "properties": {
              "begin": {
                "docs": "Beginning of time range in seconds.",
                "type": "double",
              },
              "end": {
                "docs": "End of time range in seconds.",
                "type": "double",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Tool": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "description": {
                "docs": "An optional description of what the tool does, used by the supplemental LLM to choose when and how to call the function.",
                "type": "optional<string>",
              },
              "fallback_content": {
                "docs": "Optional text passed to the supplemental LLM if the tool call fails. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation.",
                "type": "optional<string>",
              },
              "name": {
                "docs": "Name of the user-defined tool to be enabled.",
                "type": "string",
              },
              "parameters": {
                "docs": "Parameters of the tool. Is a stringified JSON schema.

These parameters define the inputs needed for the tool’s execution, including the expected data type and description for each input field. Structured as a JSON schema, this format ensures the tool receives data in the expected format.",
                "type": "string",
              },
              "type": {
                "docs": "Type of tool. Set to `function` for user-defined tools.",
                "type": "ToolType",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ToolCallMessage": {
            "docs": "When provided, the output is a tool call.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "name": {
                "docs": "Name of the tool called.",
                "type": "string",
              },
              "parameters": {
                "docs": "Parameters of the tool call. Is a stringified JSON schema.",
                "type": "string",
              },
              "response_required": {
                "docs": "Indicates whether a response to the tool call is required from the developer, either in the form of a [Tool Response message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Response%20Message.type) or a [Tool Error message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Error%20Message.type).",
                "type": "boolean",
              },
              "tool_call_id": {
                "docs": "The unique identifier for a specific tool call instance.

This ID is used to track the request and response of a particular tool invocation, ensuring that the correct response is linked to the appropriate request.",
                "type": "string",
              },
              "tool_type": {
                "docs": "Type of tool called. Either `builtin` for natively implemented tools, like web search, or `function` for user-defined tools.",
                "type": "ToolType",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a Tool Call message, this must be `tool_call`.

This message indicates that the supplemental LLM has detected a need to invoke the specified tool.",
                "type": "optional<literal<"tool_call">>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ToolErrorMessage": {
            "docs": "When provided, the output is a function call error.",
            "inline": undefined,
            "properties": {
              "code": {
                "docs": "Error code. Identifies the type of error encountered.",
                "type": "optional<string>",
              },
              "content": {
                "docs": "Optional text passed to the supplemental LLM in place of the tool call result. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation if the tool errors.",
                "type": "optional<string>",
              },
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "error": {
                "docs": "Error message from the tool call, not exposed to the LLM or user.",
                "type": "string",
              },
              "level": {
                "docs": "Indicates the severity of an error; for a Tool Error message, this must be `warn` to signal an unexpected event.",
                "type": "optional<ErrorLevel>",
              },
              "tool_call_id": {
                "docs": "The unique identifier for a specific tool call instance.

This ID is used to track the request and response of a particular tool invocation, ensuring that the Tool Error message is linked to the appropriate tool call request. The specified `tool_call_id` must match the one received in the [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).",
                "type": "string",
              },
              "tool_type": {
                "docs": "Type of tool called. Either `builtin` for natively implemented tools, like web search, or `function` for user-defined tools.",
                "type": "optional<ToolType>",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a Tool Error message, this must be `tool_error`.

Upon receiving a [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type) and failing to invoke the function, this message is sent to notify EVI of the tool's failure.",
                "type": "optional<literal<"tool_error">>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ToolResponseMessage": {
            "docs": "When provided, the output is a function call response.",
            "inline": undefined,
            "properties": {
              "content": {
                "docs": "Return value of the tool call. Contains the output generated by the tool to pass back to EVI.",
                "type": "string",
              },
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "tool_call_id": {
                "docs": "The unique identifier for a specific tool call instance.

This ID is used to track the request and response of a particular tool invocation, ensuring that the correct response is linked to the appropriate request. The specified `tool_call_id` must match the one received in the [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.tool_call_id).",
                "type": "string",
              },
              "tool_name": {
                "type": "optional<string>",
              },
              "tool_type": {
                "type": "optional<ToolType>",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a Tool Response message, this must be `tool_response`.

Upon receiving a [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type) and successfully invoking the function, this message is sent to convey the result of the function call back to EVI.",
                "type": "optional<literal<"tool_response">>",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ToolType": {
            "enum": [
              "builtin",
              "function",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "ToxicityScore": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "name": {
                "docs": "Category of toxicity.",
                "type": "string",
              },
              "score": {
                "docs": "Prediction for this category of toxicity",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Transcription": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "language": "optional<Bcp47Tag>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "TranscriptionMetadata": {
            "docs": "Transcription metadata for your media file.",
            "inline": undefined,
            "properties": {
              "confidence": {
                "docs": "Value between `0.0` and `1.0` indicating our transcription model’s relative confidence in the transcription of your media file.",
                "type": "double",
              },
              "detected_language": "optional<Bcp47Tag>",
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Url": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "url": {
                "docs": "The URL of the source media file.",
                "type": "string",
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "UserInput": {
            "docs": "User text to insert into the conversation. Text sent through a User Input message is treated as the user’s speech to EVI. EVI processes this input and provides a corresponding response.

Expression measurement results are not available for User Input messages, as the prosody model relies on audio input and cannot process text alone.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "text": {
                "docs": "User text to insert into the conversation. Text sent through a User Input message is treated as the user’s speech to EVI. EVI processes this input and provides a corresponding response.

Expression measurement results are not available for User Input messages, as the prosody model relies on audio input and cannot process text alone.",
                "type": "string",
              },
              "type": {
                "docs": "The type of message sent through the socket; must be `user_input` for our server to correctly identify and process it as a User Input message.",
                "type": "literal<"user_input">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "UserInterruption": {
            "docs": "When provided, the output is an interruption.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "time": {
                "docs": "Unix timestamp of the detected user interruption.",
                "type": "integer",
              },
              "type": {
                "docs": "The type of message sent through the socket; for a User Interruption message, this must be `user_interruption`.

This message indicates the user has interrupted the assistant’s response. EVI detects the interruption in real-time and sends this message to signal the interruption event. This message allows the system to stop the current audio playback, clear the audio queue, and prepare to handle new user input.",
                "type": "literal<"user_interruption">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "UserMessage": {
            "docs": "When provided, the output is a user message.",
            "inline": undefined,
            "properties": {
              "custom_session_id": {
                "docs": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                "type": "optional<string>",
              },
              "from_text": {
                "docs": "Indicates if this message was inserted into the conversation as text from a [User Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text) message.",
                "type": "boolean",
              },
              "interim": {
                "docs": "Indicates if this message contains an immediate and unfinalized transcript of the user's audio input. If it does, words may be repeated across successive UserMessage messages as our transcription model becomes more confident about what was said with additional context. Interim messages are useful to detect if the user is interrupting during audio playback on the client. Even without a finalized transcription, along with `UserInterrupt` messages, interim `UserMessages` are useful for detecting if the user is interrupting during audio playback on the client, signaling to stop playback in your application.",
                "type": "boolean",
              },
              "message": {
                "docs": "Transcript of the message.",
                "type": "ChatMessage",
              },
              "models": {
                "docs": "Inference model results.",
                "type": "Inference",
              },
              "time": {
                "docs": "Start and End time of user message.",
                "type": "MillisecondInterval",
              },
              "type": {
                "type": "literal<"user_message">",
              },
            },
            "source": {
              "openapi": "../asyncapi.json",
            },
          },
          "When": {
            "enum": [
              "created_before",
              "created_after",
            ],
            "inline": undefined,
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
          "Window": {
            "docs": undefined,
            "inline": undefined,
            "properties": {
              "length": {
                "default": 4,
                "docs": "The length of the sliding window.",
                "type": "optional<double>",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": undefined,
                  "min": 0.5,
                  "multipleOf": undefined,
                },
              },
              "step": {
                "default": 1,
                "docs": "The step size of the sliding window.",
                "type": "optional<double>",
                "validation": {
                  "exclusiveMax": undefined,
                  "exclusiveMin": undefined,
                  "max": undefined,
                  "min": 0.5,
                  "multipleOf": undefined,
                },
              },
            },
            "source": {
              "openapi": "../openapi.yaml",
            },
          },
        },
      },
      "rawContents": "service:
  auth: false
  base-path: ''
  endpoints:
    List Jobs:
      path: /v0/batch/jobs
      method: GET
      auth: true
      docs: Sort and filter jobs.
      source:
        openapi: ../openapi.yaml
      display-name: List Jobs
      request:
        name: ListJobsRequest
        query-parameters:
          limit:
            type: optional<integer>
            default: 50
            docs: The maximum number of jobs to include in the response.
          status:
            type: optional<Status>
            allow-multiple: true
            docs: Include only jobs with these statuses.
          when:
            type: optional<When>
            docs: >-
              Include only jobs that were created before or after
              `timestamp_ms`.
          timestamp_ms:
            type: optional<long>
            docs: Defaults to the current date and time. See `when`.
          sort_by:
            type: optional<SortBy>
            docs: The job timestamp to sort by.
          direction:
            type: optional<Direction>
            docs: The sort direction.
      response:
        docs: ''
        type: list<JobRequest>
        status-code: 200
      examples:
        - response:
            body:
              - user_id: user_id
                job_id: job_id
                request:
                  urls:
                    - urls
                  callback_url: callback_url
                  notify: true
                  files:
                    - md5sum: md5sum
                state:
                  created_timestamp_ms: 1000000
                  status: QUEUED
    Start Job:
      path: /v0/batch/jobs
      method: POST
      auth: true
      docs: Start a new batch job.
      source:
        openapi: ../openapi.yaml
      display-name: Start Job
      request:
        name: BaseRequest
        body:
          properties:
            models: optional<Models>
            transcription: optional<Transcription>
            urls:
              type: optional<list<string>>
              docs: >-
                URLs to the media files to be processed. Each must be a valid
                public URL to a media file (see recommended input filetypes) or
                an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`) of media
                files.


                If you wish to supply more than 100 URLs, consider providing
                them as an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`).
            callback_url:
              type: optional<string>
              docs: >-
                If provided, a `POST` request will be made to the URL with the
                generated predictions on completion or the error message on
                failure.
            notify:
              type: optional<boolean>
              docs: >-
                Whether to send an email notification to the user upon job
                failure.
              default: false
        content-type: application/json; charset=utf-8
      response:
        docs: ''
        type: JobId
        status-code: 200
      examples:
        - request: {}
          response:
            body:
              job_id: job_id
    Get Job Predictions:
      path: /v0/batch/jobs/{id}/predictions
      method: GET
      auth: true
      docs: Get the JSON predictions of a completed job.
      source:
        openapi: ../openapi.yaml
      path-parameters:
        id: string
      display-name: Get Job Predictions
      response:
        docs: ''
        type: list<SourceResult>
        status-code: 200
      examples:
        - path-parameters:
            id: id
          response:
            body:
              - source:
                  url: url
                  type: url
                results:
                  predictions:
                    - file: file
                      models: {}
                  errors:
                    - message: message
                      file: file
                error: error
  source:
    openapi: ../openapi.yaml
types:
  Bcp47Tag:
    enum:
      - zh
      - da
      - nl
      - en
      - value: en-AU
        name: EnAu
      - value: en-IN
        name: EnIn
      - value: en-NZ
        name: EnNz
      - value: en-GB
        name: EnGb
      - fr
      - value: fr-CA
        name: FrCa
      - de
      - hi
      - value: hi-Latn
        name: HiLatn
      - id
      - it
      - ja
      - ko
      - 'no'
      - pl
      - pt
      - value: pt-BR
        name: PtBr
      - value: pt-PT
        name: PtPt
      - ru
      - es
      - value: es-419
        name: Es419
      - sv
      - ta
      - tr
      - uk
    source:
      openapi: ../openapi.yaml
  BoundingBox:
    docs: A bounding box around a face.
    properties:
      x:
        type: double
        docs: x-coordinate of bounding box top left corner.
      'y':
        type: double
        docs: y-coordinate of bounding box top left corner.
      w:
        type: double
        docs: Bounding box width.
      h:
        type: double
        docs: Bounding box height.
    source:
      openapi: ../openapi.yaml
  BurstPrediction:
    properties:
      time: TimeInterval
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
      descriptions:
        docs: Modality-specific descriptive features and their scores.
        type: list<DescriptionsScore>
    source:
      openapi: ../openapi.yaml
  Completed:
    properties:
      created_timestamp_ms:
        type: long
        docs: When this job was created (Unix timestamp in milliseconds).
      started_timestamp_ms:
        type: long
        docs: When this job started (Unix timestamp in milliseconds).
      ended_timestamp_ms:
        type: long
        docs: When this job ended (Unix timestamp in milliseconds).
      num_predictions:
        type: integer
        docs: The number of predictions that were generated by this job.
      num_errors:
        type: integer
        docs: The number of errors that occurred while running this job.
    source:
      openapi: ../openapi.yaml
  DescriptionsScore:
    properties:
      name:
        type: string
        docs: Name of the descriptive feature being expressed.
      score:
        type: string
        docs: Embedding value for the descriptive feature being expressed.
    source:
      openapi: ../openapi.yaml
  Direction:
    enum:
      - asc
      - desc
    source:
      openapi: ../openapi.yaml
  EmotionScore:
    properties:
      name:
        type: string
        docs: Name of the emotion being expressed.
      score:
        type: string
        docs: Embedding value for the emotion being expressed.
    source:
      openapi: ../openapi.yaml
  Empty:
    type: map<string, unknown>
    docs: >-
      To include predictions for this model type, set this field to `{}`. It is
      currently not configurable further.
  Error:
    docs: When provided, the output is an error message.
    properties:
      type:
        type: literal<"error">
        docs: >-
          The type of message sent through the socket; for a Web Socket Error
          message, this must be `error`.


          This message indicates a disruption in the WebSocket connection, such
          as an unexpected disconnection, protocol error, or data transmission
          issue.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      code:
        type: string
        docs: Error code. Identifies the type of error encountered.
      slug:
        type: string
        docs: >-
          Short, human-readable identifier and description for the error. See a
          complete list of error slugs on the [Errors
          page](/docs/resources/errors).
      message:
        type: string
        docs: Detailed description of the error.
    source:
      openapi: ../asyncapi.json
  Face:
    properties:
      fps_pred:
        type: optional<double>
        docs: >-
          Number of frames per second to process. Other frames will be omitted
          from the response. Set to `0` to process every frame.
        default: 3
      prob_threshold:
        type: optional<double>
        docs: >-
          Face detection probability threshold. Faces detected with a
          probability less than this threshold will be omitted from the
          response.
        default: 0.99
        validation:
          min: 0
          max: 1
      identify_faces:
        type: optional<boolean>
        docs: >-
          Whether to return identifiers for faces across frames. If `true`,
          unique identifiers will be assigned to face bounding boxes to
          differentiate different faces. If `false`, all faces will be tagged
          with an `unknown` ID.
        default: false
      min_face_size:
        type: optional<uint64>
        docs: >-
          Minimum bounding box side length in pixels to treat as a face. Faces
          detected with a bounding box side length in pixels less than this
          threshold will be omitted from the response.
      facs: optional<Empty>
      descriptions: optional<Empty>
      save_faces:
        type: optional<boolean>
        docs: >-
          Whether to extract and save the detected faces in the artifacts zip
          created by each job.
        default: false
    source:
      openapi: ../openapi.yaml
  FacePrediction:
    properties:
      frame:
        type: uint64
        docs: Frame number
      time:
        type: double
        docs: Time in seconds when face detection occurred.
      prob:
        type: double
        docs: The predicted probability that a detected face was actually a face.
      box: BoundingBox
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
      facs:
        type: optional<list<FacsScore>>
        docs: FACS 2.0 features and their scores.
      descriptions:
        type: optional<list<DescriptionsScore>>
        docs: Modality-specific descriptive features and their scores.
    source:
      openapi: ../openapi.yaml
  FacemeshPrediction:
    properties:
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
    source:
      openapi: ../openapi.yaml
  FacsScore:
    properties:
      name:
        type: string
        docs: Name of the FACS 2.0 feature being expressed.
      score:
        type: string
        docs: Embedding value for the FACS 2.0 feature being expressed.
    source:
      openapi: ../openapi.yaml
  Failed:
    properties:
      created_timestamp_ms:
        type: long
        docs: When this job was created (Unix timestamp in milliseconds).
      started_timestamp_ms:
        type: long
        docs: When this job started (Unix timestamp in milliseconds).
      ended_timestamp_ms:
        type: long
        docs: When this job ended (Unix timestamp in milliseconds).
      message:
        type: string
        docs: An error message.
    source:
      openapi: ../openapi.yaml
  File:
    properties:
      filename:
        type: optional<string>
        docs: The name of the file.
      content_type:
        type: optional<string>
        docs: The content type of the file.
      md5sum:
        type: string
        docs: The MD5 checksum of the file.
    source:
      openapi: ../openapi.yaml
  Granularity:
    enum:
      - word
      - sentence
      - utterance
      - conversational_turn
    docs: >-
      The granularity at which to generate predictions. `utterance` corresponds
      to a natural pause or break in conversation, while `conversational_turn`
      corresponds to a change in speaker.
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsBurstPrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<BurstPrediction>
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsFacePrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<FacePrediction>
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsFacemeshPrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<FacemeshPrediction>
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsLanguagePrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<LanguagePrediction>
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsNerPrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<NerPrediction>
    source:
      openapi: ../openapi.yaml
  GroupedPredictionsProsodyPrediction:
    properties:
      id:
        type: string
        docs: >-
          An automatically generated label to identify individuals in your media
          file. Will be `unknown` if you have chosen to disable identification,
          or if the model is unable to distinguish between individuals.
      predictions: list<ProsodyPrediction>
    source:
      openapi: ../openapi.yaml
  InProgress:
    properties:
      created_timestamp_ms:
        type: long
        docs: When this job was created (Unix timestamp in milliseconds).
      started_timestamp_ms:
        type: long
        docs: When this job started (Unix timestamp in milliseconds).
    source:
      openapi: ../openapi.yaml
  JobRequest:
    properties:
      user_id:
        type: string
        docs: Your user ID.
        validation:
          format: uuid
      job_id:
        type: string
        docs: The ID associated with this job.
        validation:
          format: uuid
      request: Request
      state: State
    source:
      openapi: ../openapi.yaml
  JobId:
    properties:
      job_id:
        type: string
        docs: The ID of the started job.
        validation:
          format: uuid
    source:
      openapi: ../openapi.yaml
  Language:
    properties:
      granularity: optional<Granularity>
      identify_speakers:
        type: optional<boolean>
        docs: >-
          Whether to return identifiers for speakers over time. If `true`,
          unique identifiers will be assigned to spoken words to differentiate
          different speakers. If `false`, all speakers will be tagged with an
          `unknown` ID.
        default: false
      sentiment: optional<Empty>
      toxicity: optional<Empty>
    source:
      openapi: ../openapi.yaml
  LanguagePrediction:
    properties:
      text:
        type: string
        docs: A segment of text (like a word or a sentence).
      position: PositionInterval
      time: optional<TimeInterval>
      confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence in this text.
      speaker_confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence that this text was spoken by this speaker.
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
      sentiment:
        type: optional<list<SentimentScore>>
        docs: >-
          Sentiment predictions returned as a distribution. This model predicts
          the probability that a given text could be interpreted as having each
          sentiment level from `1` (negative) to `9` (positive).


          Compared to returning one estimate of sentiment, this enables a more
          nuanced analysis of a text's meaning. For example, a text with very
          neutral sentiment would have an average rating of `5`. But also a text
          that could be interpreted as having very positive sentiment or very
          negative sentiment would also have an average rating of `5`. The
          average sentiment is less informative than the distribution over
          sentiment, so this API returns a value for each sentiment level.
      toxicity:
        type: optional<list<ToxicityScore>>
        docs: >-
          Toxicity predictions returned as probabilities that the text can be
          classified into the following categories: `toxic`, `severe_toxic`,
          `obscene`, `threat`, `insult`, and `identity_hate`.
    source:
      openapi: ../openapi.yaml
  Models:
    properties:
      face: optional<Face>
      burst: optional<Empty>
      prosody: optional<Prosody>
      language: optional<Language>
      ner: optional<Ner>
      facemesh: optional<Empty>
    source:
      openapi: ../openapi.yaml
  ModelsPredictions:
    properties:
      face: optional<PredictionsOptionalNullFacePrediction>
      burst: optional<PredictionsOptionalNullBurstPrediction>
      prosody: optional<PredictionsOptionalTranscriptionMetadataProsodyPrediction>
      language: optional<PredictionsOptionalTranscriptionMetadataLanguagePrediction>
      ner: optional<PredictionsOptionalTranscriptionMetadataNerPrediction>
      facemesh: optional<PredictionsOptionalNullFacemeshPrediction>
    source:
      openapi: ../openapi.yaml
  Ner:
    properties:
      identify_speakers:
        type: optional<boolean>
        docs: >-
          Whether to return identifiers for speakers over time. If `true`,
          unique identifiers will be assigned to spoken words to differentiate
          different speakers. If `false`, all speakers will be tagged with an
          `unknown` ID.
        default: false
    source:
      openapi: ../openapi.yaml
  NerPrediction:
    properties:
      entity:
        type: string
        docs: The recognized topic or entity.
      position: PositionInterval
      entity_confidence:
        type: double
        docs: Our NER model's relative confidence in the recognized topic or entity.
      support:
        type: double
        docs: A measure of how often the entity is linked to by other entities.
      uri:
        type: string
        docs: >-
          A URL which provides more information about the recognized topic or
          entity.
      link_word:
        type: string
        docs: The specific word to which the emotion predictions are linked.
      time: optional<TimeInterval>
      confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence in this text.
      speaker_confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence that this text was spoken by this speaker.
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
    source:
      openapi: ../openapi.yaml
  'Null':
    type: map<string, unknown>
    docs: No associated metadata for this model. Value will be `null`.
  PositionInterval:
    docs: >-
      Position of a segment of text within a larger document, measured in
      characters. Uses zero-based indexing. The beginning index is inclusive and
      the end index is exclusive.
    properties:
      begin:
        type: uint64
        docs: The index of the first character in the text segment, inclusive.
      end:
        type: uint64
        docs: The index of the last character in the text segment, exclusive.
    source:
      openapi: ../openapi.yaml
  Prediction:
    properties:
      file:
        type: string
        docs: A file path relative to the top level source URL or file.
      models: ModelsPredictions
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalNullBurstPrediction:
    properties:
      metadata: optional<Null>
      grouped_predictions: list<GroupedPredictionsBurstPrediction>
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalNullFacePrediction:
    properties:
      metadata: optional<Null>
      grouped_predictions: list<GroupedPredictionsFacePrediction>
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalNullFacemeshPrediction:
    properties:
      metadata: optional<Null>
      grouped_predictions: list<GroupedPredictionsFacemeshPrediction>
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalTranscriptionMetadataLanguagePrediction:
    properties:
      metadata: optional<TranscriptionMetadata>
      grouped_predictions: list<GroupedPredictionsLanguagePrediction>
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalTranscriptionMetadataNerPrediction:
    properties:
      metadata: optional<TranscriptionMetadata>
      grouped_predictions: list<GroupedPredictionsNerPrediction>
    source:
      openapi: ../openapi.yaml
  PredictionsOptionalTranscriptionMetadataProsodyPrediction:
    properties:
      metadata: optional<TranscriptionMetadata>
      grouped_predictions: list<GroupedPredictionsProsodyPrediction>
    source:
      openapi: ../openapi.yaml
  Prosody:
    docs: >-
      NOTE: the `granularity` field is ignored if transcription is not enabled
      or if the `window` field has been set.
    properties:
      granularity: optional<Granularity>
      identify_speakers:
        type: optional<boolean>
        docs: >-
          Whether to return identifiers for speakers over time. If `true`,
          unique identifiers will be assigned to spoken words to differentiate
          different speakers. If `false`, all speakers will be tagged with an
          `unknown` ID.
        default: false
      window: optional<Window>
    source:
      openapi: ../openapi.yaml
  ProsodyPrediction:
    properties:
      text:
        type: optional<string>
        docs: A segment of text (like a word or a sentence).
      time: TimeInterval
      confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence in this text.
      speaker_confidence:
        type: optional<double>
        docs: >-
          Value between `0.0` and `1.0` that indicates our transcription model’s
          relative confidence that this text was spoken by this speaker.
      emotions:
        docs: A high-dimensional embedding in emotion space.
        type: list<EmotionScore>
    source:
      openapi: ../openapi.yaml
  Queued:
    properties:
      created_timestamp_ms:
        type: long
        docs: When this job was created (Unix timestamp in milliseconds).
    source:
      openapi: ../openapi.yaml
  Request:
    properties:
      models: optional<Models>
      transcription: optional<Transcription>
      urls:
        type: optional<list<string>>
        docs: >-
          URLs to the media files to be processed. Each must be a valid public
          URL to a media file (see recommended input filetypes) or an archive
          (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`) of media files.


          If you wish to supply more than 100 URLs, consider providing them as
          an archive (`.zip`, `.tar.gz`, `.tar.bz2`, `.tar.xz`).
      callback_url:
        type: optional<string>
        docs: >-
          If provided, a `POST` request will be made to the URL with the
          generated predictions on completion or the error message on failure.
      notify:
        type: optional<boolean>
        docs: >-
          Whether to send an email notification to the user upon job
          completion/failure.
        default: false
      files: list<File>
    source:
      openapi: ../openapi.yaml
  Results:
    properties:
      predictions: list<Prediction>
      errors: list<Error>
    source:
      openapi: ../openapi.yaml
  SentimentScore:
    properties:
      name:
        type: string
        docs: Level of sentiment, ranging from `1` (negative) to `9` (positive)
      score:
        type: string
        docs: Prediction for this level of sentiment
    source:
      openapi: ../openapi.yaml
  SortBy:
    enum:
      - created
      - started
      - ended
    source:
      openapi: ../openapi.yaml
  Source:
    discriminant: type
    base-properties: {}
    union:
      url: SourceUrl
      file: SourceFile
    source:
      openapi: ../openapi.yaml
  SourceResult:
    properties:
      source: Source
      results: optional<Results>
      error:
        type: optional<string>
        docs: An error message.
    source:
      openapi: ../openapi.yaml
  SourceFile:
    properties: {}
    extends:
      - File
    source:
      openapi: ../openapi.yaml
  SourceUrl:
    properties: {}
    extends:
      - Url
    source:
      openapi: ../openapi.yaml
  Url:
    properties:
      url:
        type: string
        docs: The URL of the source media file.
    source:
      openapi: ../openapi.yaml
  State:
    discriminant: status
    base-properties: {}
    union:
      QUEUED: StateQueued
      IN_PROGRESS: StateInProgress
      COMPLETED: StateCompleted
      FAILED: StateFailed
    source:
      openapi: ../openapi.yaml
  StateCompleted:
    properties: {}
    extends:
      - Completed
    source:
      openapi: ../openapi.yaml
  StateFailed:
    properties: {}
    extends:
      - Failed
    source:
      openapi: ../openapi.yaml
  StateInProgress:
    properties: {}
    extends:
      - InProgress
    source:
      openapi: ../openapi.yaml
  StateQueued:
    properties: {}
    extends:
      - Queued
    source:
      openapi: ../openapi.yaml
  Status:
    enum:
      - QUEUED
      - IN_PROGRESS
      - COMPLETED
      - FAILED
    source:
      openapi: ../openapi.yaml
  TimeInterval:
    docs: A time range with a beginning and end, measured in seconds.
    properties:
      begin:
        type: double
        docs: Beginning of time range in seconds.
      end:
        type: double
        docs: End of time range in seconds.
    source:
      openapi: ../openapi.yaml
  ToxicityScore:
    properties:
      name:
        type: string
        docs: Category of toxicity.
      score:
        type: string
        docs: Prediction for this category of toxicity
    source:
      openapi: ../openapi.yaml
  Transcription:
    properties:
      language: optional<Bcp47Tag>
    source:
      openapi: ../openapi.yaml
  TranscriptionMetadata:
    docs: Transcription metadata for your media file.
    properties:
      confidence:
        type: double
        docs: >-
          Value between `0.0` and `1.0` indicating our transcription model’s
          relative confidence in the transcription of your media file.
      detected_language: optional<Bcp47Tag>
    source:
      openapi: ../openapi.yaml
  When:
    enum:
      - created_before
      - created_after
    source:
      openapi: ../openapi.yaml
  Window:
    properties:
      length:
        type: optional<double>
        docs: The length of the sliding window.
        default: 4
        validation:
          min: 0.5
      step:
        type: optional<double>
        docs: The step size of the sliding window.
        default: 1
        validation:
          min: 0.5
    source:
      openapi: ../openapi.yaml
  AssistantInput:
    docs: When provided, the input is spoken by EVI.
    properties:
      type:
        type: literal<"assistant_input">
        docs: >-
          The type of message sent through the socket; must be `assistant_input`
          for our server to correctly identify and process it as an Assistant
          Input message.
      custom_session_id:
        type: optional<string>
      text:
        type: string
        docs: >-
          Assistant text to synthesize into spoken audio and insert into the
          conversation.


          EVI uses this text to generate spoken audio using our proprietary
          expressive text-to-speech model. Our model adds appropriate emotional
          inflections and tones to the text based on the user’s expressions and
          the context of the conversation. The synthesized audio is streamed
          back to the user as an [Assistant
          Message](/reference/empathic-voice-interface-evi/chat/chat#receive.Assistant%20Message.type).
    source:
      openapi: ../asyncapi.json
  AudioConfiguration:
    properties:
      encoding:
        type: Encoding
        docs: Encoding format of the audio input, such as `linear16`.
      channels:
        type: integer
        docs: Number of audio channels.
      sample_rate:
        type: integer
        docs: >-
          Audio sample rate. Number of samples per second in the audio input,
          measured in Hertz.
    source:
      openapi: ../asyncapi.json
  AudioInput:
    docs: When provided, the input is audio.
    properties:
      type:
        type: literal<"audio_input">
        docs: >-
          The type of message sent through the socket; must be `audio_input` for
          our server to correctly identify and process it as an Audio Input
          message.


          This message is used for sending audio input data to EVI for
          processing and expression measurement. Audio data should be sent as a
          continuous stream, encoded in Base64.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      data:
        type: string
        docs: >-
          Base64 encoded audio input to insert into the conversation.


          The content of an Audio Input message is treated as the user’s speech
          to EVI and must be streamed continuously. Pre-recorded audio files are
          not supported.


          For optimal transcription quality, the audio data should be
          transmitted in small chunks.


          Hume recommends streaming audio with a buffer window of 20
          milliseconds (ms), or 100 milliseconds (ms) for web applications.
    source:
      openapi: ../asyncapi.json
  BuiltInTool:
    enum:
      - web_search
      - hang_up
    source:
      openapi: ../asyncapi.json
  BuiltinToolConfig:
    properties:
      name:
        type: BuiltInTool
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
    source:
      openapi: ../asyncapi.json
  Context:
    properties:
      type:
        type: optional<ContextType>
        docs: >-
          The persistence level of the injected context. Specifies how long the
          injected context will remain active in the session.


          There are three possible context types:


          - **Persistent**: The context is appended to all user messages for the
          duration of the session.


          - **Temporary**: The context is appended only to the next user
          message.

           - **Editable**: The original context is updated to reflect the new context.

           If the type is not specified, it will default to `temporary`.
      text:
        type: string
        docs: >-
          The context to be injected into the conversation. Helps inform the
          LLM's response by providing relevant information about the ongoing
          conversation.


          This text will be appended to the end of user messages based on the
          chosen persistence level. For example, if you want to remind EVI of
          its role as a helpful weather assistant, the context you insert will
          be appended to the end of user messages as `{Context: You are a
          helpful weather assistant}`.
    source:
      openapi: ../asyncapi.json
  ContextType:
    enum:
      - editable
      - persistent
      - temporary
    source:
      openapi: ../asyncapi.json
  Encoding:
    type: literal<"linear16">
  ErrorLevel:
    type: literal<"warn">
  PauseAssistantMessage:
    docs: >-
      Pause responses from EVI. Chat history is still saved and sent after
      resuming. 
    properties:
      type:
        type: optional<literal<"pause_assistant_message">>
        docs: >-
          The type of message sent through the socket; must be
          `pause_assistant_message` for our server to correctly identify and
          process it as a Pause Assistant message.


          Once this message is sent, EVI will not respond until a [Resume
          Assistant
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Resume%20Assistant%20Message.type)
          is sent. When paused, EVI won’t respond, but transcriptions of your
          audio inputs will still be recorded.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: ../asyncapi.json
  ResumeAssistantMessage:
    docs: >-
      Resume responses from EVI. Chat history sent while paused will now be
      sent. 
    properties:
      type:
        type: optional<literal<"resume_assistant_message">>
        docs: >-
          The type of message sent through the socket; must be
          `resume_assistant_message` for our server to correctly identify and
          process it as a Resume Assistant message.


          Upon resuming, if any audio input was sent during the pause, EVI will
          retain context from all messages sent but only respond to the last
          user message. (e.g., If you ask EVI two questions while paused and
          then send a `resume_assistant_message`, EVI will respond to the second
          question and have added the first question to its conversation
          context.)
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: ../asyncapi.json
  SessionSettings:
    docs: Settings for this chat session.
    properties:
      type:
        type: literal<"session_settings">
        docs: >-
          The type of message sent through the socket; must be
          `session_settings` for our server to correctly identify and process it
          as a Session Settings message.


          Session settings are temporary and apply only to the current Chat
          session. These settings can be adjusted dynamically based on the
          requirements of each session to ensure optimal performance and user
          experience.


          For more information, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
      custom_session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the session. Used to manage conversational
          state, correlate frontend and backend data, and persist conversations
          across EVI sessions.


          If included, the response sent from Hume to your backend will include
          this ID. This allows you to correlate frontend users with their
          incoming messages.


          It is recommended to pass a `custom_session_id` if you are using a
          Custom Language Model. Please see our guide to [using a custom
          language
          model](/docs/empathic-voice-interface-evi/custom-language-model) with
          EVI to learn more.
      system_prompt:
        type: optional<string>
        docs: >-
          Instructions used to shape EVI’s behavior, responses, and style for
          the session.


          When included in a Session Settings message, the provided Prompt
          overrides the existing one specified in the EVI configuration. If no
          Prompt was defined in the configuration, this Prompt will be the one
          used for the session.


          You can use the Prompt to define a specific goal or role for EVI,
          specifying how it should act or what it should focus on during the
          conversation. For example, EVI can be instructed to act as a customer
          support representative, a fitness coach, or a travel advisor, each
          with its own set of behaviors and response styles.


          For help writing a system prompt, see our [Prompting
          Guide](/docs/empathic-voice-interface-evi/prompting).
      context:
        type: optional<Context>
        docs: >-
          Allows developers to inject additional context into the conversation,
          which is appended to the end of user messages for the session.


          When included in a Session Settings message, the provided context can
          be used to remind the LLM of its role in every user message, prevent
          it from forgetting important details, or add new relevant information
          to the conversation.


          Set to `null` to disable context injection.
      audio:
        type: optional<AudioConfiguration>
        docs: >-
          Configuration details for the audio input used during the session.
          Ensures the audio is being correctly set up for processing.


          This optional field is only required when the audio input is encoded
          in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For
          detailed instructions on how to configure session settings for PCM
          Linear 16 audio, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
      language_model_api_key:
        type: optional<string>
        docs: >-
          Third party API key for the supplemental language model.


          When provided, EVI will use this key instead of Hume’s API key for the
          supplemental LLM. This allows you to bypass rate limits and utilize
          your own API key as needed.
      tools:
        type: optional<list<Tool>>
        docs: >-
          List of user-defined tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).
      builtin_tools:
        type: optional<list<BuiltinToolConfig>>
        docs: >-
          List of built-in tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).


          Currently, the only built-in tool Hume provides is **Web Search**.
          When enabled, Web Search equips EVI with the ability to search the web
          for up-to-date information.
      metadata:
        type: optional<map<string, unknown>>
      variables:
        type: optional<map<string, optional<string>>>
        docs: Dynamic values that can be used to populate EVI prompts.
    source:
      openapi: ../asyncapi.json
  Tool:
    properties:
      type:
        type: ToolType
        docs: Type of tool. Set to `function` for user-defined tools.
      name:
        type: string
        docs: Name of the user-defined tool to be enabled.
      parameters:
        type: string
        docs: >-
          Parameters of the tool. Is a stringified JSON schema.


          These parameters define the inputs needed for the tool’s execution,
          including the expected data type and description for each input field.
          Structured as a JSON schema, this format ensures the tool receives
          data in the expected format.
      description:
        type: optional<string>
        docs: >-
          An optional description of what the tool does, used by the
          supplemental LLM to choose when and how to call the function.
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
    source:
      openapi: ../asyncapi.json
  ToolErrorMessage:
    docs: When provided, the output is a function call error.
    properties:
      type:
        type: optional<literal<"tool_error">>
        docs: >-
          The type of message sent through the socket; for a Tool Error message,
          this must be `tool_error`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and failing to invoke the function, this message is sent to notify EVI
          of the tool's failure.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_type:
        type: optional<ToolType>
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the Tool Error message is linked to the
          appropriate tool call request. The specified `tool_call_id` must match
          the one received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).
      content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the tool errors.
      error:
        type: string
        docs: Error message from the tool call, not exposed to the LLM or user.
      code:
        type: optional<string>
        docs: Error code. Identifies the type of error encountered.
      level:
        type: optional<ErrorLevel>
        docs: >-
          Indicates the severity of an error; for a Tool Error message, this
          must be `warn` to signal an unexpected event.
    source:
      openapi: ../asyncapi.json
  ToolResponseMessage:
    docs: When provided, the output is a function call response.
    properties:
      type:
        type: optional<literal<"tool_response">>
        docs: >-
          The type of message sent through the socket; for a Tool Response
          message, this must be `tool_response`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and successfully invoking the function, this message is sent to convey
          the result of the function call back to EVI.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request. The specified `tool_call_id` must match the one
          received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.tool_call_id).
      content:
        type: string
        docs: >-
          Return value of the tool call. Contains the output generated by the
          tool to pass back to EVI.
      tool_name:
        type: optional<string>
      tool_type:
        type: optional<ToolType>
    source:
      openapi: ../asyncapi.json
  ToolType:
    enum:
      - builtin
      - function
    source:
      openapi: ../asyncapi.json
  UserInput:
    docs: >-
      User text to insert into the conversation. Text sent through a User Input
      message is treated as the user’s speech to EVI. EVI processes this input
      and provides a corresponding response.


      Expression measurement results are not available for User Input messages,
      as the prosody model relies on audio input and cannot process text alone.
    properties:
      type:
        type: literal<"user_input">
        docs: >-
          The type of message sent through the socket; must be `user_input` for
          our server to correctly identify and process it as a User Input
          message.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      text:
        type: string
        docs: >-
          User text to insert into the conversation. Text sent through a User
          Input message is treated as the user’s speech to EVI. EVI processes
          this input and provides a corresponding response.


          Expression measurement results are not available for User Input
          messages, as the prosody model relies on audio input and cannot
          process text alone.
    source:
      openapi: ../asyncapi.json
  AssistantEnd:
    docs: When provided, the output is an assistant end message.
    properties:
      type:
        type: literal<"assistant_end">
        docs: >-
          The type of message sent through the socket; for an Assistant End
          message, this must be `assistant_end`.


          This message indicates the conclusion of the assistant’s response,
          signaling that the assistant has finished speaking for the current
          conversational turn.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: ../asyncapi.json
  AssistantMessage:
    docs: When provided, the output is an assistant message.
    properties:
      type:
        type: literal<"assistant_message">
        docs: >-
          The type of message sent through the socket; for an Assistant Message,
          this must be `assistant_message`.


          This message contains both a transcript of the assistant’s response
          and the expression measurement predictions of the assistant’s audio
          output.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      id:
        type: optional<string>
        docs: >-
          ID of the assistant message. Allows the Assistant Message to be
          tracked and referenced.
      message:
        type: ChatMessage
        docs: Transcript of the message.
      models:
        type: Inference
        docs: Inference model results.
      from_text:
        type: boolean
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from an [Assistant Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Assistant%20Input.text).
    source:
      openapi: ../asyncapi.json
  AudioOutput:
    docs: >-
      The type of message sent through the socket; for an Audio Output message,
      this must be `audio_output`.
    properties:
      type:
        type: literal<"audio_output">
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      id:
        type: string
        docs: >-
          ID of the audio output. Allows the Audio Output message to be tracked
          and referenced.
      index:
        type: integer
        docs: Index of the chunk of audio relative to the whole audio segment.
      data:
        type: string
        docs: >-
          Base64 encoded audio output. This encoded audio is transmitted to the
          client, where it can be decoded and played back as part of the user
          interaction.
      is_final_chunk:
        type: boolean
        docs: This AudioOutput contains the final chunk for this particular segment.
    source:
      openapi: ../asyncapi.json
  ChatMessageToolResult:
    discriminated: false
    docs: Function call response from client.
    union:
      - type: ToolResponseMessage
      - type: ToolErrorMessage
    source:
      openapi: ../asyncapi.json
    inline: true
  ChatMessage:
    properties:
      role:
        type: Role
        docs: Role of who is providing the message.
      content:
        type: optional<string>
        docs: Transcript of the message.
      tool_call:
        type: optional<ToolCallMessage>
        docs: Function call name and arguments.
      tool_result:
        type: optional<ChatMessageToolResult>
        docs: Function call response from client.
    source:
      openapi: ../asyncapi.json
  ChatMetadata:
    docs: When provided, the output is a chat metadata message.
    properties:
      type:
        type: literal<"chat_metadata">
        docs: >-
          The type of message sent through the socket; for a Chat Metadata
          message, this must be `chat_metadata`.


          The Chat Metadata message is the first message you receive after
          establishing a connection with EVI and contains important identifiers
          for the current Chat session.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      chat_group_id:
        type: string
        docs: >-
          ID of the Chat Group.


          Used to resume a Chat when passed in the
          [resumed_chat_group_id](/reference/empathic-voice-interface-evi/chat/chat#request.query.resumed_chat_group_id)
          query parameter of a subsequent connection request. This allows EVI to
          continue the conversation from where it left off within the Chat
          Group.


          Learn more about [supporting chat
          resumability](/docs/empathic-voice-interface-evi/faq#does-evi-support-chat-resumability)
          from the EVI FAQ.
      chat_id:
        type: string
        docs: >-
          ID of the Chat session. Allows the Chat session to be tracked and
          referenced.
      request_id:
        type: optional<string>
        docs: ID of the initiating request.
    source:
      openapi: ../asyncapi.json
  EmotionScores:
    properties:
      Admiration: double
      Adoration: double
      Aesthetic Appreciation: double
      Amusement: double
      Anger: double
      Anxiety: double
      Awe: double
      Awkwardness: double
      Boredom: double
      Calmness: double
      Concentration: double
      Confusion: double
      Contemplation: double
      Contempt: double
      Contentment: double
      Craving: double
      Desire: double
      Determination: double
      Disappointment: double
      Disgust: double
      Distress: double
      Doubt: double
      Ecstasy: double
      Embarrassment: double
      Empathic Pain: double
      Entrancement: double
      Envy: double
      Excitement: double
      Fear: double
      Guilt: double
      Horror: double
      Interest: double
      Joy: double
      Love: double
      Nostalgia: double
      Pain: double
      Pride: double
      Realization: double
      Relief: double
      Romance: double
      Sadness: double
      Satisfaction: double
      Shame: double
      Surprise (negative): double
      Surprise (positive): double
      Sympathy: double
      Tiredness: double
      Triumph: double
    source:
      openapi: ../asyncapi.json
  Inference:
    properties:
      prosody:
        type: optional<ProsodyInference>
        docs: >-
          Prosody model inference results.


          EVI uses the prosody model to measure 48 emotions related to speech
          and vocal characteristics within a given expression.
    source:
      openapi: ../asyncapi.json
  MillisecondInterval:
    properties:
      begin:
        type: integer
        docs: Start time of the interval in milliseconds.
      end:
        type: integer
        docs: End time of the interval in milliseconds.
    source:
      openapi: ../asyncapi.json
  ProsodyInference:
    properties:
      scores:
        type: EmotionScores
        docs: >-
          The confidence scores for 48 emotions within the detected expression
          of an audio sample.


          Scores typically range from 0 to 1, with higher values indicating a
          stronger confidence level in the measured attribute.


          See our guide on [interpreting expression measurement
          results](/docs/expression-measurement/faq#how-do-i-interpret-my-results)
          to learn more.
    source:
      openapi: ../asyncapi.json
  Role:
    enum:
      - assistant
      - system
      - user
      - all
      - tool
    source:
      openapi: ../asyncapi.json
  ToolCallMessage:
    docs: When provided, the output is a tool call.
    properties:
      name:
        type: string
        docs: Name of the tool called.
      parameters:
        type: string
        docs: Parameters of the tool call. Is a stringified JSON schema.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request.
      type:
        type: optional<literal<"tool_call">>
        docs: >-
          The type of message sent through the socket; for a Tool Call message,
          this must be `tool_call`.


          This message indicates that the supplemental LLM has detected a need
          to invoke the specified tool.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_type:
        type: ToolType
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
      response_required:
        type: boolean
        docs: >-
          Indicates whether a response to the tool call is required from the
          developer, either in the form of a [Tool Response
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Response%20Message.type)
          or a [Tool Error
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Error%20Message.type).
    source:
      openapi: ../asyncapi.json
  UserInterruption:
    docs: When provided, the output is an interruption.
    properties:
      type:
        type: literal<"user_interruption">
        docs: >-
          The type of message sent through the socket; for a User Interruption
          message, this must be `user_interruption`.


          This message indicates the user has interrupted the assistant’s
          response. EVI detects the interruption in real-time and sends this
          message to signal the interruption event. This message allows the
          system to stop the current audio playback, clear the audio queue, and
          prepare to handle new user input.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      time:
        type: integer
        docs: Unix timestamp of the detected user interruption.
    source:
      openapi: ../asyncapi.json
  UserMessage:
    docs: When provided, the output is a user message.
    properties:
      type:
        type: literal<"user_message">
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      message:
        type: ChatMessage
        docs: Transcript of the message.
      models:
        type: Inference
        docs: Inference model results.
      time:
        type: MillisecondInterval
        docs: Start and End time of user message.
      from_text:
        type: boolean
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from a [User
          Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text)
          message.
      interim:
        type: boolean
        docs: >-
          Indicates if this message contains an immediate and unfinalized
          transcript of the user's audio input. If it does, words may be
          repeated across successive UserMessage messages as our transcription
          model becomes more confident about what was said with additional
          context. Interim messages are useful to detect if the user is
          interrupting during audio playback on the client. Even without a
          finalized transcription, along with `UserInterrupt` messages, interim
          `UserMessages` are useful for detecting if the user is interrupting
          during audio playback on the client, signaling to stop playback in
          your application.
    source:
      openapi: ../asyncapi.json
",
    },
    "chat.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "channel": {
          "auth": false,
          "examples": [
            {
              "messages": [
                {
                  "body": {
                    "data": "data",
                    "type": "audio_input",
                  },
                  "type": "publish",
                },
                {
                  "body": {
                    "type": "assistant_end",
                  },
                  "type": "subscribe",
                },
              ],
            },
          ],
          "messages": {
            "publish": {
              "body": "PublishEvent",
              "origin": "client",
            },
            "subscribe": {
              "body": "SubscribeEvent",
              "origin": "server",
            },
          },
          "path": "/chat",
          "query-parameters": {
            "access_token": {
              "default": "",
              "docs": "Access token used for authenticating the client. If not provided, an `api_key` must be provided to authenticate.

The access token is generated using both an API key and a Secret key, which provides an additional layer of security compared to using just an API key.

For more details, refer to the [Authentication Strategies Guide](/docs/introduction/api-key#authentication-strategies).",
              "type": "optional<string>",
            },
            "config_id": {
              "docs": "The unique identifier for an EVI configuration.

Include this ID in your connection request to equip EVI with the Prompt, Language Model, Voice, and Tools associated with the specified configuration. If omitted, EVI will apply [default configuration settings](/docs/empathic-voice-interface-evi/configuration#default-configuration).

For help obtaining this ID, see our [Configuration Guide](/docs/empathic-voice-interface-evi/configuration).",
              "type": "optional<string>",
            },
            "config_version": {
              "docs": "The version number of the EVI configuration specified by the `config_id`.

Configs, as well as Prompts and Tools, are versioned. This versioning system supports iterative development, allowing you to progressively refine configurations and revert to previous versions if needed.

Include this parameter to apply a specific version of an EVI configuration. If omitted, the latest version will be applied.",
              "type": "optional<integer>",
            },
            "resumed_chat_group_id": {
              "docs": "The unique identifier for a Chat Group. Use this field to preserve context from a previous Chat session.

A Chat represents a single session from opening to closing a WebSocket connection. In contrast, a Chat Group is a series of resumed Chats that collectively represent a single conversation spanning multiple sessions. Each Chat includes a Chat Group ID, which is used to preserve the context of previous Chat sessions when starting a new one.

Including the Chat Group ID in the `resumed_chat_group_id` query parameter is useful for seamlessly resuming a Chat after unexpected network disconnections and for picking up conversations exactly where you left off at a later time. This ensures preserved context across multiple sessions.

There are three ways to obtain the Chat Group ID:

- [Chat Metadata](/reference/empathic-voice-interface-evi/chat/chat#receive.Chat%20Metadata.type): Upon establishing a WebSocket connection with EVI, the user receives a Chat Metadata message. This message contains a `chat_group_id`, which can be used to resume conversations within this chat group in future sessions.

- [List Chats endpoint](/reference/empathic-voice-interface-evi/chats/list-chats): Use the GET `/v0/evi/chats` endpoint to obtain the Chat Group ID of individual Chat sessions. This endpoint lists all available Chat sessions and their associated Chat Group ID.

- [List Chat Groups endpoint](/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups): Use the GET `/v0/evi/chat_groups` endpoint to obtain the Chat Group IDs of all Chat Groups associated with an API key. This endpoint returns a list of all available chat groups.",
              "type": "optional<string>",
            },
            "verbose_transcription": {
              "default": false,
              "docs": "A flag to enable verbose transcription. Set this query parameter to `"true"` to have unfinalized user transcripts be sent to the client as interim `UserMessage` messages.",
              "type": "optional<boolean>",
            },
          },
          "url": "prod",
        },
        "imports": {
          "root": "__package__.yml",
        },
        "types": {
          "PublishEvent": {
            "discriminated": false,
            "docs": undefined,
            "encoding": undefined,
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
            "union": [
              {
                "type": "root.AudioInput",
              },
              {
                "type": "root.SessionSettings",
              },
              {
                "type": "root.UserInput",
              },
              {
                "type": "root.AssistantInput",
              },
              {
                "type": "root.ToolResponseMessage",
              },
              {
                "type": "root.ToolErrorMessage",
              },
              {
                "type": "root.PauseAssistantMessage",
              },
              {
                "type": "root.ResumeAssistantMessage",
              },
            ],
          },
          "SubscribeEvent": {
            "discriminated": false,
            "docs": undefined,
            "encoding": undefined,
            "inline": undefined,
            "source": {
              "openapi": "../asyncapi.json",
            },
            "union": [
              {
                "type": "root.AssistantEnd",
              },
              {
                "type": "root.AssistantMessage",
              },
              {
                "type": "root.AudioOutput",
              },
              {
                "type": "root.ChatMetadata",
              },
              {
                "type": "root.Error",
              },
              {
                "type": "root.UserInterruption",
              },
              {
                "type": "root.UserMessage",
              },
              {
                "type": "root.ToolCallMessage",
              },
              {
                "type": "root.ToolResponseMessage",
              },
              {
                "type": "root.ToolErrorMessage",
              },
            ],
          },
        },
      },
      "rawContents": "channel:
  path: /chat
  url: prod
  auth: false
  query-parameters:
    config_id:
      type: optional<string>
      docs: >-
        The unique identifier for an EVI configuration.


        Include this ID in your connection request to equip EVI with the Prompt,
        Language Model, Voice, and Tools associated with the specified
        configuration. If omitted, EVI will apply [default configuration
        settings](/docs/empathic-voice-interface-evi/configuration#default-configuration).


        For help obtaining this ID, see our [Configuration
        Guide](/docs/empathic-voice-interface-evi/configuration).
    config_version:
      type: optional<integer>
      docs: >-
        The version number of the EVI configuration specified by the
        `config_id`.


        Configs, as well as Prompts and Tools, are versioned. This versioning
        system supports iterative development, allowing you to progressively
        refine configurations and revert to previous versions if needed.


        Include this parameter to apply a specific version of an EVI
        configuration. If omitted, the latest version will be applied.
    resumed_chat_group_id:
      type: optional<string>
      docs: >-
        The unique identifier for a Chat Group. Use this field to preserve
        context from a previous Chat session.


        A Chat represents a single session from opening to closing a WebSocket
        connection. In contrast, a Chat Group is a series of resumed Chats that
        collectively represent a single conversation spanning multiple sessions.
        Each Chat includes a Chat Group ID, which is used to preserve the
        context of previous Chat sessions when starting a new one.


        Including the Chat Group ID in the `resumed_chat_group_id` query
        parameter is useful for seamlessly resuming a Chat after unexpected
        network disconnections and for picking up conversations exactly where
        you left off at a later time. This ensures preserved context across
        multiple sessions.


        There are three ways to obtain the Chat Group ID:


        - [Chat
        Metadata](/reference/empathic-voice-interface-evi/chat/chat#receive.Chat%20Metadata.type):
        Upon establishing a WebSocket connection with EVI, the user receives a
        Chat Metadata message. This message contains a `chat_group_id`, which
        can be used to resume conversations within this chat group in future
        sessions.


        - [List Chats
        endpoint](/reference/empathic-voice-interface-evi/chats/list-chats): Use
        the GET `/v0/evi/chats` endpoint to obtain the Chat Group ID of
        individual Chat sessions. This endpoint lists all available Chat
        sessions and their associated Chat Group ID.


        - [List Chat Groups
        endpoint](/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups):
        Use the GET `/v0/evi/chat_groups` endpoint to obtain the Chat Group IDs
        of all Chat Groups associated with an API key. This endpoint returns a
        list of all available chat groups.
    verbose_transcription:
      type: optional<boolean>
      default: false
      docs: >-
        A flag to enable verbose transcription. Set this query parameter to
        `"true"` to have unfinalized user transcripts be sent to the client as
        interim `UserMessage` messages.
    access_token:
      type: optional<string>
      default: ''
      docs: >-
        Access token used for authenticating the client. If not provided, an
        `api_key` must be provided to authenticate.


        The access token is generated using both an API key and a Secret key,
        which provides an additional layer of security compared to using just an
        API key.


        For more details, refer to the [Authentication Strategies
        Guide](/docs/introduction/api-key#authentication-strategies).
  messages:
    subscribe:
      origin: server
      body: SubscribeEvent
    publish:
      origin: client
      body: PublishEvent
  examples:
    - messages:
        - type: publish
          body:
            type: audio_input
            data: data
        - type: subscribe
          body:
            type: assistant_end
imports:
  root: __package__.yml
types:
  SubscribeEvent:
    discriminated: false
    union:
      - type: root.AssistantEnd
      - type: root.AssistantMessage
      - type: root.AudioOutput
      - type: root.ChatMetadata
      - type: root.Error
      - type: root.UserInterruption
      - type: root.UserMessage
      - type: root.ToolCallMessage
      - type: root.ToolResponseMessage
      - type: root.ToolErrorMessage
    source:
      openapi: ../asyncapi.json
  PublishEvent:
    discriminated: false
    union:
      - type: root.AudioInput
      - type: root.SessionSettings
      - type: root.UserInput
      - type: root.AssistantInput
      - type: root.ToolResponseMessage
      - type: root.ToolErrorMessage
      - type: root.PauseAssistantMessage
      - type: root.ResumeAssistantMessage
    source:
      openapi: ../asyncapi.json
",
    },
    "getJobPredictions.yml": {
      "absoluteFilepath": "/DUMMY_PATH",
      "contents": {
        "imports": {
          "root": "__package__.yml",
        },
        "service": {
          "auth": false,
          "base-path": "",
          "display-name": "Get Job Predictions",
          "endpoints": {
            "get_job": {
              "auth": true,
              "display-name": "Get Job Details",
              "docs": "Get the request details and state of a given job.",
              "examples": [
                {
                  "path-parameters": {
                    "id": "id",
                  },
                  "response": {
                    "body": {
                      "job_id": "job_id",
                      "request": {
                        "callback_url": "callback_url",
                        "files": [
                          {
                            "md5sum": "md5sum",
                          },
                        ],
                        "models": {
                          "burst": {
                            "key": "value",
                          },
                          "facemesh": {
                            "key": "value",
                          },
                        },
                        "notify": true,
                        "transcription": {
                          "language": "zh",
                        },
                        "urls": [
                          "urls",
                        ],
                      },
                      "state": {
                        "created_timestamp_ms": 1000000,
                        "status": "QUEUED",
                      },
                      "user_id": "user_id",
                    },
                  },
                },
              ],
              "method": "GET",
              "pagination": undefined,
              "path": "/v0/batch/jobs/{id}",
              "path-parameters": {
                "id": "string",
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "root.JobRequest",
              },
              "source": {
                "openapi": "../openapi.yaml",
              },
            },
            "get_job_artifacts": {
              "auth": true,
              "display-name": "Get Job Artifacts",
              "docs": "Get the artifacts ZIP of a completed job.",
              "method": "GET",
              "pagination": undefined,
              "path": "/v0/batch/jobs/{id}/artifacts",
              "path-parameters": {
                "id": "string",
              },
              "response": {
                "docs": "",
                "status-code": 200,
                "type": "file",
              },
              "source": {
                "openapi": "../openapi.yaml",
              },
            },
          },
          "source": {
            "openapi": "../openapi.yaml",
          },
        },
      },
      "rawContents": "service:
  auth: false
  base-path: ''
  endpoints:
    get_job_artifacts:
      path: /v0/batch/jobs/{id}/artifacts
      method: GET
      auth: true
      docs: Get the artifacts ZIP of a completed job.
      source:
        openapi: ../openapi.yaml
      path-parameters:
        id: string
      display-name: Get Job Artifacts
      response:
        docs: ''
        type: file
        status-code: 200
    get_job:
      path: /v0/batch/jobs/{id}
      method: GET
      auth: true
      docs: Get the request details and state of a given job.
      source:
        openapi: ../openapi.yaml
      path-parameters:
        id: string
      display-name: Get Job Details
      response:
        docs: ''
        type: root.JobRequest
        status-code: 200
      examples:
        - path-parameters:
            id: id
          response:
            body:
              user_id: user_id
              job_id: job_id
              request:
                models:
                  burst:
                    key: value
                  facemesh:
                    key: value
                transcription:
                  language: zh
                urls:
                  - urls
                callback_url: callback_url
                notify: true
                files:
                  - md5sum: md5sum
              state:
                created_timestamp_ms: 1000000
                status: QUEUED
  source:
    openapi: ../openapi.yaml
  display-name: Get Job Predictions
imports:
  root: __package__.yml
",
    },
  },
  "packageMarkers": {},
  "rootApiFile": {
    "contents": {
      "auth": "Authentication",
      "auth-schemes": {
        "Authentication": {
          "header": "X-Hume-Api-Key",
          "name": "apiKey",
          "type": "string",
        },
      },
      "default-environment": "Default",
      "default-url": "Base",
      "display-name": "Hume AI Batch API",
      "environments": {
        "Default": {
          "urls": {
            "Base": "https://api.hume.ai",
            "prod": "wss://api.hume.ai/v0/evi",
          },
        },
      },
      "error-discrimination": {
        "strategy": "status-code",
      },
      "name": "api",
    },
    "defaultUrl": "Base",
    "rawContents": "name: api
error-discrimination:
  strategy: status-code
display-name: Hume AI Batch API
environments:
  Default:
    urls:
      Base: https://api.hume.ai
      prod: wss://api.hume.ai/v0/evi
default-environment: Default
default-url: Base
auth-schemes:
  Authentication:
    header: X-Hume-Api-Key
    name: apiKey
    type: string
auth: Authentication
",
  },
}