{
    "info": {
        "title": "Hume AI Empathic Voice Interface (EVI)",
        "version": "0.2.289",
        "description": "Chat with Empathic Voice Interface (EVI)."
    },
    "x-topics": [],
    "servers": {
        "prod": {
            "url": "wss://api.hume.ai/v0/evi",
            "protocol": "wss"
        }
    },
    "channels": {
        "/chat": {
            "description": "Chat with Empathic Voice Interface (EVI)",
            "bindings": {
                "ws": {
                    "query": {
                        "required": [],
                        "properties": {
                            "config_id": {
                                "description": "The unique identifier for an EVI configuration.\n\nInclude this ID in your connection request to equip EVI with the Prompt, Language Model, Voice, and Tools associated with the specified configuration. If omitted, EVI will apply [default configuration settings](/docs/empathic-voice-interface-evi/configuration#default-configuration).\n\nFor help obtaining this ID, see our [Configuration Guide](/docs/empathic-voice-interface-evi/configuration).",
                                "title": "Config Id",
                                "type": "string"
                            },
                            "config_version": {
                                "description": "The version number of the EVI configuration specified by the `config_id`.\n\nConfigs, as well as Prompts and Tools, are versioned. This versioning system supports iterative development, allowing you to progressively refine configurations and revert to previous versions if needed.\n\nInclude this parameter to apply a specific version of an EVI configuration. If omitted, the latest version will be applied.",
                                "title": "Config Version",
                                "type": "integer"
                            },
                            "resumed_chat_group_id": {
                                "description": "The unique identifier for a Chat Group. Use this field to preserve context from a previous Chat session.\n\nA Chat represents a single session from opening to closing a WebSocket connection. In contrast, a Chat Group is a series of resumed Chats that collectively represent a single conversation spanning multiple sessions. Each Chat includes a Chat Group ID, which is used to preserve the context of previous Chat sessions when starting a new one.\n\nIncluding the Chat Group ID in the `resumed_chat_group_id` query parameter is useful for seamlessly resuming a Chat after unexpected network disconnections and for picking up conversations exactly where you left off at a later time. This ensures preserved context across multiple sessions.\n\nThere are three ways to obtain the Chat Group ID:\n\n- [Chat Metadata](/reference/empathic-voice-interface-evi/chat/chat#receive.Chat%20Metadata.type): Upon establishing a WebSocket connection with EVI, the user receives a Chat Metadata message. This message contains a `chat_group_id`, which can be used to resume conversations within this chat group in future sessions.\n\n- [List Chats endpoint](/reference/empathic-voice-interface-evi/chats/list-chats): Use the GET `/v0/evi/chats` endpoint to obtain the Chat Group ID of individual Chat sessions. This endpoint lists all available Chat sessions and their associated Chat Group ID.\n\n- [List Chat Groups endpoint](/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups): Use the GET `/v0/evi/chat_groups` endpoint to obtain the Chat Group IDs of all Chat Groups associated with an API key. This endpoint returns a list of all available chat groups.",
                                "title": "Resumed Chat Group Id",
                                "type": "string"
                            },
                            "verbose_transcription": {
                                "type": "boolean",
                                "description": "A flag to enable verbose transcription. Set this query parameter to `\"true\"` to have unfinalized user transcripts be sent to the client as interim `UserMessage` messages.",
                                "default": false,
                                "title": "Verbose Transcription"
                            },
                            "access_token": {
                                "type": "string",
                                "description": "Access token used for authenticating the client. If not provided, an `api_key` must be provided to authenticate.\n\nThe access token is generated using both an API key and a Secret key, which provides an additional layer of security compared to using just an API key.\n\nFor more details, refer to the [Authentication Strategies Guide](/docs/introduction/api-key#authentication-strategies).",
                                "default": "",
                                "title": "Access Token"
                            }
                        },
                        "type": "object"
                    }
                }
            },
            "publish": {
                "message": {
                    "oneOf": [
                        {
                            "$ref": "#/components/messages/AudioInput"
                        },
                        {
                            "$ref": "#/components/messages/SessionSettings"
                        },
                        {
                            "$ref": "#/components/messages/UserInput"
                        },
                        {
                            "$ref": "#/components/messages/AssistantInput"
                        },
                        {
                            "$ref": "#/components/messages/ToolResponseMessage"
                        },
                        {
                            "$ref": "#/components/messages/ToolErrorMessage"
                        },
                        {
                            "$ref": "#/components/messages/PauseAssistantMessage"
                        },
                        {
                            "$ref": "#/components/messages/ResumeAssistantMessage"
                        }
                    ]
                }
            },
            "subscribe": {
                "message": {
                    "oneOf": [
                        {
                            "$ref": "#/components/messages/AssistantEnd"
                        },
                        {
                            "$ref": "#/components/messages/AssistantMessage"
                        },
                        {
                            "$ref": "#/components/messages/AudioOutput"
                        },
                        {
                            "$ref": "#/components/messages/ChatMetadata"
                        },
                        {
                            "$ref": "#/components/messages/Error"
                        },
                        {
                            "$ref": "#/components/messages/UserInterruption"
                        },
                        {
                            "$ref": "#/components/messages/UserMessage"
                        },
                        {
                            "$ref": "#/components/messages/ToolCallMessage"
                        },
                        {
                            "$ref": "#/components/messages/ToolResponseMessage"
                        },
                        {
                            "$ref": "#/components/messages/ToolErrorMessage"
                        }
                    ]
                }
            }
        }
    },
    "components": {
        "messages": {
            "AudioInput": {
                "name": "AudioInput",
                "description": "When provided, the input is audio.",
                "payload": {
                    "$ref": "#/components/schemas/AudioInput"
                }
            },
            "SessionSettings": {
                "name": "SessionSettings",
                "description": "Settings for this chat session.",
                "payload": {
                    "$ref": "#/components/schemas/SessionSettings"
                }
            },
            "UserInput": {
                "name": "UserInput",
                "description": "User text to insert into the conversation. Text sent through a User Input message is treated as the user’s speech to EVI. EVI processes this input and provides a corresponding response.\n\nExpression measurement results are not available for User Input messages, as the prosody model relies on audio input and cannot process text alone.",
                "payload": {
                    "$ref": "#/components/schemas/UserInput"
                }
            },
            "AssistantInput": {
                "name": "AssistantInput",
                "description": "When provided, the input is spoken by EVI.",
                "payload": {
                    "$ref": "#/components/schemas/AssistantInput"
                }
            },
            "ToolResponseMessage": {
                "name": "ToolResponseMessage",
                "description": "When provided, the output is a function call response.",
                "payload": {
                    "$ref": "#/components/schemas/ToolResponseMessage"
                }
            },
            "ToolErrorMessage": {
                "name": "ToolErrorMessage",
                "description": "When provided, the output is a function call error.",
                "payload": {
                    "$ref": "#/components/schemas/ToolErrorMessage"
                }
            },
            "PauseAssistantMessage": {
                "name": "PauseAssistantMessage",
                "description": "Pause responses from EVI. Chat history is still saved and sent after resuming. ",
                "payload": {
                    "$ref": "#/components/schemas/PauseAssistantMessage"
                }
            },
            "ResumeAssistantMessage": {
                "name": "ResumeAssistantMessage",
                "description": "Resume responses from EVI. Chat history sent while paused will now be sent. ",
                "payload": {
                    "$ref": "#/components/schemas/ResumeAssistantMessage"
                }
            },
            "AssistantEnd": {
                "name": "AssistantEnd",
                "description": "When provided, the output is an assistant end message.",
                "payload": {
                    "$ref": "#/components/schemas/AssistantEnd"
                }
            },
            "AssistantMessage": {
                "name": "AssistantMessage",
                "description": "When provided, the output is an assistant message.",
                "payload": {
                    "$ref": "#/components/schemas/AssistantMessage"
                }
            },
            "AudioOutput": {
                "name": "AudioOutput",
                "description": "The type of message sent through the socket; for an Audio Output message, this must be `audio_output`.",
                "payload": {
                    "$ref": "#/components/schemas/AudioOutput"
                }
            },
            "ChatMetadata": {
                "name": "ChatMetadata",
                "description": "When provided, the output is a chat metadata message.",
                "payload": {
                    "$ref": "#/components/schemas/ChatMetadata"
                }
            },
            "Error": {
                "name": "Error",
                "description": "When provided, the output is an error message.",
                "payload": {
                    "$ref": "#/components/schemas/Error"
                }
            },
            "UserInterruption": {
                "name": "UserInterruption",
                "description": "When provided, the output is an interruption.",
                "payload": {
                    "$ref": "#/components/schemas/UserInterruption"
                }
            },
            "UserMessage": {
                "name": "UserMessage",
                "description": "When provided, the output is a user message.",
                "payload": {
                    "$ref": "#/components/schemas/UserMessage"
                }
            },
            "ToolCallMessage": {
                "name": "ToolCallMessage",
                "description": "When provided, the output is a tool call.",
                "payload": {
                    "$ref": "#/components/schemas/ToolCallMessage"
                }
            }
        },
        "schemas": {
            "AssistantInput": {
                "description": "When provided, the input is spoken by EVI.",
                "properties": {
                    "type": {
                        "const": "assistant_input",
                        "description": "The type of message sent through the socket; must be `assistant_input` for our server to correctly identify and process it as an Assistant Input message.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "title": "Custom Session Id"
                    },
                    "text": {
                        "description": "Assistant text to synthesize into spoken audio and insert into the conversation.\n\nEVI uses this text to generate spoken audio using our proprietary expressive text-to-speech model. Our model adds appropriate emotional inflections and tones to the text based on the user’s expressions and the context of the conversation. The synthesized audio is streamed back to the user as an [Assistant Message](/reference/empathic-voice-interface-evi/chat/chat#receive.Assistant%20Message.type).",
                        "title": "Text",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "text"
                ],
                "title": "AssistantInput",
                "type": "object"
            },
            "AudioConfiguration": {
                "properties": {
                    "encoding": {
                        "$ref": "#/components/schemas/Encoding",
                        "description": "Encoding format of the audio input, such as `linear16`."
                    },
                    "channels": {
                        "description": "Number of audio channels.",
                        "title": "Channels",
                        "type": "integer"
                    },
                    "sample_rate": {
                        "description": "Audio sample rate. Number of samples per second in the audio input, measured in Hertz.",
                        "title": "Sample Rate",
                        "type": "integer"
                    }
                },
                "required": [
                    "encoding",
                    "channels",
                    "sample_rate"
                ],
                "title": "AudioConfiguration",
                "type": "object"
            },
            "AudioInput": {
                "description": "When provided, the input is audio.",
                "properties": {
                    "type": {
                        "const": "audio_input",
                        "description": "The type of message sent through the socket; must be `audio_input` for our server to correctly identify and process it as an Audio Input message.\n\nThis message is used for sending audio input data to EVI for processing and expression measurement. Audio data should be sent as a continuous stream, encoded in Base64.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "data": {
                        "description": "Base64 encoded audio input to insert into the conversation.\n\nThe content of an Audio Input message is treated as the user’s speech to EVI and must be streamed continuously. Pre-recorded audio files are not supported.\n\nFor optimal transcription quality, the audio data should be transmitted in small chunks.\n\nHume recommends streaming audio with a buffer window of 20 milliseconds (ms), or 100 milliseconds (ms) for web applications.",
                        "format": "base64",
                        "title": "Data",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "data"
                ],
                "title": "AudioInput",
                "type": "object"
            },
            "BuiltInTool": {
                "enum": [
                    "web_search",
                    "hang_up"
                ],
                "title": "BuiltInTool",
                "type": "string"
            },
            "BuiltinToolConfig": {
                "properties": {
                    "name": {
                        "$ref": "#/components/schemas/BuiltInTool"
                    },
                    "fallback_content": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Optional text passed to the supplemental LLM if the tool call fails. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation.",
                        "title": "Fallback Content"
                    }
                },
                "required": [
                    "name"
                ],
                "title": "BuiltinToolConfig",
                "type": "object"
            },
            "Context": {
                "properties": {
                    "type": {
                        "$ref": "#/components/schemas/ContextType",
                        "default": "temporary",
                        "description": "The persistence level of the injected context. Specifies how long the injected context will remain active in the session.\n\nThere are three possible context types:\n\n- **Persistent**: The context is appended to all user messages for the duration of the session.\n\n- **Temporary**: The context is appended only to the next user message.\n\n - **Editable**: The original context is updated to reflect the new context.\n\n If the type is not specified, it will default to `temporary`."
                    },
                    "text": {
                        "description": "The context to be injected into the conversation. Helps inform the LLM's response by providing relevant information about the ongoing conversation.\n\nThis text will be appended to the end of user messages based on the chosen persistence level. For example, if you want to remind EVI of its role as a helpful weather assistant, the context you insert will be appended to the end of user messages as `{Context: You are a helpful weather assistant}`.",
                        "title": "Text",
                        "type": "string"
                    }
                },
                "required": [
                    "text"
                ],
                "title": "Context",
                "type": "object"
            },
            "ContextType": {
                "enum": [
                    "editable",
                    "persistent",
                    "temporary"
                ],
                "title": "ContextType",
                "type": "string"
            },
            "Encoding": {
                "enum": [
                    "linear16"
                ],
                "title": "Encoding",
                "type": "string"
            },
            "ErrorLevel": {
                "enum": [
                    "warn"
                ],
                "title": "ErrorLevel",
                "type": "string"
            },
            "PauseAssistantMessage": {
                "description": "Pause responses from EVI. Chat history is still saved and sent after resuming. ",
                "properties": {
                    "type": {
                        "const": "pause_assistant_message",
                        "default": "pause_assistant_message",
                        "description": "The type of message sent through the socket; must be `pause_assistant_message` for our server to correctly identify and process it as a Pause Assistant message.\n\nOnce this message is sent, EVI will not respond until a [Resume Assistant message](/reference/empathic-voice-interface-evi/chat/chat#send.Resume%20Assistant%20Message.type) is sent. When paused, EVI won’t respond, but transcriptions of your audio inputs will still be recorded.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    }
                },
                "title": "PauseAssistantMessage",
                "type": "object"
            },
            "ResumeAssistantMessage": {
                "description": "Resume responses from EVI. Chat history sent while paused will now be sent. ",
                "properties": {
                    "type": {
                        "const": "resume_assistant_message",
                        "default": "resume_assistant_message",
                        "description": "The type of message sent through the socket; must be `resume_assistant_message` for our server to correctly identify and process it as a Resume Assistant message.\n\nUpon resuming, if any audio input was sent during the pause, EVI will retain context from all messages sent but only respond to the last user message. (e.g., If you ask EVI two questions while paused and then send a `resume_assistant_message`, EVI will respond to the second question and have added the first question to its conversation context.)",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    }
                },
                "title": "ResumeAssistantMessage",
                "type": "object"
            },
            "SessionSettings": {
                "description": "Settings for this chat session.",
                "properties": {
                    "type": {
                        "const": "session_settings",
                        "description": "The type of message sent through the socket; must be `session_settings` for our server to correctly identify and process it as a Session Settings message.\n\nSession settings are temporary and apply only to the current Chat session. These settings can be adjusted dynamically based on the requirements of each session to ensure optimal performance and user experience.\n\nFor more information, please refer to the [Session Settings section](/docs/empathic-voice-interface-evi/configuration#session-settings) on the EVI Configuration page.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Unique identifier for the session. Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.\n\nIf included, the response sent from Hume to your backend will include this ID. This allows you to correlate frontend users with their incoming messages.\n\nIt is recommended to pass a `custom_session_id` if you are using a Custom Language Model. Please see our guide to [using a custom language model](/docs/empathic-voice-interface-evi/custom-language-model) with EVI to learn more.",
                        "title": "Custom Session Id"
                    },
                    "system_prompt": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Instructions used to shape EVI’s behavior, responses, and style for the session.\n\nWhen included in a Session Settings message, the provided Prompt overrides the existing one specified in the EVI configuration. If no Prompt was defined in the configuration, this Prompt will be the one used for the session.\n\nYou can use the Prompt to define a specific goal or role for EVI, specifying how it should act or what it should focus on during the conversation. For example, EVI can be instructed to act as a customer support representative, a fitness coach, or a travel advisor, each with its own set of behaviors and response styles.\n\nFor help writing a system prompt, see our [Prompting Guide](/docs/empathic-voice-interface-evi/prompting).",
                        "title": "System Prompt"
                    },
                    "context": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/Context"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Allows developers to inject additional context into the conversation, which is appended to the end of user messages for the session.\n\nWhen included in a Session Settings message, the provided context can be used to remind the LLM of its role in every user message, prevent it from forgetting important details, or add new relevant information to the conversation.\n\nSet to `null` to disable context injection."
                    },
                    "audio": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/AudioConfiguration"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Configuration details for the audio input used during the session. Ensures the audio is being correctly set up for processing.\n\nThis optional field is only required when the audio input is encoded in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For detailed instructions on how to configure session settings for PCM Linear 16 audio, please refer to the [Session Settings section](/docs/empathic-voice-interface-evi/configuration#session-settings) on the EVI Configuration page."
                    },
                    "language_model_api_key": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Third party API key for the supplemental language model.\n\nWhen provided, EVI will use this key instead of Hume’s API key for the supplemental LLM. This allows you to bypass rate limits and utilize your own API key as needed.",
                        "title": "Language Model Api Key"
                    },
                    "tools": {
                        "anyOf": [
                            {
                                "items": {
                                    "$ref": "#/components/schemas/Tool"
                                },
                                "type": "array"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "List of user-defined tools to enable for the session.\n\nTools are resources used by EVI to perform various tasks, such as searching the web or calling external APIs. Built-in tools, like web search, are natively integrated, while user-defined tools are created and invoked by the user. To learn more, see our [Tool Use Guide](/docs/empathic-voice-interface-evi/tool-use).",
                        "title": "Tools"
                    },
                    "builtin_tools": {
                        "anyOf": [
                            {
                                "items": {
                                    "$ref": "#/components/schemas/BuiltinToolConfig"
                                },
                                "type": "array"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "List of built-in tools to enable for the session.\n\nTools are resources used by EVI to perform various tasks, such as searching the web or calling external APIs. Built-in tools, like web search, are natively integrated, while user-defined tools are created and invoked by the user. To learn more, see our [Tool Use Guide](/docs/empathic-voice-interface-evi/tool-use).\n\nCurrently, the only built-in tool Hume provides is **Web Search**. When enabled, Web Search equips EVI with the ability to search the web for up-to-date information.",
                        "title": "Builtin Tools"
                    },
                    "metadata": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "title": "Metadata"
                    },
                    "variables": {
                        "anyOf": [
                            {
                                "additionalProperties": {
                                    "type": "string"
                                },
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Dynamic values that can be used to populate EVI prompts.",
                        "title": "Variables"
                    }
                },
                "required": [
                    "type"
                ],
                "title": "SessionSettings",
                "type": "object"
            },
            "Tool": {
                "properties": {
                    "type": {
                        "$ref": "#/components/schemas/ToolType",
                        "description": "Type of tool. Set to `function` for user-defined tools."
                    },
                    "name": {
                        "description": "Name of the user-defined tool to be enabled.",
                        "title": "Name",
                        "type": "string"
                    },
                    "parameters": {
                        "description": "Parameters of the tool. Is a stringified JSON schema.\n\nThese parameters define the inputs needed for the tool’s execution, including the expected data type and description for each input field. Structured as a JSON schema, this format ensures the tool receives data in the expected format.",
                        "title": "Parameters",
                        "type": "string"
                    },
                    "description": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "An optional description of what the tool does, used by the supplemental LLM to choose when and how to call the function.",
                        "title": "Description"
                    },
                    "fallback_content": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Optional text passed to the supplemental LLM if the tool call fails. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation.",
                        "title": "Fallback Content"
                    }
                },
                "required": [
                    "type",
                    "name",
                    "parameters"
                ],
                "title": "Tool",
                "type": "object"
            },
            "ToolErrorMessage": {
                "description": "When provided, the output is a function call error.",
                "properties": {
                    "type": {
                        "const": "tool_error",
                        "default": "tool_error",
                        "description": "The type of message sent through the socket; for a Tool Error message, this must be `tool_error`.\n\nUpon receiving a [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type) and failing to invoke the function, this message is sent to notify EVI of the tool's failure.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "tool_type": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ToolType"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": "function",
                        "description": "Type of tool called. Either `builtin` for natively implemented tools, like web search, or `function` for user-defined tools."
                    },
                    "tool_call_id": {
                        "description": "The unique identifier for a specific tool call instance.\n\nThis ID is used to track the request and response of a particular tool invocation, ensuring that the Tool Error message is linked to the appropriate tool call request. The specified `tool_call_id` must match the one received in the [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).",
                        "title": "Tool Call Id",
                        "type": "string"
                    },
                    "content": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Optional text passed to the supplemental LLM in place of the tool call result. The LLM then uses this text to generate a response back to the user, ensuring continuity in the conversation if the tool errors.",
                        "title": "Content"
                    },
                    "error": {
                        "description": "Error message from the tool call, not exposed to the LLM or user.",
                        "title": "Error",
                        "type": "string"
                    },
                    "code": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Error code. Identifies the type of error encountered.",
                        "title": "Code"
                    },
                    "level": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ErrorLevel"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": "warn",
                        "description": "Indicates the severity of an error; for a Tool Error message, this must be `warn` to signal an unexpected event."
                    }
                },
                "required": [
                    "tool_call_id",
                    "error"
                ],
                "title": "ToolErrorMessage",
                "type": "object"
            },
            "ToolResponseMessage": {
                "description": "When provided, the output is a function call response.",
                "properties": {
                    "type": {
                        "const": "tool_response",
                        "default": "tool_response",
                        "description": "The type of message sent through the socket; for a Tool Response message, this must be `tool_response`.\n\nUpon receiving a [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type) and successfully invoking the function, this message is sent to convey the result of the function call back to EVI.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "tool_call_id": {
                        "description": "The unique identifier for a specific tool call instance.\n\nThis ID is used to track the request and response of a particular tool invocation, ensuring that the correct response is linked to the appropriate request. The specified `tool_call_id` must match the one received in the [Tool Call message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.tool_call_id).",
                        "title": "Tool Call Id",
                        "type": "string"
                    },
                    "content": {
                        "description": "Return value of the tool call. Contains the output generated by the tool to pass back to EVI.",
                        "title": "Content",
                        "type": "string"
                    },
                    "tool_name": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "title": "Tool Name"
                    },
                    "tool_type": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ToolType"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null
                    }
                },
                "required": [
                    "tool_call_id",
                    "content"
                ],
                "title": "ToolResponseMessage",
                "type": "object"
            },
            "ToolType": {
                "enum": [
                    "builtin",
                    "function"
                ],
                "title": "ToolType",
                "type": "string"
            },
            "UserInput": {
                "description": "User text to insert into the conversation. Text sent through a User Input message is treated as the user’s speech to EVI. EVI processes this input and provides a corresponding response.\n\nExpression measurement results are not available for User Input messages, as the prosody model relies on audio input and cannot process text alone.",
                "properties": {
                    "type": {
                        "const": "user_input",
                        "description": "The type of message sent through the socket; must be `user_input` for our server to correctly identify and process it as a User Input message.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "text": {
                        "description": "User text to insert into the conversation. Text sent through a User Input message is treated as the user’s speech to EVI. EVI processes this input and provides a corresponding response.\n\nExpression measurement results are not available for User Input messages, as the prosody model relies on audio input and cannot process text alone.",
                        "title": "Text",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "text"
                ],
                "title": "UserInput",
                "type": "object"
            },
            "AssistantEnd": {
                "description": "When provided, the output is an assistant end message.",
                "properties": {
                    "type": {
                        "const": "assistant_end",
                        "description": "The type of message sent through the socket; for an Assistant End message, this must be `assistant_end`.\n\nThis message indicates the conclusion of the assistant’s response, signaling that the assistant has finished speaking for the current conversational turn.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    }
                },
                "required": [
                    "type"
                ],
                "title": "AssistantEnd",
                "type": "object"
            },
            "AssistantMessage": {
                "description": "When provided, the output is an assistant message.",
                "properties": {
                    "type": {
                        "const": "assistant_message",
                        "description": "The type of message sent through the socket; for an Assistant Message, this must be `assistant_message`.\n\nThis message contains both a transcript of the assistant’s response and the expression measurement predictions of the assistant’s audio output.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "id": {
                        "description": "ID of the assistant message. Allows the Assistant Message to be tracked and referenced.",
                        "title": "Id",
                        "type": "string"
                    },
                    "message": {
                        "$ref": "#/components/schemas/ChatMessage",
                        "description": "Transcript of the message."
                    },
                    "models": {
                        "$ref": "#/components/schemas/Inference",
                        "description": "Inference model results."
                    },
                    "from_text": {
                        "description": "Indicates if this message was inserted into the conversation as text from an [Assistant Input message](/reference/empathic-voice-interface-evi/chat/chat#send.Assistant%20Input.text).",
                        "title": "From Text",
                        "type": "boolean"
                    }
                },
                "required": [
                    "type",
                    "message",
                    "models",
                    "from_text"
                ],
                "title": "AssistantMessage",
                "type": "object"
            },
            "AudioOutput": {
                "description": "The type of message sent through the socket; for an Audio Output message, this must be `audio_output`.",
                "properties": {
                    "type": {
                        "const": "audio_output",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "id": {
                        "description": "ID of the audio output. Allows the Audio Output message to be tracked and referenced.",
                        "title": "Id",
                        "type": "string"
                    },
                    "index": {
                        "description": "Index of the chunk of audio relative to the whole audio segment.",
                        "title": "Index",
                        "type": "integer"
                    },
                    "data": {
                        "description": "Base64 encoded audio output. This encoded audio is transmitted to the client, where it can be decoded and played back as part of the user interaction.",
                        "format": "base64",
                        "title": "Data",
                        "type": "string"
                    },
                    "is_final_chunk": {
                        "description": "This AudioOutput contains the final chunk for this particular segment.",
                        "title": "Is Final Chunk",
                        "type": "boolean"
                    }
                },
                "required": [
                    "type",
                    "id",
                    "index",
                    "data",
                    "is_final_chunk"
                ],
                "title": "AudioOutput",
                "type": "object"
            },
            "ChatMessage": {
                "properties": {
                    "role": {
                        "$ref": "#/components/schemas/Role",
                        "description": "Role of who is providing the message."
                    },
                    "content": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Transcript of the message.",
                        "title": "Content"
                    },
                    "tool_call": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ToolCallMessage"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Function call name and arguments."
                    },
                    "tool_result": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ToolResponseMessage"
                            },
                            {
                                "$ref": "#/components/schemas/ToolErrorMessage"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Function call response from client.",
                        "title": "Tool Result"
                    }
                },
                "required": [
                    "role"
                ],
                "title": "ChatMessage",
                "type": "object"
            },
            "ChatMetadata": {
                "description": "When provided, the output is a chat metadata message.",
                "properties": {
                    "type": {
                        "const": "chat_metadata",
                        "description": "The type of message sent through the socket; for a Chat Metadata message, this must be `chat_metadata`.\n\nThe Chat Metadata message is the first message you receive after establishing a connection with EVI and contains important identifiers for the current Chat session.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "chat_group_id": {
                        "description": "ID of the Chat Group.\n\nUsed to resume a Chat when passed in the [resumed_chat_group_id](/reference/empathic-voice-interface-evi/chat/chat#request.query.resumed_chat_group_id) query parameter of a subsequent connection request. This allows EVI to continue the conversation from where it left off within the Chat Group.\n\nLearn more about [supporting chat resumability](/docs/empathic-voice-interface-evi/faq#does-evi-support-chat-resumability) from the EVI FAQ.",
                        "title": "Chat Group Id",
                        "type": "string"
                    },
                    "chat_id": {
                        "description": "ID of the Chat session. Allows the Chat session to be tracked and referenced.",
                        "title": "Chat Id",
                        "type": "string"
                    },
                    "request_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "description": "ID of the initiating request.",
                        "title": "Request Id"
                    }
                },
                "required": [
                    "type",
                    "chat_group_id",
                    "chat_id",
                    "request_id"
                ],
                "title": "ChatMetadata",
                "type": "object"
            },
            "EmotionScores": {
                "properties": {
                    "Admiration": {
                        "title": "Admiration",
                        "type": "number"
                    },
                    "Adoration": {
                        "title": "Adoration",
                        "type": "number"
                    },
                    "Aesthetic Appreciation": {
                        "title": "Aesthetic Appreciation",
                        "type": "number"
                    },
                    "Amusement": {
                        "title": "Amusement",
                        "type": "number"
                    },
                    "Anger": {
                        "title": "Anger",
                        "type": "number"
                    },
                    "Anxiety": {
                        "title": "Anxiety",
                        "type": "number"
                    },
                    "Awe": {
                        "title": "Awe",
                        "type": "number"
                    },
                    "Awkwardness": {
                        "title": "Awkwardness",
                        "type": "number"
                    },
                    "Boredom": {
                        "title": "Boredom",
                        "type": "number"
                    },
                    "Calmness": {
                        "title": "Calmness",
                        "type": "number"
                    },
                    "Concentration": {
                        "title": "Concentration",
                        "type": "number"
                    },
                    "Confusion": {
                        "title": "Confusion",
                        "type": "number"
                    },
                    "Contemplation": {
                        "title": "Contemplation",
                        "type": "number"
                    },
                    "Contempt": {
                        "title": "Contempt",
                        "type": "number"
                    },
                    "Contentment": {
                        "title": "Contentment",
                        "type": "number"
                    },
                    "Craving": {
                        "title": "Craving",
                        "type": "number"
                    },
                    "Desire": {
                        "title": "Desire",
                        "type": "number"
                    },
                    "Determination": {
                        "title": "Determination",
                        "type": "number"
                    },
                    "Disappointment": {
                        "title": "Disappointment",
                        "type": "number"
                    },
                    "Disgust": {
                        "title": "Disgust",
                        "type": "number"
                    },
                    "Distress": {
                        "title": "Distress",
                        "type": "number"
                    },
                    "Doubt": {
                        "title": "Doubt",
                        "type": "number"
                    },
                    "Ecstasy": {
                        "title": "Ecstasy",
                        "type": "number"
                    },
                    "Embarrassment": {
                        "title": "Embarrassment",
                        "type": "number"
                    },
                    "Empathic Pain": {
                        "title": "Empathic Pain",
                        "type": "number"
                    },
                    "Entrancement": {
                        "title": "Entrancement",
                        "type": "number"
                    },
                    "Envy": {
                        "title": "Envy",
                        "type": "number"
                    },
                    "Excitement": {
                        "title": "Excitement",
                        "type": "number"
                    },
                    "Fear": {
                        "title": "Fear",
                        "type": "number"
                    },
                    "Guilt": {
                        "title": "Guilt",
                        "type": "number"
                    },
                    "Horror": {
                        "title": "Horror",
                        "type": "number"
                    },
                    "Interest": {
                        "title": "Interest",
                        "type": "number"
                    },
                    "Joy": {
                        "title": "Joy",
                        "type": "number"
                    },
                    "Love": {
                        "title": "Love",
                        "type": "number"
                    },
                    "Nostalgia": {
                        "title": "Nostalgia",
                        "type": "number"
                    },
                    "Pain": {
                        "title": "Pain",
                        "type": "number"
                    },
                    "Pride": {
                        "title": "Pride",
                        "type": "number"
                    },
                    "Realization": {
                        "title": "Realization",
                        "type": "number"
                    },
                    "Relief": {
                        "title": "Relief",
                        "type": "number"
                    },
                    "Romance": {
                        "title": "Romance",
                        "type": "number"
                    },
                    "Sadness": {
                        "title": "Sadness",
                        "type": "number"
                    },
                    "Satisfaction": {
                        "title": "Satisfaction",
                        "type": "number"
                    },
                    "Shame": {
                        "title": "Shame",
                        "type": "number"
                    },
                    "Surprise (negative)": {
                        "title": "Surprise (Negative)",
                        "type": "number"
                    },
                    "Surprise (positive)": {
                        "title": "Surprise (Positive)",
                        "type": "number"
                    },
                    "Sympathy": {
                        "title": "Sympathy",
                        "type": "number"
                    },
                    "Tiredness": {
                        "title": "Tiredness",
                        "type": "number"
                    },
                    "Triumph": {
                        "title": "Triumph",
                        "type": "number"
                    }
                },
                "required": [
                    "Admiration",
                    "Adoration",
                    "Aesthetic Appreciation",
                    "Amusement",
                    "Anger",
                    "Anxiety",
                    "Awe",
                    "Awkwardness",
                    "Boredom",
                    "Calmness",
                    "Concentration",
                    "Confusion",
                    "Contemplation",
                    "Contempt",
                    "Contentment",
                    "Craving",
                    "Desire",
                    "Determination",
                    "Disappointment",
                    "Disgust",
                    "Distress",
                    "Doubt",
                    "Ecstasy",
                    "Embarrassment",
                    "Empathic Pain",
                    "Entrancement",
                    "Envy",
                    "Excitement",
                    "Fear",
                    "Guilt",
                    "Horror",
                    "Interest",
                    "Joy",
                    "Love",
                    "Nostalgia",
                    "Pain",
                    "Pride",
                    "Realization",
                    "Relief",
                    "Romance",
                    "Sadness",
                    "Satisfaction",
                    "Shame",
                    "Surprise (negative)",
                    "Surprise (positive)",
                    "Sympathy",
                    "Tiredness",
                    "Triumph"
                ],
                "title": "EmotionScores",
                "type": "object"
            },
            "Error": {
                "description": "When provided, the output is an error message.",
                "properties": {
                    "type": {
                        "const": "error",
                        "description": "The type of message sent through the socket; for a Web Socket Error message, this must be `error`.\n\nThis message indicates a disruption in the WebSocket connection, such as an unexpected disconnection, protocol error, or data transmission issue.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "code": {
                        "description": "Error code. Identifies the type of error encountered.",
                        "title": "Code",
                        "type": "string"
                    },
                    "slug": {
                        "description": "Short, human-readable identifier and description for the error. See a complete list of error slugs on the [Errors page](/docs/resources/errors).",
                        "title": "Slug",
                        "type": "string"
                    },
                    "message": {
                        "description": "Detailed description of the error.",
                        "title": "Message",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "code",
                    "slug",
                    "message"
                ],
                "title": "Error",
                "type": "object"
            },
            "Inference": {
                "properties": {
                    "prosody": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ProsodyInference"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "description": "Prosody model inference results.\n\nEVI uses the prosody model to measure 48 emotions related to speech and vocal characteristics within a given expression."
                    }
                },
                "required": [
                    "prosody"
                ],
                "title": "Inference",
                "type": "object"
            },
            "MillisecondInterval": {
                "properties": {
                    "begin": {
                        "description": "Start time of the interval in milliseconds.",
                        "title": "Begin",
                        "type": "integer"
                    },
                    "end": {
                        "description": "End time of the interval in milliseconds.",
                        "title": "End",
                        "type": "integer"
                    }
                },
                "required": [
                    "begin",
                    "end"
                ],
                "title": "MillisecondInterval",
                "type": "object"
            },
            "ProsodyInference": {
                "properties": {
                    "scores": {
                        "$ref": "#/components/schemas/EmotionScores",
                        "description": "The confidence scores for 48 emotions within the detected expression of an audio sample.\n\nScores typically range from 0 to 1, with higher values indicating a stronger confidence level in the measured attribute.\n\nSee our guide on [interpreting expression measurement results](/docs/expression-measurement/faq#how-do-i-interpret-my-results) to learn more."
                    }
                },
                "required": [
                    "scores"
                ],
                "title": "ProsodyInference",
                "type": "object"
            },
            "Role": {
                "enum": [
                    "assistant",
                    "system",
                    "user",
                    "all",
                    "tool"
                ],
                "title": "Role",
                "type": "string"
            },
            "ToolCallMessage": {
                "description": "When provided, the output is a tool call.",
                "properties": {
                    "name": {
                        "description": "Name of the tool called.",
                        "title": "Name",
                        "type": "string"
                    },
                    "parameters": {
                        "description": "Parameters of the tool call. Is a stringified JSON schema.",
                        "title": "Parameters",
                        "type": "string"
                    },
                    "tool_call_id": {
                        "description": "The unique identifier for a specific tool call instance.\n\nThis ID is used to track the request and response of a particular tool invocation, ensuring that the correct response is linked to the appropriate request.",
                        "title": "Tool Call Id",
                        "type": "string"
                    },
                    "type": {
                        "const": "tool_call",
                        "default": "tool_call",
                        "description": "The type of message sent through the socket; for a Tool Call message, this must be `tool_call`.\n\nThis message indicates that the supplemental LLM has detected a need to invoke the specified tool.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "tool_type": {
                        "$ref": "#/components/schemas/ToolType",
                        "description": "Type of tool called. Either `builtin` for natively implemented tools, like web search, or `function` for user-defined tools."
                    },
                    "response_required": {
                        "description": "Indicates whether a response to the tool call is required from the developer, either in the form of a [Tool Response message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Response%20Message.type) or a [Tool Error message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Error%20Message.type).",
                        "title": "Response Required",
                        "type": "boolean"
                    }
                },
                "required": [
                    "name",
                    "parameters",
                    "tool_call_id",
                    "tool_type",
                    "response_required"
                ],
                "title": "ToolCallMessage",
                "type": "object"
            },
            "UserInterruption": {
                "description": "When provided, the output is an interruption.",
                "properties": {
                    "type": {
                        "const": "user_interruption",
                        "description": "The type of message sent through the socket; for a User Interruption message, this must be `user_interruption`.\n\nThis message indicates the user has interrupted the assistant’s response. EVI detects the interruption in real-time and sends this message to signal the interruption event. This message allows the system to stop the current audio playback, clear the audio queue, and prepare to handle new user input.",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "time": {
                        "description": "Unix timestamp of the detected user interruption.",
                        "title": "Time",
                        "type": "integer"
                    }
                },
                "required": [
                    "type",
                    "time"
                ],
                "title": "UserInterruption",
                "type": "object"
            },
            "UserMessage": {
                "description": "When provided, the output is a user message.",
                "properties": {
                    "type": {
                        "const": "user_message",
                        "title": "Type",
                        "type": "string"
                    },
                    "custom_session_id": {
                        "anyOf": [
                            {
                                "type": "string"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "description": "Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.",
                        "title": "Custom Session Id"
                    },
                    "message": {
                        "$ref": "#/components/schemas/ChatMessage",
                        "description": "Transcript of the message."
                    },
                    "models": {
                        "$ref": "#/components/schemas/Inference",
                        "description": "Inference model results."
                    },
                    "time": {
                        "$ref": "#/components/schemas/MillisecondInterval",
                        "description": "Start and End time of user message."
                    },
                    "from_text": {
                        "description": "Indicates if this message was inserted into the conversation as text from a [User Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text) message.",
                        "title": "From Text",
                        "type": "boolean"
                    },
                    "interim": {
                        "description": "Indicates if this message contains an immediate and unfinalized transcript of the user's audio input. If it does, words may be repeated across successive UserMessage messages as our transcription model becomes more confident about what was said with additional context. Interim messages are useful to detect if the user is interrupting during audio playback on the client. Even without a finalized transcription, along with `UserInterrupt` messages, interim `UserMessages` are useful for detecting if the user is interrupting during audio playback on the client, signaling to stop playback in your application.",
                        "title": "Interim",
                        "type": "boolean"
                    }
                },
                "required": [
                    "type",
                    "message",
                    "models",
                    "time",
                    "from_text",
                    "interim"
                ],
                "title": "UserMessage",
                "type": "object"
            }
        }
    },
    "asyncapi": "2.6.0"
}