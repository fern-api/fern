// Code generated by Fern. DO NOT EDIT.

package v2

import (
	fmt "fmt"
)

type V2ChatRequest struct {
	// The name of a compatible [Cohere model](https://docs.cohere.com/docs/models) (such as command-r or command-r-plus) or the ID of a [fine-tuned](https://docs.cohere.com/docs/chat-fine-tuning) model.
	Model string `json:"model" url:"-"`
	// A list of relevant documents that the model can cite to generate a more accurate reply. Each document is either a string or document object with content and metadata.
	Documents []string `json:"documents,omitempty" url:"-"`
	// Used to select the [safety instruction](/docs/safety-modes) inserted into the prompt. Defaults to `CONTEXTUAL`.
	// When `NONE` is specified, the safety instruction will be omitted.
	//
	// Safety modes are not yet configurable in combination with `tools`, `tool_results` and `documents` parameters.
	//
	// **Note**: This parameter is only compatible with models [Command R 08-2024](/docs/command-r#august-2024-release), [Command R+ 08-2024](/docs/command-r-plus#august-2024-release) and newer.
	//
	// Compatible Deployments: Cohere Platform, Azure, AWS Sagemaker/Bedrock, Private Deployments
	SafetyMode *V2ChatRequestSafetyMode `json:"safety_mode,omitempty" url:"-"`
	// The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.
	MaxTokens *int `json:"max_tokens,omitempty" url:"-"`
	// A list of up to 5 strings that the model will use to stop generation. If the model generates a string that matches any of the strings in the list, it will stop generating tokens and return the generated text up to that point not including the stop sequence.
	StopSequences []string `json:"stop_sequences,omitempty" url:"-"`
	// Defaults to `0.3`.
	//
	// A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.
	//
	// Randomness can be further maximized by increasing the  value of the `p` parameter.
	Temperature *float64 `json:"temperature,omitempty" url:"-"`
	// Whether to return the prompt in the response.
	ReturnPrompt *bool `json:"return_prompt,omitempty" url:"-"`
}

// Used to select the [safety instruction](/docs/safety-modes) inserted into the prompt. Defaults to `CONTEXTUAL`.
// When `NONE` is specified, the safety instruction will be omitted.
//
// Safety modes are not yet configurable in combination with `tools`, `tool_results` and `documents` parameters.
//
// **Note**: This parameter is only compatible with models [Command R 08-2024](/docs/command-r#august-2024-release), [Command R+ 08-2024](/docs/command-r-plus#august-2024-release) and newer.
//
// Compatible Deployments: Cohere Platform, Azure, AWS Sagemaker/Bedrock, Private Deployments
type V2ChatRequestSafetyMode string

const (
	V2ChatRequestSafetyModeContextual V2ChatRequestSafetyMode = "CONTEXTUAL"
	V2ChatRequestSafetyModeStrict     V2ChatRequestSafetyMode = "STRICT"
	V2ChatRequestSafetyModeNone       V2ChatRequestSafetyMode = "NONE"
)

func NewV2ChatRequestSafetyModeFromString(s string) (V2ChatRequestSafetyMode, error) {
	switch s {
	case "CONTEXTUAL":
		return V2ChatRequestSafetyModeContextual, nil
	case "STRICT":
		return V2ChatRequestSafetyModeStrict, nil
	case "NONE":
		return V2ChatRequestSafetyModeNone, nil
	}
	var t V2ChatRequestSafetyMode
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (v V2ChatRequestSafetyMode) Ptr() *V2ChatRequestSafetyMode {
	return &v
}
